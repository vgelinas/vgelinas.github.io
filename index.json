[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m interested in Data Science and Machine Learning, with interests ranging between data storytelling, data analysis and deploying machine learning models. This site is a place to share my projects or cool things I\u0026rsquo;ve come across.\nPreviously, I was a mathematics research fellow at Trinity College, Dublin. Before that, I completed a PhD at the University of Toronto, where my advisor was the incredible Ragnar-Olaf Buchweitz. I\u0026rsquo;m originally from Montréal, and speak French fluently.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://vgelinas.github.io/author/vincent-gelinas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/vincent-gelinas/","section":"authors","summary":"I\u0026rsquo;m interested in Data Science and Machine Learning, with interests ranging between data storytelling, data analysis and deploying machine learning models. This site is a place to share my projects or cool things I\u0026rsquo;ve come across.","tags":null,"title":"Vincent Gélinas","type":"authors"},{"authors":null,"categories":null,"content":"This comic was posted by xkcd last friday:\nI thought I\u0026rsquo;d estimate the number of throws needed to make 30 shots in a row. How long will this take?\nA coin flipping problem This problem is pretty much the same as the following coin flipping problem: What is the expected number of coin flips needed before you get N consecutive tails?\nThis is a pretty well-known problem, and google will tell you a lot. The only difference is the probability of success (50% for a fair coin, and 30% for a free throw), but let\u0026rsquo;s start simple.\n\u0026nbsp;\nExpected number of flips before getting one tail Let\u0026rsquo;s start with getting a single tail. Intuitively, we should expect 2 flips are needed on average to get a tail, but let\u0026rsquo;s work it out.\nThe expected value E is the weighted average over each possible outcome, weighted by the odds of that outcome. In this scenario, the number of flips required could be k = 1, 2, 3, 4, \u0026hellip; and so on. For each number of flips k required, we then figure out the odds:\n k = 1: These are the odds of getting tail at the start. The probability is 1/2. k = 2: These are the odds of getting head, then tail. This comes out to (1/2)(1/2) = 1/4. k = 3: These are the odds of getting head, head, then tail. This is $(1/2)^3 = 1/8$. \u0026hellip;  Following the pattern, if you require exactly $k$ flips then you had $k-1$ heads followed by a tail, and the odds of this are $(1/2)^k$. The expected value is then \\begin{align*} E \u0026amp;= \\sum_{outcome} ({\\rm outcome})*({\\rm odds}) \\newline \u0026amp;= \\ \\ \\sum_{k = 1}^{\\infty} k (1/2)^k. \\end{align*} This is a known variant of the geometric series. The general sum can be expressed as \\begin{align*} \\sum_{k = 1}^{\\infty} kx^k = \\frac{x}{(x-1)^2} \\end{align*} valid for all $|x| \u0026lt; 1$. Setting $x = 1/2$ then gives $E = 2$.\nSo, our initial guess was right: you\u0026rsquo;d expect to need 2 flips before getting a tail.\n\u0026nbsp;\nExpected number of flips to get two consecutive tails We can again compute the expected value $E$. This time, we will use a trick to simplify the calculations: If you miss the first flip, the expected number of flips needed increases by one!\nLet\u0026rsquo;s run over the possible scenarios for the first two flips:\n If the first flip is head, then your expected value at this point is $E+1$. This happens with probability 1/2. If the first flip is tail, and then head, then your expected value becomes $E+2$. This happens with probability 1/4. If your first two flips are tail, then you are done. This happens with probability 1/4.  Since this covers all outcomes, the expected value can then be computed as the sum of expected values for each outcome, weighted by the odds of that outcome. This gives\n\\begin{align*} E \u0026amp;= \\frac{1}{2}(E+1) + \\frac{1}{4}(E+2) + \\frac{1}{4}(2). \\end{align*} This simplifies to \\begin{align*} E \u0026amp;= (\\frac{1}{2} + \\frac{1}{4})E + (\\frac{1}{2} + \\frac{1}{4} + \\frac{1}{2}) \\newline \u0026amp;= \\frac{3}{4}E + \\frac{6}{4}. \\end{align*} Solving gives $E=6$. So we\u0026rsquo;d expect about 6 flips before seeing two consecutive tails.\n\u0026nbsp;\nExpected number of flips to get N consecutive tails We can use the same trick as above to go from $2$ to $N$. This time, we look at the possible scenarios for the first N flips and the expected value of each scenario.\nIf your coin flips start as:\n a head, then your expected value becomes $E+1$. The odds of this are 1/2. a tail, followed by a head, your expected value becomes $E+2$. The odds are 1/4. tail, tail, then head, then your expected value becomes $E+3$. The odds are 1/8. \u0026hellip; (N-1) tails, then head, then your expected value becomes $E+N$. The odds are (1/2)^N. N tails, then you\u0026rsquo;re done. The odds of this are also (1/2)^N.  Summing up the expected value of each outcome, we obtain\n\\begin{align*} E \u0026amp;= \\frac{1}{2}(E+1) + \\frac{1}{4}(E+2) + \\frac{1}{8}(E+3) + \u0026hellip; + \\frac{1}{2^N}(E+N) + \\frac{1}{2^N} N. \\end{align*}\nCollecting terms, this simplifies to \\begin{align*} E = \\left(\\sum_{k=1}^N \\frac{1}{2^k}\\right) E + \\left( \\sum_{k=1}^N \\frac{k}{2^k} \\right) + \\frac{1}{2^N} N.\n\\end{align*}\nAs before, these sums are variants of the geometric series. Their general values are \\begin{align*} \u0026amp;\\sum_{k=1}^N x^k = \\frac{x^{N+1} - 1}{x-1}\\newline \u0026amp;\\sum_{k=1}^N kx^k = \\frac{(Nx - N - 1)x^{N+1} + x}{(x-1)^2}. \\end{align*}\nSetting $x = \\frac{1}{2}$ and solving for $E$ gives $E = 2^{N+1} - 2$. Hence we should expect to flip on average $2^{N+1} - 2$ coins before getting $N$ consecutive tails.\n\u0026nbsp;\nBack to Space Basketball We can use a similar logic to solve our original problem. In the coin problem the odds of success/failure were 50%/50%, while our free throw success/failure rates were 30%/70%.\nIn general, denote by\n p: the chance of success for free throws. q: the chance of failure for free throws.  In our setting these values are $p = 0.30$ and $q = 1 - p = 0.70$.\nLet\u0026rsquo;s compute the expected number E of free throws needed to make N shots in a row. Like the previous case, we can break down the expected value in terms of what happens on the first $N$ throws:\n If the 1st throw is a failure, then the expectation becomes $E+1$. The odds of this are $q$. If we get success, then failure, the expectation is $E+2$. The odds of this are $pq$. If we get success, success, then failure, the expectation becomes $E+3$ with odds $p^2q$. \u0026hellip; If we get (N-1) successes then failure, the expectation is $E+N$ with odds $p^{N-1}q$. If we get N successes, we are done. The odds of this are $p^N$.  The expected value $E$ is again the sum of expectations over all outcomes, weighted by the odds of each outcome. In other words,\n$$ E = q(E+1) + pq(E+2) + p^2q(E+3) + \u0026hellip; + p^{N-1}q(E+N) + p^N N. $$\nSolving this using the same technique as above (and remembering $q = 1 - p$), we obtain\n$$ E = \\frac{1-p^N}{p^N(1-p)}. $$\nNow, our free throw accuracy is 30%, and so our expected number of shots before getting N=30 consecutive shots in is roughly\n$$ E \\approx 6.93848×10^{15} = 6,938,480,000,000,000.$$\nA non-starter.\nKawhi Leonard is listed as having free throw accuracy of 88.9% this season. His expected number of shots to make 30 in a row, for comparison, is about\n$$ E \\approx 298 $$\nwhich is way more manageable.\nWe can get professional player stats for the 2020 Basketball season from various sites. For example, the average NBA player had a free throw success rate of 77.1% in the last season. Collecting a few players, we can see how long each player needs to make 30 shots in a row.\n   player free throw success rate expected value     Devin Booker 0.916 154   Kawhi Leonard 0.889 298   Kyle Lowry 0.861 634   Average NBA Player 0.771 10675   LeBron James 0.697 166558    ","date":1593993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593993600,"objectID":"baab258067c53638cf1bb65437efcefc","permalink":"https://vgelinas.github.io/post/space-basketball-probability/","publishdate":"2020-07-06T00:00:00Z","relpermalink":"/post/space-basketball-probability/","section":"post","summary":"A fun probability question from an xkcd comic","tags":null,"title":"Space Basketball Probability","type":"post"},{"authors":null,"categories":null,"content":"The Flask python library can help create quick and minimalist dashboards. In this project, I created a mini-dashboard with a mix of Python, SQL and Flask to help me visualise smartwear data. This is the result:\nProject structure The project layers are as follows:\n  Fitbit API wrapper: A python class that handles interactions with the Fitbit REST API, dealing with the token authentication/refresh process.\n  Database: A MySQL database holding the Fitbit API authentication credentials, as well as tables of fitness data (continuously updated).\n  Server: A python class that handles communicating with the Database, and fetching/updating Fitbit API credentials.\n  Plotting module: Creates plots from database tables.\n  Flask app: Serve in a beautiful webpage and voilà!\n  See the project code in the GitHub repository.\n","date":1592870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592870400,"objectID":"e362d9b199a98978e5fa6e1a14e8512b","permalink":"https://vgelinas.github.io/post/flask-dashboard/","publishdate":"2020-06-23T00:00:00Z","relpermalink":"/post/flask-dashboard/","section":"post","summary":"Making a simple dashboard with Python, SQL and Flask.","tags":null,"title":"A Simple Dashboard in Flask","type":"post"},{"authors":null,"categories":null,"content":"In this notebook we run through a basic prediction task using the Census Income UCI Dataset. The task is to predict whether income exceeds $50k/year based on census data.\nDataset information This dataset consists of 48842 instances and 15 features. The features take on a mix of categorical/numerical values:\n age: 16+ (continuous). workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked. fnlwgt: Final weight, see below (continuous). education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. education_num: Total number of years of education (continuous). marital_status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces. relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried. race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black. sex: Female, Male. capital_gain: continuous. capital_loss: continuous. hours_per_week: Number of hours worked per week (continuous). native_country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad\u0026amp;Tobago, Peru, Hong, Holand-Netherlands. Income: \u0026gt;50k, \u0026lt;=50k.  The dataset is already split into train-test sets of sizes (2/3, 1/3). Both sets contain missing values, denoted by \u0026lsquo;?'.\nDescription of fnlwgt (final weight): The UCI repository lists the additional information for the fnlwgt column:\n\u0026ldquo;The weights on the Current Population Survey (CPS) files are controlled to independent estimates of the civilian noninstitutional population of the US. These are prepared monthly for us by Population Division here at the Census Bureau. We use 3 sets of controls. These are:\n A single cell estimate of the population 16+ for each state. Controls for Hispanic Origin by age and sex. Controls by Race, age and sex.\u0026rdquo;  Dependencies  Python 3+ Pandas, numpy, scikit-learn  # Standard libraries import pandas as pd import numpy as np # Train and evaluate models import sklearn from sklearn.model_selection import train_test_split from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score from sklearn.model_selection import GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingClassifier  1. Data cleaning and preprocessing To be consistent in cleaning our data, we will merge the train and test sets into a full dataset and separate it later for model validation.\n# Load data columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income'] data_train = pd.read_csv(\u0026quot;./data/adult.data\u0026quot;, sep=', ', header=None, names=columns, engine='python') data_test = pd.read_csv(\u0026quot;./data/adult.test\u0026quot;, sep=', ', header=None, skiprows=1, names=columns, engine='python') display(data_train.head(3)) display(data_test.head(3))   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  age workclass fnlwgt education education_num marital_status occupation relationship race sex capital_gain capital_loss hours_per_week native_country income     0 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States \u0026lt;=50K   1 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States \u0026lt;=50K   2 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States \u0026lt;=50K      .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  age workclass fnlwgt education education_num marital_status occupation relationship race sex capital_gain capital_loss hours_per_week native_country income     0 25 Private 226802 11th 7 Never-married Machine-op-inspct Own-child Black Male 0 0 40 United-States \u0026lt;=50K.   1 38 Private 89814 HS-grad 9 Married-civ-spouse Farming-fishing Husband White Male 0 0 50 United-States \u0026lt;=50K.   2 28 Local-gov 336951 Assoc-acdm 12 Married-civ-spouse Protective-serv Husband White Male 0 0 40 United-States \u0026gt;50K.     # Merge datasets data = pd.concat([data_train, data_test]) data   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  age workclass fnlwgt education education_num marital_status occupation relationship race sex capital_gain capital_loss hours_per_week native_country income     0 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States \u0026lt;=50K   1 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States \u0026lt;=50K   2 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States \u0026lt;=50K   3 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States \u0026lt;=50K   4 28 Private 338409 Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba \u0026lt;=50K   ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...   16276 39 Private 215419 Bachelors 13 Divorced Prof-specialty Not-in-family White Female 0 0 36 United-States \u0026lt;=50K.   16277 64 ? 321403 HS-grad 9 Widowed ? Other-relative Black Male 0 0 40 United-States \u0026lt;=50K.   16278 38 Private 374983 Bachelors 13 Married-civ-spouse Prof-specialty Husband White Male 0 0 50 United-States \u0026lt;=50K.   16279 44 Private 83891 Bachelors 13 Divorced Adm-clerical Own-child Asian-Pac-Islander Male 5455 0 40 United-States \u0026lt;=50K.   16280 35 Self-emp-inc 182148 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 60 United-States \u0026gt;50K.    48842 rows × 15 columns\n 1.1. Relevant features The value of the fnlwgt (final weight) column should have no predictive power, at least without further processing. We will drop this column.\n# Drop fnlwgt column data.drop('fnlwgt', axis=1, inplace=True)  1.2. Missing values Missing values are represented as the string \u0026lsquo;?'. We first replace them with NaNs.\n# Replace '?' with NaN data.replace('?', np.nan, inplace=True)  # Count missing values data.isna().sum()  age 0 workclass 2799 education 0 education_num 0 marital_status 0 occupation 2809 relationship 0 race 0 sex 0 capital_gain 0 capital_loss 0 hours_per_week 0 native_country 857 income 0 dtype: int64  The missing values are all within categorical variables. Since we can\u0026rsquo;t usefully infer them, we drop each row with a missing value.\ndata.dropna(axis=0, inplace=True)  Finally, reset index for convenience and store full dataset.\n# reset index data.reset_index(drop=True, inplace=True)  # Store full dataset data.to_csv(\u0026quot;./data/adult.csv\u0026quot;, index=False)  1.3. Encode target feature as integers The target feature (income column) consists of strings of the form below:\ndata.income  0 \u0026lt;=50K 1 \u0026lt;=50K 2 \u0026lt;=50K 3 \u0026lt;=50K 4 \u0026lt;=50K ... 45217 \u0026lt;=50K. 45218 \u0026lt;=50K. 45219 \u0026lt;=50K. 45220 \u0026lt;=50K. 45221 \u0026gt;50K. Name: income, Length: 45222, dtype: object  We encode income below 50k as 0, and above 50k as 1.\ndata.income = data.income.str.strip(\u0026quot;.\u0026quot;)  data.income = data.income.map({\u0026quot;\u0026lt;=50K\u0026quot;: 0, \u0026quot;\u0026gt;50K\u0026quot;: 1})  data.income.value_counts()  0 34014 1 11208 Name: income, dtype: int64  data   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  age workclass education education_num marital_status occupation relationship race sex capital_gain capital_loss hours_per_week native_country income     0 39 State-gov Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States 0   1 50 Self-emp-not-inc Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States 0   2 38 Private HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States 0   3 53 Private 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States 0   4 28 Private Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba 0   ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...   45217 33 Private Bachelors 13 Never-married Prof-specialty Own-child White Male 0 0 40 United-States 0   45218 39 Private Bachelors 13 Divorced Prof-specialty Not-in-family White Female 0 0 36 United-States 0   45219 38 Private Bachelors 13 Married-civ-spouse Prof-specialty Husband White Male 0 0 50 United-States 0   45220 44 Private Bachelors 13 Divorced Adm-clerical Own-child Asian-Pac-Islander Male 5455 0 40 United-States 0   45221 35 Self-emp-inc Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 60 United-States 1    45222 rows × 14 columns\n 1.4. One-Hot encode the categorical values data = pd.get_dummies(data) data.shape  (45222, 104)  data.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  age education_num capital_gain capital_loss hours_per_week income workclass_Federal-gov workclass_Local-gov workclass_Private workclass_Self-emp-inc ... native_country_Portugal native_country_Puerto-Rico native_country_Scotland native_country_South native_country_Taiwan native_country_Thailand native_country_Trinadad\u0026amp;Tobago native_country_United-States native_country_Vietnam native_country_Yugoslavia     0 39 13 2174 0 40 0 0 0 0 0 ... 0 0 0 0 0 0 0 1 0 0   1 50 13 0 0 13 0 0 0 0 0 ... 0 0 0 0 0 0 0 1 0 0   2 38 9 0 0 40 0 0 0 1 0 ... 0 0 0 0 0 0 0 1 0 0    3 rows × 104 columns\n 1.5. Feature scaling X, y = data.drop('income', axis=1), data.income  scaler = StandardScaler()  scaler.fit(X.loc[:, 'age': 'hours_per_week']) X.loc[:, 'age': 'hours_per_week'] = scaler.transform(X.loc[:, 'age': 'hours_per_week'])  X.head(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  age education_num capital_gain capital_loss hours_per_week workclass_Federal-gov workclass_Local-gov workclass_Private workclass_Self-emp-inc workclass_Self-emp-not-inc ... native_country_Portugal native_country_Puerto-Rico native_country_Scotland native_country_South native_country_Taiwan native_country_Thailand native_country_Trinadad\u0026amp;Tobago native_country_United-States native_country_Vietnam native_country_Yugoslavia     0 0.034201 1.128753 0.142888 -0.21878 -0.078120 0 0 0 0 0 ... 0 0 0 0 0 0 0 1 0 0   1 0.866417 1.128753 -0.146733 -0.21878 -2.326738 0 0 0 0 1 ... 0 0 0 0 0 0 0 1 0 0   2 -0.041455 -0.438122 -0.146733 -0.21878 -0.078120 0 0 1 0 0 ... 0 0 0 0 0 0 0 1 0 0   3 1.093385 -1.221559 -0.146733 -0.21878 -0.078120 0 0 1 0 0 ... 0 0 0 0 0 0 0 1 0 0   4 -0.798015 1.128753 -0.146733 -0.21878 -0.078120 0 0 1 0 0 ... 0 0 0 0 0 0 0 0 0 0    5 rows × 103 columns\n 2. Data modelling 2.1. Train test split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)  2.2. Gradient Boosted Trees We will use gradient boosted trees, with parameters tuned via grid search cross-validation. First, let\u0026rsquo;s look at the cross-validation scores with default values to get a baseline.\n# Gradient boosted trees with default values gbt = GradientBoostingClassifier(random_state=0) kfold = KFold(n_splits=5, shuffle=True, random_state=0) scores = cross_val_score(gbt, X, y, cv=kfold) print(\u0026quot;Cross-validation scores: mean {:.3f}, std {:.3f}\u0026quot;.format(scores.mean(), scores.std()))  Cross-validation scores: mean 0.863, std 0.001  We\u0026rsquo;ll try tuning the following parameters:\n max_depth (defaults to 3) n_estimators (defaults to 100)  We also keep random_state=0 as above for reproducibility.\n# Parameter grid to optimise over param_grid = { 'random_state': [0], 'max_depth': [2, 3, 4, 5], 'n_estimators': [50, 100, 150, 200, 250, 300], } # Train our model grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5) grid_search.fit(X_train, y_train) # Get scores and model information print(\u0026quot;Best parameters: {}\u0026quot;.format(grid_search.best_params_)) print(\u0026quot;Best cross-validation score: {:.3f}\u0026quot;.format(grid_search.best_score_)) print(\u0026quot;Best estimator:\\n{}\u0026quot;.format(grid_search.best_estimator_))  Best parameters: {'max_depth': 5, 'n_estimators': 200, 'random_state': 0} Best cross-validation score: 0.869 Best estimator: GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance', max_depth=5, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, presort='deprecated', random_state=0, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False)  With some tuning of parameters, we see that our accuracy should raise to ~0.869 using max_depth = 5 and n_estimators=200. Let\u0026rsquo;s try our newly fitted model on the test set.\n# Get train and test set scores print(\u0026quot;Train set score: {:.3f}\u0026quot;.format(grid_search.score(X_train, y_train))) print(\u0026quot;Test set score: {:.3f}\u0026quot;.format(grid_search.score(X_test, y_test)))  Train set score: 0.884 Test set score: 0.870  Our gradient boosted tree model then achieves a test set accuracy of 87%.\n2.3. Further tuning Should we wish to tune parameters further, it would be useful to look at the grid search history to see the accuracy as a function of parameters. This can be extracted by the cv_results_ method, as seen below.\nresults = pd.DataFrame(grid_search.cv_results_) results.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mean_fit_time std_fit_time mean_score_time std_score_time param_max_depth param_n_estimators param_random_state params split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score mean_test_score std_test_score rank_test_score     0 1.160780 0.037878 0.007860 0.000821 2 50 0 {'max_depth': 2, 'n_estimators': 50, 'random_s... 0.847583 0.855079 0.854931 0.850361 0.852573 0.852105 0.002848 24   1 2.187243 0.120700 0.009864 0.000370 2 100 0 {'max_depth': 2, 'n_estimators': 100, 'random_... 0.851562 0.861566 0.857880 0.853457 0.857585 0.856410 0.003531 23   2 3.099185 0.006702 0.011799 0.000122 2 150 0 {'max_depth': 2, 'n_estimators': 150, 'random_... 0.855248 0.863777 0.859502 0.856848 0.858322 0.858739 0.002895 21   3 4.104170 0.012035 0.013877 0.000131 2 200 0 {'max_depth': 2, 'n_estimators': 200, 'random_... 0.858343 0.867168 0.860386 0.859502 0.860829 0.861246 0.003081 20   4 5.089706 0.007495 0.015999 0.000137 2 250 0 {'max_depth': 2, 'n_estimators': 250, 'random_... 0.859080 0.868790 0.862893 0.860091 0.861566 0.862484 0.003408 17     However to keep this post short, we will leave it at that.\n","date":1591056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591056000,"objectID":"199eca66d6b5e0bcbbe671fdb0aa1727","permalink":"https://vgelinas.github.io/post/predicting-income-from-census-data/","publishdate":"2020-06-02T00:00:00Z","relpermalink":"/post/predicting-income-from-census-data/","section":"post","summary":"In this jupyter notebook we run through a basic prediction task using the Census Income UCI Dataset.","tags":null,"title":"Income From Census Data","type":"post"},{"authors":null,"categories":null,"content":"In this project we will explore some Fitbit activity data pulled via orcasgit\u0026rsquo;s python-fitbit api. We will go through the following steps:\n Data collection Data cleaning Data visualisation  Dependencies  Python 3+ The python-fitbit package The ratelimit package The datetime, json, matplotlib and pandas standard libraries  Let\u0026rsquo;s load our packages.\nimport fitbit import json import pandas as pd import matplotlib.pyplot as plt from datetime import datetime, timedelta from ratelimit import limits, sleep_and_retry %matplotlib inline  1. Data Collection We do this in two steps:\n We first access the API via python-fitbit, dealing with the necessary authentication steps. We then sample some responses, and build datasets by querying over a range of dates.  1.1. Authentication setup To collect personal data, we first need to set-up a Fitbit app, and to collect the client_id and client_secret for this app. For this project I\u0026rsquo;ve chosen to keep these in a credentials.json file stored in a dedicated subfolder named \u0026lsquo;oauth\u0026rsquo;, but just make sure you have these on hand.\n!cat oauth/credentials.json  {\u0026quot;client_id\u0026quot;: \u0026quot;YOUR_CLIENT_ID\u0026quot;, \u0026quot;client_secret\u0026quot;: \u0026quot;YOUR_CLIENT_SECRET\u0026quot;}\r We also need tokens for authentication. We need:\n An access token. A refresh token. An expiration time for the access token (the refresh token never expires).  These can be obtained by going to the Manage my apps section on the Fitbit website, selecting your app and navigating to \u0026ldquo;OAuth 2.0 tutorial page\u0026rdquo;. Alternatively, you can run the script \u0026ldquo;gather_keys_oauth2.py\u0026rdquo; from the python-fitbit github page, in which case you should set your Fitbit app\u0026rsquo;s callback URL to https://127.0.0.1:8080/.\nThe access token serves to authenticate and typically expires after ~8 hours. The refresh token is then used to obtain a new pair (access_token, refresh_token) from the API. Similar to above, I chose to store these in a json file named \u0026lsquo;tokens\u0026rsquo;.\n!cat oauth/tokens.json  {\u0026quot;access_token\u0026quot;: \u0026quot;YOUR_ACCESS_TOKEN\u0026quot;, \u0026quot;expires_in\u0026quot;: 28800, \u0026quot;refresh_token\u0026quot;: \u0026quot;YOUR_REFRESH_TOKEN\u0026quot;, \u0026quot;scope\u0026quot;: [\u0026quot;location\u0026quot;, \u0026quot;heartrate\u0026quot;, \u0026quot;social\u0026quot;, \u0026quot;weight\u0026quot;, \u0026quot;settings\u0026quot;, \u0026quot;profile\u0026quot;, \u0026quot;nutrition\u0026quot;, \u0026quot;activity\u0026quot;, \u0026quot;sleep\u0026quot;], \u0026quot;token_type\u0026quot;: \u0026quot;Bearer\u0026quot;, \u0026quot;user_id\u0026quot;: \u0026quot;USER_ID\u0026quot;, \u0026quot;expires_at\u0026quot;: 1590460521.3563423}\r The only important keys above are \u0026ldquo;access_token\u0026rdquo;, \u0026ldquo;refresh_token\u0026rdquo; and \u0026ldquo;expires_at\u0026rdquo; (the rest corresponds to optional arguments).\nNext up, the code below instantiates a fitbit client which will handle API calls for us. We pass along the credentials and tokens as arguments, and we also pass a \u0026ldquo;token refresh\u0026rdquo; function which will store the new (access_token, refresh_token) pair sent by the API whenever the first one expires.\n# Load credentials with open(\u0026quot;./oauth/credentials.json\u0026quot;, \u0026quot;r\u0026quot;) as f: credentials = json.load(f) # Load tokens with open(\u0026quot;./oauth/tokens.json\u0026quot;, \u0026quot;r\u0026quot;) as f: tokens = json.load(f) client_id = credentials['client_id'] client_secret = credentials['client_secret'] access_token = tokens['access_token'] refresh_token = tokens['refresh_token'] expires_at = tokens['expires_at'] # Token refresh method def refresh_callback(token): \u0026quot;\u0026quot;\u0026quot; Called when the OAuth token has been refreshed \u0026quot;\u0026quot;\u0026quot; with open(\u0026quot;./oauth/tokens.json\u0026quot;, \u0026quot;w\u0026quot;) as f: json.dump(token, f) # Initialise client client = fitbit.Fitbit(client_id=client_id, client_secret=client_secret, access_token=access_token, refresh_token=refresh_token, refresh_cb=refresh_callback)  The first time this is called you should be served an authorisation page for authentication, but afterwards the refresh token song \u0026amp; dance should handle this in the background, and we won\u0026rsquo;t need to set it up again unless you lose your tokens.\n1.2. A first look at the response data The python-fitbit api supports the methods listed here. For example, we could call:\n client.sleep, to get basic sleep data (bed time and wake time, time awake at night, \u0026hellip;). client.activities, to get timestamps for activities (walking, running, cycling, \u0026hellip;) and summary data (number of steps, minutes active, \u0026hellip;). client.intraday_time_series, to get granular data on various activities (such as heart rate or steps rate for every minute of the day).  We\u0026rsquo;ll be interested in the activities and intraday steps data. Now, let\u0026rsquo;s take a look at the response for one date, say May 1st.\n# Get activity data for May 1st # The API takes a date formatted as 'YYYY-MM-DD' date = '2020-05-01' activities_response = client.activities(date=date) # Display response activities_response  {'activities': [{'activityId': 90013, 'activityParentId': 90013, 'activityParentName': 'Walk', 'calories': 302, 'description': 'Walking less than 2 mph, strolling very slowly', 'duration': 2714000, 'hasStartTime': True, 'isFavorite': False, 'lastModified': '2020-05-01T13:10:18.000Z', 'logId': 30758911349, 'name': 'Walk', 'startDate': '2020-05-01', 'startTime': '08:20', 'steps': 4000}], 'goals': {'activeMinutes': 30, 'caloriesOut': 2745, 'distance': 5, 'steps': 12500}, 'summary': {'activeScore': -1, 'activityCalories': 1379, 'caloriesBMR': 1659, 'caloriesOut': 2826, 'distances': [{'activity': 'total', 'distance': 4.69}, {'activity': 'tracker', 'distance': 4.69}, {'activity': 'loggedActivities', 'distance': 0}, {'activity': 'veryActive', 'distance': 2.6}, {'activity': 'moderatelyActive', 'distance': 0.4}, {'activity': 'lightlyActive', 'distance': 1.68}, {'activity': 'sedentaryActive', 'distance': 0}], 'fairlyActiveMinutes': 25, 'heartRateZones': [{'caloriesOut': 1916.74877, 'max': 94, 'min': 30, 'minutes': 1255, 'name': 'Out of Range'}, {'caloriesOut': 775.70893, 'max': 132, 'min': 94, 'minutes': 137, 'name': 'Fat Burn'}, {'caloriesOut': 81.56868, 'max': 160, 'min': 132, 'minutes': 8, 'name': 'Cardio'}, {'caloriesOut': 0, 'max': 220, 'min': 160, 'minutes': 0, 'name': 'Peak'}], 'lightlyActiveMinutes': 210, 'marginalCalories': 828, 'restingHeartRate': 59, 'sedentaryMinutes': 578, 'steps': 9887, 'veryActiveMinutes': 49}}  Let\u0026rsquo;s look at the type of the response object.\ntype(activities_response)  dict  The response consists of nested dictionaries. We\u0026rsquo;ll extract two datasets from the \u0026lsquo;activities\u0026rsquo; and \u0026lsquo;summary\u0026rsquo; keys.\n# Get activities dataset activities = activities_response['activities'] activities = pd.DataFrame(activities) activities   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  activityId activityParentId activityParentName calories description duration hasStartTime isFavorite lastModified logId name startDate startTime steps     0 90013 90013 Walk 302 Walking less than 2 mph, strolling very slowly 2714000 True False 2020-05-01T13:10:18.000Z 30758911349 Walk 2020-05-01 08:20 4000     # Get summary dataset summary = activities_response['summary'] # Remove sub-dictionaries del summary['distances'] del summary['heartRateZones'] summary = pd.DataFrame(summary, index=[0]) # all values are scalars, must pass an index summary   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  activeScore activityCalories caloriesBMR caloriesOut fairlyActiveMinutes lightlyActiveMinutes marginalCalories restingHeartRate sedentaryMinutes steps veryActiveMinutes     0 -1 1379 1659 2826 25 210 828 59 578 9887 49     Next, let\u0026rsquo;s look at the intraday step data.\n# Get intraday steps data steps_response = client.intraday_time_series('activities/steps', base_date=date, detail_level=\u0026quot;1min\u0026quot;) # Extract dataset from response object steps = steps_response['activities-steps-intraday']['dataset'] # Display dataset steps = pd.DataFrame(steps) steps   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  time value     0 00:00:00 0   1 00:01:00 0   2 00:02:00 0   3 00:03:00 0   4 00:04:00 0   ... ... ...   1435 23:55:00 0   1436 23:56:00 0   1437 23:57:00 0   1438 23:58:00 0   1439 23:59:00 0    1440 rows × 2 columns\n We get the minute-by-minute count of steps on that day. Let\u0026rsquo;s take a quick look at a plot.\nsteps.plot() plt.show()  1.3. Collect activity and intraday steps data since October 1st. We can now build our datasets, which will consists of general activity data and intraday steps data from October 1st to yesterday. We will:\n Produce a list of dates in \u0026lsquo;YYYY-MM-DD\u0026rsquo; string format for our queries. Query the API for each date, extracting our \u0026lsquo;activities\u0026rsquo;, \u0026lsquo;summary\u0026rsquo; and \u0026lsquo;steps\u0026rsquo; datasets from the response. Limit our query rate to 150/hour (since this is the Fitbit API rate limit). Combine and store the results.  First, let\u0026rsquo;s get a list of dates. We can use the pandas date_range method to produce a list of datetime objects, and format them using the strftime method.\n# Get date range from October 1st to yesterday start = pd.to_datetime(\u0026quot;2019-10-01\u0026quot;) date_range = pd.date_range(start=start, end=datetime.today() - timedelta(days=1)) date_range = [datetime.strftime(date, \u0026quot;%Y-%m-%d\u0026quot;) for date in date_range] date_range[-5:]  ['2020-05-14', '2020-05-15', '2020-05-16', '2020-05-17', '2020-05-18']  Next, we query the API for each date in date_range.\nAs seen when we first took a look at the response data, we actually make two API calls per date (i.e. client.activities and client.intraday_time_series). Since the Fitbit API has a rate limit of 150 calls/hour, we should query at most 75 dates an hour. We can accomplish this via the ratelimit package, which lets you limit the number of times a function is called over a time period.\nFinally, we call the API for each day, timestamp the resulting datasets, and store the total in csv files locally. We do this for each of the \u0026lsquo;activities\u0026rsquo;, \u0026lsquo;summary\u0026rsquo; and \u0026lsquo;steps\u0026rsquo; datasets. The script below accomplishes this.\n# We define a data collection function, and we use the ratelimit package # to limit our function to 150 API calls / hour. ONE_HOUR = 3600 @sleep_and_retry @limits(calls=70, period=ONE_HOUR) def call_fitbit_api(date): \u0026quot;\u0026quot;\u0026quot; Call the Fitbit API for given date in format 'YYYY-MM-DD', Return tuple (activities, summary, steps) of dataframes \u0026quot;\u0026quot;\u0026quot; # Call API twice to get activities and steps responses activities_data = client.activities(date=date) steps_data = client.intraday_time_series('activities/steps', base_date=date, detail_level='1min') # Get activities dataset activities = activities_data['activities'] activities = pd.DataFrame(activities) # Get summary dataset summary = activities_data['summary'] del summary['distances'] del summary['heartRateZones'] summary = pd.DataFrame(summary, index=[0]) # Get steps intraday dataset steps = steps_data['activities-steps-intraday']['dataset'] steps = pd.DataFrame(steps) # Add a date column activities['date'] = [date for i in activities.index] summary['date'] = [date] steps['date'] = [date for i in steps.index] return activities, summary, steps def get_fitbit_data(date_range): \u0026quot;\u0026quot;\u0026quot; Collect 'activities', 'summary' and 'steps' datasets over given dates Store as CSV files with format RESOURCE_DATE_to_DATE.csv \u0026quot;\u0026quot;\u0026quot; daily_df = { 'activities': [], 'summary': [], 'steps': [] } for date in date_range: # Call API and get three datasets activities, summary, steps = call_fitbit_api(date) # Append to previous datasets daily_df['activities'].append(activities) daily_df['summary'].append(summary) daily_df['steps'].append(steps) # Store total dataset as file with format \u0026quot;resource_DATE_to_DATE.csv\u0026quot; start, end = date_range[0], date_range[-1] for resource in daily_df: df = pd.concat(daily_df[resource], ignore_index=True) df.to_csv(\u0026quot;./data/raw/{}_{}_to_{}.csv\u0026quot;.format(resource, start, end), index=False)  # Collect Fitbit 'activities', 'summary' and 'steps' data since October 1st, 2019 get_fitbit_data(date_range=date_range)  2. Cleaning the data It\u0026rsquo;s time to take a look at each dataset.\n2.1. The activity dataset activities = pd.read_csv(\u0026quot;./data/raw/activities_2019-10-01_to_2020-05-18.csv\u0026quot;) activities.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  activityId activityParentId activityParentName calories description duration hasStartTime isFavorite lastModified logId name startDate startTime steps date distance     0 90013.0 90013.0 Walk 245.0 Walking less than 2 mph, strolling very slowly 1843000.0 True False 2019-10-01T15:36:45.000Z 2.568779e+10 Walk 2019-10-01 10:46 2059.0 2019-10-01 NaN   1 90013.0 90013.0 Walk 194.0 Walking less than 2 mph, strolling very slowly 1792000.0 True False 2019-10-01T17:44:08.000Z 2.569041e+10 Walk 2019-10-01 11:52 1977.0 2019-10-01 NaN   2 90013.0 90013.0 Walk 165.0 Walking less than 2 mph, strolling very slowly 1485000.0 True False 2019-10-02T02:00:43.000Z 2.570412e+10 Walk 2019-10-01 18:20 1443.0 2019-10-01 NaN     activities.shape  (354, 16)  We have 16 columns, many of which contain logging information, True/False data or duplicate information which is not useful to us. Let\u0026rsquo;s drop these.\ndrop_columns = ['activityId', 'activityParentId', 'activityParentName', 'hasStartTime', 'isFavorite', 'lastModified', 'logId', 'startDate'] activities.drop(drop_columns, axis=1, inplace=True)  Next, let\u0026rsquo;s look at the distance column. Consulting the documentation, we see that this means logged distance. Since I\u0026rsquo;ve rarely used the feature, it looks like the column consists mostly of missing values.\nactivities.distance.value_counts()  0.310468 1 0.773283 1 Name: distance, dtype: int64  Since we only have 2 non-missing values in 354 rows, let\u0026rsquo;s drop the column.\nactivities.drop('distance', axis=1, inplace=True)  Some of the column names are in camelCase. Let\u0026rsquo;s rename them to Python\u0026rsquo;s favored snake_case.\nactivities.rename(columns={'startTime': 'start_time'}, inplace=True) activities.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  calories description duration name start_time steps date     0 245.0 Walking less than 2 mph, strolling very slowly 1843000.0 Walk 10:46 2059.0 2019-10-01   1 194.0 Walking less than 2 mph, strolling very slowly 1792000.0 Walk 11:52 1977.0 2019-10-01   2 165.0 Walking less than 2 mph, strolling very slowly 1485000.0 Walk 18:20 1443.0 2019-10-01     The duration column isn\u0026rsquo;t easy to parse and is missing units. The Fitbit api documentation lists the duration as being in millisecond, so let\u0026rsquo;s put it in minutes and rename accordingly.\nactivities.duration = activities.duration.apply(lambda x: round(x/60000)) activities.rename(columns={'duration': 'duration_min'}, inplace=True) activities.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  calories description duration_min name start_time steps date     0 245.0 Walking less than 2 mph, strolling very slowly 31 Walk 10:46 2059.0 2019-10-01   1 194.0 Walking less than 2 mph, strolling very slowly 30 Walk 11:52 1977.0 2019-10-01   2 165.0 Walking less than 2 mph, strolling very slowly 25 Walk 18:20 1443.0 2019-10-01     To help with analysis, let\u0026rsquo;s format the start_time column as \u0026ldquo;YYYY-MM-DD H:M:S\u0026rdquo; to more easily convert to a datetime object. Since we have the activity duration, we can also add an end_time column.\n# Format start_time column and convert to datetime object activities.start_time = activities.date + \u0026quot; \u0026quot; + activities.start_time + \u0026quot;:00\u0026quot; activities.start_time = pd.to_datetime(activities.start_time) # Create end_time column by adding the duration_min column to start_time activities_duration = activities.duration_min.apply(lambda x: timedelta(minutes=x)) activities['end_time'] = activities.start_time + activities_duration # Display result activities.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  calories description duration_min name start_time steps date end_time     0 245.0 Walking less than 2 mph, strolling very slowly 31 Walk 2019-10-01 10:46:00 2059.0 2019-10-01 2019-10-01 11:17:00   1 194.0 Walking less than 2 mph, strolling very slowly 30 Walk 2019-10-01 11:52:00 1977.0 2019-10-01 2019-10-01 12:22:00   2 165.0 Walking less than 2 mph, strolling very slowly 25 Walk 2019-10-01 18:20:00 1443.0 2019-10-01 2019-10-01 18:45:00     Finally, let\u0026rsquo;s reorder the columns for readability.\n# Reorder columns column_order = ['date', 'name', 'description', 'start_time', 'end_time', 'duration_min', 'steps', 'calories'] activities = activities[column_order] # Store dataset start, end = date_range[0], date_range[-1] activities.to_csv(\u0026quot;./data/tidy/activities_{}_to_{}.csv\u0026quot;.format(start, end), index=False) # Look at end result activities   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  date name description start_time end_time duration_min steps calories     0 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 10:46:00 2019-10-01 11:17:00 31 2059.0 245.0   1 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 11:52:00 2019-10-01 12:22:00 30 1977.0 194.0   2 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 18:20:00 2019-10-01 18:45:00 25 1443.0 165.0   3 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 19:38:00 2019-10-01 20:05:00 27 1624.0 176.0   4 2019-10-02 Walk Walking less than 2 mph, strolling very slowly 2019-10-02 13:38:00 2019-10-02 15:20:00 102 7035.0 552.0   ... ... ... ... ... ... ... ... ...   349 2020-05-14 Walk Walking less than 2 mph, strolling very slowly 2020-05-14 08:22:00 2020-05-14 09:07:00 45 4394.0 361.0   350 2020-05-15 Walk Walking less than 2 mph, strolling very slowly 2020-05-15 12:23:00 2020-05-15 13:59:00 96 9430.0 658.0   351 2020-05-15 Run Running - 5 mph (12 min/mile) 2020-05-15 20:13:00 2020-05-15 20:34:00 21 2865.0 245.0   352 2020-05-16 Walk Walking less than 2 mph, strolling very slowly 2020-05-16 10:29:00 2020-05-16 12:33:00 124 10487.0 855.0   353 2020-05-17 Walk Walking less than 2 mph, strolling very slowly 2020-05-17 11:11:00 2020-05-17 13:15:00 124 10757.0 835.0    354 rows × 8 columns\n 2.2. The summary dataset Now let\u0026rsquo;s take a look at the second dataset.\nsummary = pd.read_csv(\u0026quot;./data/raw/summary_2019-10-01_to_2020-05-18.csv\u0026quot;) summary   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  activeScore activityCalories caloriesBMR caloriesOut fairlyActiveMinutes lightlyActiveMinutes marginalCalories restingHeartRate sedentaryMinutes steps veryActiveMinutes date     0 -1 4097 1663 5018 165 420 2670 58 209 25576 152 2019-10-01   1 -1 1967 1663 3211 27 360 1145 58 379 16471 40 2019-10-02   2 -1 1540 1663 2923 22 251 920 57 674 13510 50 2019-10-03   3 -1 1470 1663 2883 44 163 940 56 659 11443 65 2019-10-04   4 -1 1776 1663 3136 35 178 1110 56 572 18711 120 2019-10-05   ... ... ... ... ... ... ... ... ... ... ... ... ...   226 -1 1179 1659 2614 8 218 680 57 636 8938 40 2020-05-14   227 -1 1589 1659 2988 19 183 1011 56 565 15358 99 2020-05-15   228 -1 1512 1659 2903 44 160 959 55 614 15115 82 2020-05-16   229 -1 1922 1659 3230 40 224 1215 55 399 18880 100 2020-05-17   230 -1 468 1659 2046 8 116 235 54 713 2341 0 2020-05-18    231 rows × 12 columns\n Now, looking around the documentation the activeScore column seems to be an old feature. All values are -1 in our dataset so there\u0026rsquo;s not much loss of information in dropping the column.\n(summary.activeScore == -1).all()  True  summary.drop('activeScore', axis=1, inplace=True) summary.head(2)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  activityCalories caloriesBMR caloriesOut fairlyActiveMinutes lightlyActiveMinutes marginalCalories restingHeartRate sedentaryMinutes steps veryActiveMinutes date     0 4097 1663 5018 165 420 2670 58 209 25576 152 2019-10-01   1 1967 1663 3211 27 360 1145 58 379 16471 40 2019-10-02     Next, we again format all columns to snake_case and reorder for readability.\n# Rename columns to snake_case columns_map = { 'activityCalories': 'activity_calories', 'caloriesBMR': 'calories_BMR', 'caloriesOut': 'calories_out', 'fairlyActiveMinutes': 'fairly_active_minutes', 'lightlyActiveMinutes': 'lightly_active_minutes', 'marginalCalories': 'marginal_calories', 'restingHeartRate': 'resting_heart_rate', 'sedentaryMinutes': 'sedentary_minutes', 'veryActiveMinutes': 'very_active_minutes' } summary.rename(columns=columns_map, inplace=True) # Reorder columns column_order = ['date', 'steps', 'very_active_minutes', 'fairly_active_minutes', 'lightly_active_minutes', 'sedentary_minutes', 'activity_calories', 'marginal_calories', 'calories_out', 'calories_BMR', 'resting_heart_rate'] summary = summary[column_order] # Store dataset start, end = summary.date[0], summary.date[len(summary.index)-1] summary.to_csv(\u0026quot;./data/tidy/summary_{}_to_{}.csv\u0026quot;.format(start, end), index=False) # Look at result summary.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  date steps very_active_minutes fairly_active_minutes lightly_active_minutes sedentary_minutes activity_calories marginal_calories calories_out calories_BMR resting_heart_rate     0 2019-10-01 25576 152 165 420 209 4097 2670 5018 1663 58   1 2019-10-02 16471 40 27 360 379 1967 1145 3211 1663 58   2 2019-10-03 13510 50 22 251 674 1540 920 2923 1663 57     2.3. The steps dataset Finally, we look at the intraday steps dataset.\nsteps = pd.read_csv(\u0026quot;./data/raw/steps_2019-10-01_to_2020-05-18.csv\u0026quot;) steps   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  time value date     0 00:00:00 0 2019-10-01   1 00:01:00 0 2019-10-01   2 00:02:00 0 2019-10-01   3 00:03:00 0 2019-10-01   4 00:04:00 0 2019-10-01   ... ... ... ...   332635 23:55:00 0 2020-05-18   332636 23:56:00 0 2020-05-18   332637 23:57:00 0 2020-05-18   332638 23:58:00 0 2020-05-18   332639 23:59:00 0 2020-05-18    332640 rows × 3 columns\n We can combine the time and date into a single column, in datetime format. We also rename value to the more descriptive \u0026lsquo;stepcount\u0026rsquo;.\n# Combine date and time steps.time = steps.date + \u0026quot; \u0026quot; + steps.time # Rename value to stepcount steps.rename(columns={'value': 'stepcount'}, inplace=True) # Get endpoint dates to store the file start, end = steps.date[0], steps.date[len(steps.index) - 1] # Drop date column and store steps.drop('date', axis=1, inplace=True) steps.to_csv(\u0026quot;./data/tidy/steps_{}_to_{}.csv\u0026quot;.format(start, end), index=False) # Look at end result steps   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  time stepcount     0 2019-10-01 00:00:00 0   1 2019-10-01 00:01:00 0   2 2019-10-01 00:02:00 0   3 2019-10-01 00:03:00 0   4 2019-10-01 00:04:00 0   ... ... ...   332635 2020-05-18 23:55:00 0   332636 2020-05-18 23:56:00 0   332637 2020-05-18 23:57:00 0   332638 2020-05-18 23:58:00 0   332639 2020-05-18 23:59:00 0    332640 rows × 2 columns\n 3. Visualisations We now have clean datasets to explore further. We take a look at visualisations in Part II.\n","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589760000,"objectID":"a636816090865ee734a3f837be4f8c99","permalink":"https://vgelinas.github.io/post/fitbit-data-exploration-part-i/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/post/fitbit-data-exploration-part-i/","section":"post","summary":"In this jupyter notebook we explore some personal Fitbit data. In this first part, we will walk through the data collection and data cleaning process.","tags":null,"title":"Fitbit Data Exploration Part I","type":"post"},{"authors":null,"categories":null,"content":"In Part I we showed how to connect to the Fitbit API via Python, and we built some datasets consisting of activities and intraday steps data from October to today. In this post we walk through some data visualisations, and will take a look in particular at the intraday steps data.\n3.1. Activity statistics per week day Let\u0026rsquo;s compile some statistics based on day of the week. First, let\u0026rsquo;s take a look at summary data.\n# Use parse_dates to interpret our date column as datetime objects summary = pd.read_csv(\u0026quot;./data/tidy/summary_2019-10-01_to_2020-05-18.csv\u0026quot;, parse_dates=['date']) summary   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  date steps very_active_minutes fairly_active_minutes lightly_active_minutes sedentary_minutes activity_calories marginal_calories calories_out calories_BMR resting_heart_rate     0 2019-10-01 25576 152 165 420 209 4097 2670 5018 1663 58   1 2019-10-02 16471 40 27 360 379 1967 1145 3211 1663 58   2 2019-10-03 13510 50 22 251 674 1540 920 2923 1663 57   3 2019-10-04 11443 65 44 163 659 1470 940 2883 1663 56   4 2019-10-05 18711 120 35 178 572 1776 1110 3136 1663 56   ... ... ... ... ... ... ... ... ... ... ... ...   226 2020-05-14 8938 40 8 218 636 1179 680 2614 1659 57   227 2020-05-15 15358 99 19 183 565 1589 1011 2988 1659 56   228 2020-05-16 15115 82 44 160 614 1512 959 2903 1659 55   229 2020-05-17 18880 100 40 224 399 1922 1215 3230 1659 55   230 2020-05-18 2341 0 8 116 713 468 235 2046 1659 54    231 rows × 11 columns\n We can use strftime to convert the date to a week day, and get group statistics per day of the week.\n# Add a weekday column summary['weekday'] = summary.date.apply(lambda x: datetime.strftime(x, \u0026quot;%A\u0026quot;)) # Get statistics per day of the week weekly_statistics = summary.groupby('weekday').describe() # Row indices are days of the week, put them in order row_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'] weekly_statistics = weekly_statistics.loc[row_order, :] # Show results weekly_statistics   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }  \n   steps very_active_minutes ... calories_BMR resting_heart_rate    count mean std min 25% 50% 75% max count mean ... 75% max count mean std min 25% 50% 75% max   weekday                          Monday 33.0 10817.393939 4075.727205 2341.0 7793.0 11663.0 13248.0 17668.0 33.0 50.848485 ... 1663.0 1663.0 33.0 55.030303 1.610218 53.0 54.0 55.0 56.0 59.0   Tuesday 33.0 11202.878788 4575.416680 3250.0 8211.0 10953.0 12994.0 25576.0 33.0 57.666667 ... 1663.0 1663.0 33.0 55.030303 1.878910 52.0 53.0 55.0 56.0 59.0   Wednesday 33.0 11007.939394 3307.219952 4486.0 9083.0 10795.0 13538.0 18402.0 33.0 53.909091 ... 1663.0 1663.0 33.0 55.303030 1.704495 52.0 54.0 55.0 56.0 58.0   Thursday 33.0 10867.212121 2882.811852 4525.0 8938.0 10856.0 13064.0 16621.0 33.0 54.212121 ... 1663.0 1663.0 33.0 55.424242 1.581738 52.0 54.0 56.0 56.0 59.0   Friday 33.0 12526.757576 4179.309722 2028.0 9887.0 12445.0 15358.0 18958.0 33.0 66.454545 ... 1663.0 1663.0 33.0 55.424242 1.581738 52.0 54.0 56.0 56.0 59.0   Saturday 33.0 14507.181818 4188.612206 6315.0 12446.0 15049.0 16924.0 23152.0 33.0 74.151515 ... 1663.0 1663.0 33.0 55.575758 1.581738 53.0 55.0 55.0 56.0 60.0   Sunday 33.0 12488.121212 4849.136191 4560.0 9659.0 11756.0 15944.0 25193.0 33.0 64.636364 ... 1663.0 1663.0 33.0 55.393939 1.456438 53.0 54.0 55.0 56.0 59.0    7 rows × 80 columns\n # Plot the mean and first quartile for number of steps per weekday mean_steps = weekly_statistics.steps[['mean', '25%']] mean_steps.plot(kind='bar') plt.title('Weekly stepcount since October 1st, 2019') plt.ylabel('steps') plt.ylim([0, 18000]) plt.show()  3.2. Visualising walks over the day Let\u0026rsquo;s now look at the steps intraday data.\n# Load in dataset steps = pd.read_csv(\u0026quot;./data/tidy/steps_2019-10-01_to_2020-05-18.csv\u0026quot;, parse_dates=['time']) steps   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  time stepcount     0 2019-10-01 00:00:00 0   1 2019-10-01 00:01:00 0   2 2019-10-01 00:02:00 0   3 2019-10-01 00:03:00 0   4 2019-10-01 00:04:00 0   ... ... ...   332635 2020-05-18 23:55:00 0   332636 2020-05-18 23:56:00 0   332637 2020-05-18 23:57:00 0   332638 2020-05-18 23:58:00 0   332639 2020-05-18 23:59:00 0    332640 rows × 2 columns\n Let\u0026rsquo;s visualise steps intraday data over a given day. We look at May 1st again.\ndate = '2020-05-01' # Restrict to logs for given date day_df = steps[steps.time.apply(lambda x: datetime.strftime(x, \u0026quot;%Y-%m-%d\u0026quot;)) == date].copy() # Restrict to within waking hours start_of_day = pd.to_datetime('2020-05-01 07:00:00') end_of_day = pd.to_datetime('2020-05-01 23:00:00') day_df = day_df[(day_df.time \u0026gt;= start_of_day)\u0026amp;(day_df.time \u0026lt;= end_of_day)] # Convert time back to hr:min:sec format and set as index day_df.time = day_df.time.apply(lambda x: datetime.strftime(x, \u0026quot;%H:%M:%S\u0026quot;)) day_df.set_index('time', inplace=True)  Now let\u0026rsquo;s plot steps during the day on May 1st.\n# Plot steps on May 1st fig, ax = plt.subplots() day_df.rolling(15).mean().plot(ax=ax) # 15 min rolling avg to smooth out noise ax.set_title('Steps on May 1st, 2020') ax.set_xlabel('Time of Day') ax.set_ylabel('Steps per min') plt.show()  Here we can tell which period corresponds to exercise, and which results from general activity, but let\u0026rsquo;s be more systematic about this. We can isolate the steps that result from walks alone and not from general activity. The activity dataset has a start_time and end_time for each activity (walk, run, \u0026hellip;) and we may use these to filter our dataset.\n# Load activities dataset, parsing start_time and end_time columns as datetime objects time_col = ['start_time', 'end_time'] activities = pd.read_csv(\u0026quot;./data/tidy/activities_2019-10-01_to_2020-05-18.csv\u0026quot;, parse_dates=time_col) activities.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  date name description start_time end_time duration_min steps calories     0 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 10:46:00 2019-10-01 11:17:00 31 2059.0 245.0   1 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 11:52:00 2019-10-01 12:22:00 30 1977.0 194.0   2 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 18:20:00 2019-10-01 18:45:00 25 1443.0 165.0     Let\u0026rsquo;s add a column named \u0026lsquo;on_walk\u0026rsquo; to the steps dataset, with a True/False value. For this we cook up a helper function as below:\n# Helper function to filter the intraday steps data by activity type def is_during_activity(t, activity): \u0026quot;\u0026quot;\u0026quot; Takes a datetime object t and activity name Returns True if during activity, else False \u0026quot;\u0026quot;\u0026quot; # Get the activities dataset for that day date = datetime.strftime(t, \u0026quot;%Y-%m-%d\u0026quot;) df = activities[activities.date == date] # Subset to rows which represent activity df = df[df.name == activity] # Check if t is within the bounds of the activity for i in df.index: if df.loc[i, 'start_time'] \u0026lt;= t \u0026lt;= df.loc[i, 'end_time']: return True return False # Add 'on_walk' column to steps dataframe steps['on_walk'] = steps.time.apply(is_during_activity, args=('Walk',))  Let\u0026rsquo;s take a look at the stepcount during walks.\nsteps[steps.on_walk == True]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  time stepcount on_walk     646 2019-10-01 10:46:00 54 True   647 2019-10-01 10:47:00 76 True   648 2019-10-01 10:48:00 97 True   649 2019-10-01 10:49:00 75 True   650 2019-10-01 10:50:00 72 True   ... ... ... ...   330551 2020-05-17 13:11:00 113 True   330552 2020-05-17 13:12:00 92 True   330553 2020-05-17 13:13:00 77 True   330554 2020-05-17 13:14:00 35 True   330555 2020-05-17 13:15:00 43 True    15007 rows × 3 columns\n Using this, we can create a new dataframe consisting of walks stepcount data.\n# Set all steps outside of walks to zero walks = steps.copy() walks.stepcount = walks.stepcount.where(walks.on_walk == True, 0) # Drop 'on_walk' column walks.drop('on_walk', axis=1, inplace=True)  Let\u0026rsquo;s look at May 1st again.\ndate = '2020-05-01' # Restrict to logs for given date day_walks = walks[walks.time.apply(lambda x: datetime.strftime(x, \u0026quot;%Y-%m-%d\u0026quot;)) == date].copy() # Restrict to within waking hours start_of_day = pd.to_datetime('2020-05-01 07:00:00') end_of_day = pd.to_datetime('2020-05-01 23:00:00') day_walks = day_walks[(day_walks.time \u0026gt;= start_of_day)\u0026amp;(day_walks.time \u0026lt;= end_of_day)] # Convert time back to hr:min:sec format and set as index day_walks.time = day_walks.time.apply(lambda x: datetime.strftime(x, \u0026quot;%H:%M:%S\u0026quot;)) day_walks.set_index('time', inplace=True)  # Plot walks on May 1st fig, ax = plt.subplots() day_walks.rolling(15).mean().plot(ax=ax) # 15 min rolling avg to smooth out noise ax.set_title('Steps on May 1st 2020 during a walk') ax.set_xlabel('Time of Day') ax.set_ylabel('Steps per min') plt.show()  Visualise walk times for each day of the week. We can build a picture of the \u0026lsquo;average\u0026rsquo; day over the last 5 months, broken down by day of the week.\n# Add a weekday column to walks dataset for grouping walks['weekday'] = walks.time.apply(lambda x: datetime.strftime(x, \u0026quot;%A\u0026quot;)) walks   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  time stepcount weekday     0 2019-10-01 00:00:00 0 Tuesday   1 2019-10-01 00:01:00 0 Tuesday   2 2019-10-01 00:02:00 0 Tuesday   3 2019-10-01 00:03:00 0 Tuesday   4 2019-10-01 00:04:00 0 Tuesday   ... ... ... ...   332635 2020-05-18 23:55:00 0 Monday   332636 2020-05-18 23:56:00 0 Monday   332637 2020-05-18 23:57:00 0 Monday   332638 2020-05-18 23:58:00 0 Monday   332639 2020-05-18 23:59:00 0 Monday    332640 rows × 3 columns\n To build our daily picture, let\u0026rsquo;s first group the dataset by day of the week, then average the stepcount for each given minute. This should give us a sense of the distribution of walks on each day.\n# change date column to hour:min strings for grouping walks.time = walks.time.apply(lambda x: datetime.strftime(x, \u0026quot;%H:%M\u0026quot;)) # for each day of the week, average step count over all dates walks_weekday = walks.groupby('weekday') weekdays = {} for day_name, df in walks_weekday: # group by minute, then average over dates df = df.groupby('time').mean() weekdays[day_name] = df  We can also get rid of the timestamps during the night, since I\u0026rsquo;m not up for midnight walks too often.\n# Restrict to waking hours, say 7:00am to 23:59pm for day in weekdays: weekdays[day] = weekdays[day].iloc[420:]  Now, let\u0026rsquo;s look at the distribution of walks on Mondays.\nweekdays['Monday'].rolling(15).mean().plot() plt.show()  Finally, we do this for each day of the week separately.\n# Plot each day of the week days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'] fig1, axes1 = plt.subplots(1, 5, figsize=(25, 5)) fig2, axes2 = plt.subplots(1, 2, figsize=(25, 5)) # Plot Monday-Friday first for i in range(5): # Take 15min rolling average df = weekdays[days[i]].rolling(15).mean() # Relabel df.rename(columns={'stepcount': 'steps/min'}, inplace=True) # Plot day df.plot(ax=axes1[i]) axes1[i].set_title(days[i]) axes1[i].set_xlabel(\u0026quot;Time of Day\u0026quot;) # Then plot Saturday-Sunday for i in range(2): # Take 15 min rolling average df = weekdays[days[5+i]].rolling(15).mean() # Relabel df.rename(columns={'stepcount': 'steps/min'}, inplace=True) # Plot day df.plot(ax=axes2[i]) axes2[i].set_title(days[5+i]) axes2[i].set_xlabel(\u0026quot;Time of Day\u0026quot;)  ","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589760000,"objectID":"b5336ff08691fcce7f6d9f17e9171f91","permalink":"https://vgelinas.github.io/post/fitbit-data-exploration-part-ii/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/post/fitbit-data-exploration-part-ii/","section":"post","summary":"The second part of the Fitbit data exploration project, where we explore some visualisations of the data obtained in part I.","tags":null,"title":"Fitbit Data Exploration Part II","type":"post"}]