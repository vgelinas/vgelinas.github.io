[{"authors":["admin"],"categories":null,"content":"I am a mathematician living in Toronto and working in predictive analytics, data science and machine learning. This site is to showcase some of my interests.\nPreviously, I was a mathematics research fellow at Trinity College, Dublin. Before that, I completed a PhD at the University of Toronto, where my advisor was the incredible Ragnar-Olaf Buchweitz. I am originally from Montréal, and speak French natively.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://vgelinas.github.io/author/vincent-gelinas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/vincent-gelinas/","section":"authors","summary":"I am a mathematician living in Toronto and working in predictive analytics, data science and machine learning. This site is to showcase some of my interests.\nPreviously, I was a mathematics research fellow at Trinity College, Dublin.","tags":null,"title":"Vincent Gélinas","type":"authors"},{"authors":null,"categories":null,"content":"In this project we will explore some Fitbit activity data pulled via orcasgit\u0026rsquo;s python-fitbit api. We will go through the following steps:\n Data collection Data cleaning Data visualisation  Dependencies  Python 3+ The python-fitbit api The ratelimit library The pandas, matplotlib, datetime standard libraries  import fitbit import json import pandas as pd import matplotlib.pyplot as plt from datetime import datetime, timedelta from ratelimit import limits, sleep_and_retry %matplotlib inline  1. Data Collection 1.1. Authentication setup To obtain personal data, we will use the python-fitbit API, a Python wrapper for the Fitbit API. This first requires you to set-up a Fitbit app, and to collect the client_id and client_secret for this app. For this project I\u0026rsquo;ve chosen to keep these in a credentials.json file stored in a dedicated subfolder named \u0026lsquo;oauth\u0026rsquo;.\n!cat oauth/credentials.json  {\u0026quot;client_id\u0026quot;: \u0026quot;YOUR_CLIENT_ID\u0026quot;, \u0026quot;client_secret\u0026quot;: \u0026quot;YOUR_CLIENT_SECRET\u0026quot;}\r In order to access the API, we will also need access tokens to authenticate. An access and refresh token can be obtained by following the official guide here. Alternatively, one can obtain them directly by running the script \u0026ldquo;gather_your_keys.py\u0026rdquo; available at the python-fitbit github page. Similarly, I chose to store these in a json file named \u0026lsquo;tokens\u0026rsquo;.\n!cat oauth/tokens.json  {\u0026quot;access_token\u0026quot;: \u0026quot;YOUR_ACCESS_TOKEN\u0026quot;, \u0026quot;expires_in\u0026quot;: 28800, \u0026quot;refresh_token\u0026quot;: \u0026quot;YOUR_REFRESH_TOKEN\u0026quot;, \u0026quot;scope\u0026quot;: [\u0026quot;profile\u0026quot;, \u0026quot;social\u0026quot;, \u0026quot;sleep\u0026quot;, \u0026quot;settings\u0026quot;, \u0026quot;location\u0026quot;, \u0026quot;heartrate\u0026quot;, \u0026quot;activity\u0026quot;, \u0026quot;nutrition\u0026quot;, \u0026quot;weight\u0026quot;], \u0026quot;token_type\u0026quot;: \u0026quot;Bearer\u0026quot;, \u0026quot;user_id\u0026quot;: \u0026quot;USER_ID\u0026quot;, \u0026quot;expires_at\u0026quot;: 1589930317.083361}\r The only important keys are \u0026ldquo;access_token\u0026rdquo; and \u0026ldquo;refresh_token\u0026rdquo; (the rest corresponds to optional arguments). Given our credentials and access/refresh tokens, we can instantiate a fitbit client which will handle API calls for us, using the tokens for authentication.\nThe access_token is only good for 8 hours after first emitted. The way the API handles this is by giving you a refresh_token, which never deprecates, and can be used to obtain a new pair (access_token, refresh_token) of valid tokens after the access token expires. The client then needs to know where to store this pair (in our case, in tokens.json), and so we will pass it a token refresh method telling it how to do this.\n# load credentials with open(\u0026quot;./oauth/credentials.json\u0026quot;, \u0026quot;r\u0026quot;) as f: credentials = json.load(f) # load tokens with open(\u0026quot;./oauth/tokens.json\u0026quot;, \u0026quot;r\u0026quot;) as f: tokens = json.load(f) client_id = credentials['client_id'] client_secret = credentials['client_secret'] access_token = tokens['access_token'] refresh_token = tokens['refresh_token'] expires_at = tokens['expires_at'] # token refresh method def refresh_callback(token): \u0026quot;\u0026quot;\u0026quot; Called when the OAuth token has been refreshed \u0026quot;\u0026quot;\u0026quot; with open(\u0026quot;./oauth/tokens.json\u0026quot;, \u0026quot;w\u0026quot;) as f: json.dump(token, f) # initialise client client = fitbit.Fitbit(client_id=client_id, client_secret=client_secret, access_token=access_token, refresh_token=refresh_token, refresh_cb=refresh_callback)  From there on, everytime the access token expires, new access and refresh tokens are issued and stored by the refresh_callback method, and we don\u0026rsquo;t need to reauthorise.\n1.2. A first look at the response data The python-fitbit api supports the methods listed here. We\u0026rsquo;ll be interested in getting the activities data, as well as the intraday steps data which logs our minute-by-minute stepcount during the day.\n# Get activity data for one date, say May 1st date = '2020-05-01' activities_response = client.activities(date=date) # Look at the response object display(activities_response) print(\u0026quot;The type of activities_response is: {}\u0026quot;.format(type(activities_response)))  {'activities': [{'activityId': 90013, 'activityParentId': 90013, 'activityParentName': 'Walk', 'calories': 302, 'description': 'Walking less than 2 mph, strolling very slowly', 'duration': 2714000, 'hasStartTime': True, 'isFavorite': False, 'lastModified': '2020-05-01T13:10:18.000Z', 'logId': 30758911349, 'name': 'Walk', 'startDate': '2020-05-01', 'startTime': '08:20', 'steps': 4000}], 'goals': {'activeMinutes': 30, 'caloriesOut': 2745, 'distance': 5, 'steps': 12500}, 'summary': {'activeScore': -1, 'activityCalories': 1379, 'caloriesBMR': 1659, 'caloriesOut': 2826, 'distances': [{'activity': 'total', 'distance': 4.69}, {'activity': 'tracker', 'distance': 4.69}, {'activity': 'loggedActivities', 'distance': 0}, {'activity': 'veryActive', 'distance': 2.6}, {'activity': 'moderatelyActive', 'distance': 0.4}, {'activity': 'lightlyActive', 'distance': 1.68}, {'activity': 'sedentaryActive', 'distance': 0}], 'fairlyActiveMinutes': 25, 'heartRateZones': [{'caloriesOut': 1916.74877, 'max': 94, 'min': 30, 'minutes': 1255, 'name': 'Out of Range'}, {'caloriesOut': 775.70893, 'max': 132, 'min': 94, 'minutes': 137, 'name': 'Fat Burn'}, {'caloriesOut': 81.56868, 'max': 160, 'min': 132, 'minutes': 8, 'name': 'Cardio'}, {'caloriesOut': 0, 'max': 220, 'min': 160, 'minutes': 0, 'name': 'Peak'}], 'lightlyActiveMinutes': 210, 'marginalCalories': 828, 'restingHeartRate': 59, 'sedentaryMinutes': 578, 'steps': 9887, 'veryActiveMinutes': 49}} The type of activities_response is: \u0026lt;class 'dict'\u0026gt;  The dataset consists of nested dictionaries. We\u0026rsquo;ll extract two datasets from the \u0026lsquo;activities\u0026rsquo; and \u0026lsquo;summary\u0026rsquo; keys.\n# Get activities dataset activities = activities_response['activities'] activities = pd.DataFrame(activities) display(activities)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  activityId activityParentId activityParentName calories description duration hasStartTime isFavorite lastModified logId name startDate startTime steps     0 90013 90013 Walk 302 Walking less than 2 mph, strolling very slowly 2714000 True False 2020-05-01T13:10:18.000Z 30758911349 Walk 2020-05-01 08:20 4000     # Get summary dataset summary = activities_response['summary'] # Remove sub-dictionaries del summary['distances'] del summary['heartRateZones'] summary = pd.DataFrame(summary, index=[0]) # all values are scalars, must pass an index summary   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  activeScore activityCalories caloriesBMR caloriesOut fairlyActiveMinutes lightlyActiveMinutes marginalCalories restingHeartRate sedentaryMinutes steps veryActiveMinutes     0 -1 1379 1659 2826 25 210 828 59 578 9887 49     Next, let\u0026rsquo;s look at the intraday step data.\n# Get intraday steps data steps_response = client.intraday_time_series('activities/steps', base_date=date, detail_level=\u0026quot;1min\u0026quot;) # Extract dataset from response object steps = steps_response['activities-steps-intraday']['dataset'] # Display dataset steps = pd.DataFrame(steps) display(steps)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  time value     0 00:00:00 0   1 00:01:00 0   2 00:02:00 0   3 00:03:00 0   4 00:04:00 0   ... ... ...   1435 23:55:00 0   1436 23:56:00 0   1437 23:57:00 0   1438 23:58:00 0   1439 23:59:00 0    1440 rows × 2 columns\n We get the minute-by-minute count of steps on that day. Let\u0026rsquo;s take a quick look at a plot.\nsteps.plot()  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff4f1838670\u0026gt;  1.3. Collect activity and intraday steps data since October 1st. We can now iterate this, calling the fitbit API for each day since October 1st and collecting the result into three datasets. We saw that the python-fitbit api takes in dates formatted as \u0026lsquo;YYYY-MM-DD\u0026rsquo;; we can use the date_range method in pandas to produce a list of datetime objects, and then format the dates using the strftime method in datetime.\nAdditionally, we have to remember that the Fitbit API has a rate limit of 150 API calls / hour, and so we will use the ratelimit library to restrict our collection rate.\nFirst, let\u0026rsquo;s get a list of dates.\n# Get date range from October 1st to today start = pd.to_datetime(\u0026quot;2019-10-01\u0026quot;) date_range = pd.date_range(start=start, end=datetime.today() - timedelta(days=1)) date_range = [datetime.strftime(date, \u0026quot;%Y-%m-%d\u0026quot;) for date in date_range] date_range[-5:]  ['2020-05-14', '2020-05-15', '2020-05-16', '2020-05-17', '2020-05-18']  Next, we call the API for each day in the list, timestamp the resulting datasets, then store the total datasets in csv files. We do this for each of the \u0026lsquo;activities\u0026rsquo;, \u0026lsquo;summary\u0026rsquo; and \u0026lsquo;steps\u0026rsquo; datasets.\n# We define a data collection function, and we use the ratelimit library # to limit our function to 150 API calls / hour. ONE_HOUR = 3600 @sleep_and_retry @limits(calls=70, period=ONE_HOUR) def call_fitbit_api(date): \u0026quot;\u0026quot;\u0026quot; Call the Fitbit API for given date in format 'YYYY-MM-DD', Return tuple (activities, summary, steps) dataframes \u0026quot;\u0026quot;\u0026quot; # Call API twice to get activities and steps responses activities_data = client.activities(date=date) steps_data = client.intraday_time_series('activities/steps', base_date=date, detail_level='1min') # Get activities dataset activities = activities_data['activities'] activities = pd.DataFrame(activities) # Get summary dataset summary = activities_data['summary'] del summary['distances'] del summary['heartRateZones'] summary = pd.DataFrame(summary, index=[0]) # Get steps intraday dataset steps = steps_data['activities-steps-intraday']['dataset'] steps = pd.DataFrame(steps) # Add a date column activities['date'] = [date for i in activities.index] summary['date'] = [date] steps['date'] = [date for i in steps.index] return activities, summary, steps def get_fitbit_data(date_range): \u0026quot;\u0026quot;\u0026quot; Collect 'activities', 'summary' and 'steps' datasets over date range Store as CSV files with format RESOURCE_DATE_to_DATE.csv \u0026quot;\u0026quot;\u0026quot; daily_df = { 'activities': [], 'summary': [], 'steps': [] } for date in date_range: # Call API and get three datasets activities, summary, steps = call_fitbit_api(date) # Append to previous datasets daily_df['activities'].append(activities) daily_df['summary'].append(summary) daily_df['steps'].append(steps) # Store total dataset as file with format \u0026quot;resource_DATE_to_DATE.csv\u0026quot; start, end = date_range[0], date_range[-1] for resource in daily_df: df = pd.concat(daily_df[resource], ignore_index=True) df.to_csv(\u0026quot;./data/raw/{}_{}_to_{}.csv\u0026quot;.format(resource, start, end), index=False)  # Collect Fitbit 'activities', 'summary' and 'steps' data since October 1st, 2019 get_fitbit_data(date_range=date_range)  2. Cleaning the data 2.1. The activity dataset activities = pd.read_csv(\u0026quot;./data/raw/activities_2019-10-01_to_2020-05-18.csv\u0026quot;) display(activities.head(3)) activities.shape   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  activityId activityParentId activityParentName calories description duration hasStartTime isFavorite lastModified logId name startDate startTime steps date distance     0 90013.0 90013.0 Walk 245.0 Walking less than 2 mph, strolling very slowly 1843000.0 True False 2019-10-01T15:36:45.000Z 2.568779e+10 Walk 2019-10-01 10:46 2059.0 2019-10-01 NaN   1 90013.0 90013.0 Walk 194.0 Walking less than 2 mph, strolling very slowly 1792000.0 True False 2019-10-01T17:44:08.000Z 2.569041e+10 Walk 2019-10-01 11:52 1977.0 2019-10-01 NaN   2 90013.0 90013.0 Walk 165.0 Walking less than 2 mph, strolling very slowly 1485000.0 True False 2019-10-02T02:00:43.000Z 2.570412e+10 Walk 2019-10-01 18:20 1443.0 2019-10-01 NaN     (354, 16)  Some of the columns contain logging id information, True/False data or duplicate information which is not useful to us. Let\u0026rsquo;s drop these.\ndrop_columns = ['activityId', 'activityParentId', 'activityParentName', 'hasStartTime', 'isFavorite', 'lastModified', 'logId', 'startDate'] activities.drop(drop_columns, axis=1, inplace=True)  Next, let\u0026rsquo;s look at the distance column. Consulting the documentation, we see that this means logged distance. Since I\u0026rsquo;ve rarely used the feature, it looks like the column consists mostly of missing values.\nactivities.distance.value_counts()  0.310468 1 0.773283 1 Name: distance, dtype: int64  Since we only have 2 non-missing values in 354 rows, let\u0026rsquo;s drop the column.\nactivities.drop('distance', axis=1, inplace=True)  Some of the column names are in camelCase. Let\u0026rsquo;s rename them to Python\u0026rsquo;s favored snake_case.\nactivities.rename(columns={'startTime': 'start_time'}, inplace=True) activities.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  calories description duration name start_time steps date     0 245.0 Walking less than 2 mph, strolling very slowly 1843000.0 Walk 10:46 2059.0 2019-10-01   1 194.0 Walking less than 2 mph, strolling very slowly 1792000.0 Walk 11:52 1977.0 2019-10-01   2 165.0 Walking less than 2 mph, strolling very slowly 1485000.0 Walk 18:20 1443.0 2019-10-01     The duration isn\u0026rsquo;t immediately readable here. The Fitbit api documentation lists the duration as being in millisecond, so let\u0026rsquo;s put it in minutes and rename accordingly.\nactivities.duration = activities.duration.apply(lambda x: round(x/60000)) activities.rename(columns={'duration': 'duration_min'}, inplace=True) activities.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  calories description duration_min name start_time steps date     0 245.0 Walking less than 2 mph, strolling very slowly 31 Walk 10:46 2059.0 2019-10-01   1 194.0 Walking less than 2 mph, strolling very slowly 30 Walk 11:52 1977.0 2019-10-01   2 165.0 Walking less than 2 mph, strolling very slowly 25 Walk 18:20 1443.0 2019-10-01     To help with analysis, let\u0026rsquo;s format the start_time column as \u0026ldquo;YYYY-MM-DD H:M:S\u0026rdquo; to more easily convert to a datetime object. Since we have the activity duration, we can also add an end_time column.\n# Format start_time column and convert to datetime object activities.start_time = activities.date + \u0026quot; \u0026quot; + activities.start_time + \u0026quot;:00\u0026quot; activities.start_time = pd.to_datetime(activities.start_time) # Create end_time column by adding the duration_min column to start_time activities_duration = activities.duration_min.apply(lambda x: timedelta(minutes=x)) activities['end_time'] = activities.start_time + activities_duration # Display result activities.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  calories description duration_min name start_time steps date end_time     0 245.0 Walking less than 2 mph, strolling very slowly 31 Walk 2019-10-01 10:46:00 2059.0 2019-10-01 2019-10-01 11:17:00   1 194.0 Walking less than 2 mph, strolling very slowly 30 Walk 2019-10-01 11:52:00 1977.0 2019-10-01 2019-10-01 12:22:00   2 165.0 Walking less than 2 mph, strolling very slowly 25 Walk 2019-10-01 18:20:00 1443.0 2019-10-01 2019-10-01 18:45:00     Finally, let\u0026rsquo;s reorder the columns for readability.\n# Reorder columns column_order = ['date', 'name', 'description', 'start_time', 'end_time', 'duration_min', 'steps', 'calories'] activities = activities[column_order] # Store dataset start, end = activities.date[0], activities.date[len(activities.index)-1] activities.to_csv(\u0026quot;./data/tidy/activities_{}_to_{}.csv\u0026quot;.format(start, end), index=False) # Look at end result activities   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  date name description start_time end_time duration_min steps calories     0 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 10:46:00 2019-10-01 11:17:00 31 2059.0 245.0   1 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 11:52:00 2019-10-01 12:22:00 30 1977.0 194.0   2 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 18:20:00 2019-10-01 18:45:00 25 1443.0 165.0   3 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 19:38:00 2019-10-01 20:05:00 27 1624.0 176.0   4 2019-10-02 Walk Walking less than 2 mph, strolling very slowly 2019-10-02 13:38:00 2019-10-02 15:20:00 102 7035.0 552.0   ... ... ... ... ... ... ... ... ...   349 2020-05-14 Walk Walking less than 2 mph, strolling very slowly 2020-05-14 08:22:00 2020-05-14 09:07:00 45 4394.0 361.0   350 2020-05-15 Walk Walking less than 2 mph, strolling very slowly 2020-05-15 12:23:00 2020-05-15 13:59:00 96 9430.0 658.0   351 2020-05-15 Run Running - 5 mph (12 min/mile) 2020-05-15 20:13:00 2020-05-15 20:34:00 21 2865.0 245.0   352 2020-05-16 Walk Walking less than 2 mph, strolling very slowly 2020-05-16 10:29:00 2020-05-16 12:33:00 124 10487.0 855.0   353 2020-05-17 Walk Walking less than 2 mph, strolling very slowly 2020-05-17 11:11:00 2020-05-17 13:15:00 124 10757.0 835.0    354 rows × 8 columns\n 2.2. The summary dataset summary = pd.read_csv(\u0026quot;./data/raw/summary_2019-10-01_to_2020-05-18.csv\u0026quot;) summary   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  activeScore activityCalories caloriesBMR caloriesOut fairlyActiveMinutes lightlyActiveMinutes marginalCalories restingHeartRate sedentaryMinutes steps veryActiveMinutes date     0 -1 4097 1663 5018 165 420 2670 58 209 25576 152 2019-10-01   1 -1 1967 1663 3211 27 360 1145 58 379 16471 40 2019-10-02   2 -1 1540 1663 2923 22 251 920 57 674 13510 50 2019-10-03   3 -1 1470 1663 2883 44 163 940 56 659 11443 65 2019-10-04   4 -1 1776 1663 3136 35 178 1110 56 572 18711 120 2019-10-05   ... ... ... ... ... ... ... ... ... ... ... ... ...   226 -1 1179 1659 2614 8 218 680 57 636 8938 40 2020-05-14   227 -1 1589 1659 2988 19 183 1011 56 565 15358 99 2020-05-15   228 -1 1512 1659 2903 44 160 959 55 614 15115 82 2020-05-16   229 -1 1922 1659 3230 40 224 1215 55 399 18880 100 2020-05-17   230 -1 468 1659 2046 8 116 235 54 713 2341 0 2020-05-18    231 rows × 12 columns\n The activeScore columns is added by the python-fitbit wrapper to the Fitbit API. All values are -1 in our dataset so there\u0026rsquo;s not much loss of information in dropping the column (maybe figure this out a bit better).\n(summary.activeScore == -1).all()  True  summary.drop('activeScore', axis=1, inplace=True) summary.head(2)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  activityCalories caloriesBMR caloriesOut fairlyActiveMinutes lightlyActiveMinutes marginalCalories restingHeartRate sedentaryMinutes steps veryActiveMinutes date     0 4097 1663 5018 165 420 2670 58 209 25576 152 2019-10-01   1 1967 1663 3211 27 360 1145 58 379 16471 40 2019-10-02     Next, we again format all columns to snake_case and reorder for readability.\n# Rename columns to snake_case columns_map = { 'activityCalories': 'activity_calories', 'caloriesBMR': 'calories_BMR', 'caloriesOut': 'calories_out', 'fairlyActiveMinutes': 'fairly_active_minutes', 'lightlyActiveMinutes': 'lightly_active_minutes', 'marginalCalories': 'marginal_calories', 'restingHeartRate': 'resting_heart_rate', 'sedentaryMinutes': 'sedentary_minutes', 'veryActiveMinutes': 'very_active_minutes' } summary.rename(columns=columns_map, inplace=True) # Reorder columns column_order = ['date', 'steps', 'very_active_minutes', 'fairly_active_minutes', 'lightly_active_minutes', 'sedentary_minutes', 'activity_calories', 'marginal_calories', 'calories_out', 'calories_BMR', 'resting_heart_rate'] summary = summary[column_order] # Store dataset start, end = summary.date[0], summary.date[len(summary.index)-1] summary.to_csv(\u0026quot;./data/tidy/summary_{}_to_{}.csv\u0026quot;.format(start, end), index=False) # Look at result summary.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  date steps very_active_minutes fairly_active_minutes lightly_active_minutes sedentary_minutes activity_calories marginal_calories calories_out calories_BMR resting_heart_rate     0 2019-10-01 25576 152 165 420 209 4097 2670 5018 1663 58   1 2019-10-02 16471 40 27 360 379 1967 1145 3211 1663 58   2 2019-10-03 13510 50 22 251 674 1540 920 2923 1663 57     2.3. The steps dataset Finally, we look at the intraday steps dataset.\nsteps = pd.read_csv(\u0026quot;./data/raw/steps_2019-10-01_to_2020-05-18.csv\u0026quot;) steps   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  time value date     0 00:00:00 0 2019-10-01   1 00:01:00 0 2019-10-01   2 00:02:00 0 2019-10-01   3 00:03:00 0 2019-10-01   4 00:04:00 0 2019-10-01   ... ... ... ...   332635 23:55:00 0 2020-05-18   332636 23:56:00 0 2020-05-18   332637 23:57:00 0 2020-05-18   332638 23:58:00 0 2020-05-18   332639 23:59:00 0 2020-05-18    332640 rows × 3 columns\n We can combine the time and date into a single column, in datetime format. We also rename value to the more descriptive \u0026lsquo;stepcount\u0026rsquo;.\n# Combine date and time steps.time = steps.date + \u0026quot; \u0026quot; + steps.time # Rename value to stepcount steps.rename(columns={'value': 'stepcount'}, inplace=True) # Get endpoint dates to store the file start, end = steps.date[0], steps.date[len(steps.index) - 1] # Drop date column and store steps.drop('date', axis=1, inplace=True) steps.to_csv(\u0026quot;./data/tidy/steps_{}_to_{}.csv\u0026quot;.format(start, end), index=False) # Look at end result steps   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  time stepcount     0 2019-10-01 00:00:00 0   1 2019-10-01 00:01:00 0   2 2019-10-01 00:02:00 0   3 2019-10-01 00:03:00 0   4 2019-10-01 00:04:00 0   ... ... ...   332635 2020-05-18 23:55:00 0   332636 2020-05-18 23:56:00 0   332637 2020-05-18 23:57:00 0   332638 2020-05-18 23:58:00 0   332639 2020-05-18 23:59:00 0    332640 rows × 2 columns\n ","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589760000,"objectID":"a636816090865ee734a3f837be4f8c99","permalink":"https://vgelinas.github.io/post/fitbit-data-exploration-part-i/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/post/fitbit-data-exploration-part-i/","section":"post","summary":"In this jupyter notebook we explore some personal Fitbit data. In this first part, we will walk through the data collection and data cleaning process.","tags":null,"title":"Fitbit Data Exploration Part I","type":"post"},{"authors":null,"categories":null,"content":"In Part I we showed how to connect to the Fitbit API via Python, and we built some datasets consisting of activities and intraday steps data from October to today. In this post we walk through some data visualisations, and will take a look in particular at the intraday steps data.\n3. Visualisations 3.1. Activity statistics per week day Let\u0026rsquo;s compile some statistics based on day of the week.\nsummary = pd.read_csv(\u0026quot;./data/tidy/summary_2019-10-01_to_2020-05-18.csv\u0026quot;) # convert date to datetime format summary.date = pd.to_datetime(summary.date) summary   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  date steps very_active_minutes fairly_active_minutes lightly_active_minutes sedentary_minutes activity_calories marginal_calories calories_out calories_BMR resting_heart_rate     0 2019-10-01 25576 152 165 420 209 4097 2670 5018 1663 58   1 2019-10-02 16471 40 27 360 379 1967 1145 3211 1663 58   2 2019-10-03 13510 50 22 251 674 1540 920 2923 1663 57   3 2019-10-04 11443 65 44 163 659 1470 940 2883 1663 56   4 2019-10-05 18711 120 35 178 572 1776 1110 3136 1663 56   ... ... ... ... ... ... ... ... ... ... ... ...   226 2020-05-14 8938 40 8 218 636 1179 680 2614 1659 57   227 2020-05-15 15358 99 19 183 565 1589 1011 2988 1659 56   228 2020-05-16 15115 82 44 160 614 1512 959 2903 1659 55   229 2020-05-17 18880 100 40 224 399 1922 1215 3230 1659 55   230 2020-05-18 2341 0 8 116 713 468 235 2046 1659 54    231 rows × 11 columns\n We can use strftime to convert the date to a week day, and get group statistics per day of the week.\n# Add a weekday column summary['weekday'] = summary.date.apply(lambda x: datetime.strftime(x, \u0026quot;%A\u0026quot;)) # Get statistics per day of the week weekly_statistics = summary.groupby('weekday').describe() # Row indices are days of the week, put them in order row_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'] weekly_statistics = weekly_statistics.loc[row_order, :] # Show results weekly_statistics   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }  \n   steps very_active_minutes ... calories_BMR resting_heart_rate    count mean std min 25% 50% 75% max count mean ... 75% max count mean std min 25% 50% 75% max   weekday                          Monday 33.0 10817.393939 4075.727205 2341.0 7793.0 11663.0 13248.0 17668.0 33.0 50.848485 ... 1663.0 1663.0 33.0 55.030303 1.610218 53.0 54.0 55.0 56.0 59.0   Tuesday 33.0 11202.878788 4575.416680 3250.0 8211.0 10953.0 12994.0 25576.0 33.0 57.666667 ... 1663.0 1663.0 33.0 55.030303 1.878910 52.0 53.0 55.0 56.0 59.0   Wednesday 33.0 11007.939394 3307.219952 4486.0 9083.0 10795.0 13538.0 18402.0 33.0 53.909091 ... 1663.0 1663.0 33.0 55.303030 1.704495 52.0 54.0 55.0 56.0 58.0   Thursday 33.0 10867.212121 2882.811852 4525.0 8938.0 10856.0 13064.0 16621.0 33.0 54.212121 ... 1663.0 1663.0 33.0 55.424242 1.581738 52.0 54.0 56.0 56.0 59.0   Friday 33.0 12526.757576 4179.309722 2028.0 9887.0 12445.0 15358.0 18958.0 33.0 66.454545 ... 1663.0 1663.0 33.0 55.424242 1.581738 52.0 54.0 56.0 56.0 59.0   Saturday 33.0 14507.181818 4188.612206 6315.0 12446.0 15049.0 16924.0 23152.0 33.0 74.151515 ... 1663.0 1663.0 33.0 55.575758 1.581738 53.0 55.0 55.0 56.0 60.0   Sunday 33.0 12488.121212 4849.136191 4560.0 9659.0 11756.0 15944.0 25193.0 33.0 64.636364 ... 1663.0 1663.0 33.0 55.393939 1.456438 53.0 54.0 55.0 56.0 59.0    7 rows × 80 columns\n # Plot the mean and first quartile for number of steps per weekday mean_steps = weekly_statistics.steps[['mean', '25%']] mean_steps.plot(kind='bar') plt.title('Weekly stepcount since October 1st, 2019') plt.ylabel('steps') plt.ylim([0, 18000]) plt.show()  3.2. Visualising walks over the day # Load in dataset steps = pd.read_csv(\u0026quot;./data/tidy/steps_2019-10-01_to_2020-05-18.csv\u0026quot;) display(steps) # Convert time to datetime object for analysis steps.time = pd.to_datetime(steps.time)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  time stepcount     0 2019-10-01 00:00:00 0   1 2019-10-01 00:01:00 0   2 2019-10-01 00:02:00 0   3 2019-10-01 00:03:00 0   4 2019-10-01 00:04:00 0   ... ... ...   332635 2020-05-18 23:55:00 0   332636 2020-05-18 23:56:00 0   332637 2020-05-18 23:57:00 0   332638 2020-05-18 23:58:00 0   332639 2020-05-18 23:59:00 0    332640 rows × 2 columns\n Let\u0026rsquo;s visualise steps intraday data over a given day.\ndate = '2020-05-01' # Restrict to logs for given date day_df = steps[steps.time.apply(lambda x: datetime.strftime(x, \u0026quot;%Y-%m-%d\u0026quot;)) == date].copy() # deep copy # Restrict to within waking hours start_of_day = pd.to_datetime('2020-05-01 07:00:00') end_of_day = pd.to_datetime('2020-05-01 23:00:00') day_df = day_df[(day_df.time \u0026gt;= start_of_day)\u0026amp;(day_df.time \u0026lt;= end_of_day)] # Convert time back to hr:min:sec format and set as index day_df.time = day_df.time.apply(lambda x: datetime.strftime(x, \u0026quot;%H:%M:%S\u0026quot;)) day_df.set_index('time', inplace=True)  # Plot steps on May 1st fig, ax = plt.subplots() day_df.rolling(15).mean().plot(ax=ax) # 15 min rolling avg to smooth out noise ax.set_title('Steps on May 1st, 2020') ax.set_xlabel('Time of Day') ax.set_ylabel('Steps per min') plt.show()  Let\u0026rsquo;s isolate the steps that result from walks and not from general activity. The activity dataset has timestamps for each activity (walk, run, \u0026hellip;) and we may use these to filter our dataset. Let\u0026rsquo;s add a column named \u0026lsquo;on_walk\u0026rsquo; to the steps dataset, with a True/False value.\n# Load activities dataset activities = pd.read_csv(\u0026quot;./data/tidy/activities_2019-10-01_to_2020-05-17.csv\u0026quot;) display(activities.head(3)) # Convert time columns to datetime objects for analysis activities.start_time = pd.to_datetime(activities.start_time) activities.end_time = pd.to_datetime(activities.end_time)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  date name description start_time end_time duration_min steps calories     0 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 10:46:00 2019-10-01 11:17:00 31 2059.0 245.0   1 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 11:52:00 2019-10-01 12:22:00 30 1977.0 194.0   2 2019-10-01 Walk Walking less than 2 mph, strolling very slowly 2019-10-01 18:20:00 2019-10-01 18:45:00 25 1443.0 165.0     # Helper function to filter the intraday steps data by activity type def is_during_activity(t, activity): \u0026quot;\u0026quot;\u0026quot; Takes a datetime object t and activity name Returns True if during activity, else False \u0026quot;\u0026quot;\u0026quot; # Get the activities dataset for that day date = datetime.strftime(t, \u0026quot;%Y-%m-%d\u0026quot;) df = activities[activities.date == date] # Subset to rows which represent activity df = df[df.name == activity] # Check if t is within the bounds of the activity for i in df.index: if df.loc[i, 'start_time'] \u0026lt;= t \u0026lt;= df.loc[i, 'end_time']: return True return False # Add 'on_walk' column to steps dataframe steps['on_walk'] = steps.time.apply(is_during_activity, args=('Walk',))  steps   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  time stepcount on_walk     0 2019-10-01 00:00:00 0 False   1 2019-10-01 00:01:00 0 False   2 2019-10-01 00:02:00 0 False   3 2019-10-01 00:03:00 0 False   4 2019-10-01 00:04:00 0 False   ... ... ... ...   332635 2020-05-18 23:55:00 0 False   332636 2020-05-18 23:56:00 0 False   332637 2020-05-18 23:57:00 0 False   332638 2020-05-18 23:58:00 0 False   332639 2020-05-18 23:59:00 0 False    332640 rows × 3 columns\n Let\u0026rsquo;s create a new dataframe consisting of walks intraday steps data.\n# Remove all steps which are not during a walk walks = steps.copy() walks.stepcount = walks.stepcount.where(walks.on_walk == True, 0) # Drop 'on_walk' column walks.drop('on_walk', axis=1, inplace=True)  Let\u0026rsquo;s look at May 1st again.\ndate = '2020-05-01' # Restrict to logs for given date day_walks = walks[walks.time.apply(lambda x: datetime.strftime(x, \u0026quot;%Y-%m-%d\u0026quot;)) == date].copy() # Restrict to within waking hours start_of_day = pd.to_datetime('2020-05-01 07:00:00') end_of_day = pd.to_datetime('2020-05-01 23:00:00') day_walks = day_walks[(day_walks.time \u0026gt;= start_of_day)\u0026amp;(day_walks.time \u0026lt;= end_of_day)] # Convert time back to hr:min:sec format and set as index day_walks.time = day_walks.time.apply(lambda x: datetime.strftime(x, \u0026quot;%H:%M:%S\u0026quot;)) day_walks.set_index('time', inplace=True)  # Plot walks on May 1st fig, ax = plt.subplots() day_walks.rolling(15).mean().plot(ax=ax) # 15 min rolling avg to smooth out noise ax.set_title('Steps on May 1st 2020 during a walk') ax.set_xlabel('Time of Day') ax.set_ylabel('Steps per min') plt.show()  Visualise walk times for each day of the week. We can build a picture of the \u0026lsquo;average\u0026rsquo; day over the last 5 months, broken down by day of the week.\n# Add a weekday column to walks dataset for grouping walks['weekday'] = walks.time.apply(lambda x: datetime.strftime(x, \u0026quot;%A\u0026quot;)) walks   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  time stepcount weekday     0 2019-10-01 00:00:00 0 Tuesday   1 2019-10-01 00:01:00 0 Tuesday   2 2019-10-01 00:02:00 0 Tuesday   3 2019-10-01 00:03:00 0 Tuesday   4 2019-10-01 00:04:00 0 Tuesday   ... ... ... ...   332635 2020-05-18 23:55:00 0 Monday   332636 2020-05-18 23:56:00 0 Monday   332637 2020-05-18 23:57:00 0 Monday   332638 2020-05-18 23:58:00 0 Monday   332639 2020-05-18 23:59:00 0 Monday    332640 rows × 3 columns\n # change date column to hour:min strings for grouping walks.time = walks.time.apply(lambda x: datetime.strftime(x, \u0026quot;%H:%M\u0026quot;)) # for each day of the week, average step count over all dates walks_weekday = walks.groupby('weekday') weekdays = {} for day_name, df in walks_weekday: # group by minute, then average over dates df = df.groupby('time').mean() weekdays[day_name] = df  # Restrict to waking hours, say 7:00am to 23:59pm for day in weekdays: weekdays[day] = weekdays[day].iloc[420:]  weekdays['Saturday'].rolling(15).mean().plot() plt.show()  We can now visualise each day of the week, separately.\n# Plot each day of the week days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'] fig1, axes1 = plt.subplots(1, 5, figsize=(25, 5)) fig2, axes2 = plt.subplots(1, 2, figsize=(25, 5)) # Plot Monday-Friday first for i in range(5): # Take 15min rolling average df = weekdays[days[i]].rolling(15).mean() # Relabel df.rename(columns={'stepcount': 'steps/min'}, inplace=True) # Plot day df.plot(ax=axes1[i]) axes1[i].set_title(days[i]) axes1[i].set_xlabel(\u0026quot;Time of Day\u0026quot;) # Then plot Saturday-Sunday for i in range(2): # Take 15 min rolling average df = weekdays[days[5+i]].rolling(15).mean() # Relabel df.rename(columns={'stepcount': 'steps/min'}, inplace=True) # Plot day df.plot(ax=axes2[i]) axes2[i].set_title(days[5+i]) axes2[i].set_xlabel(\u0026quot;Time of Day\u0026quot;)  ","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589760000,"objectID":"b5336ff08691fcce7f6d9f17e9171f91","permalink":"https://vgelinas.github.io/post/fitbit-data-exploration-part-ii/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/post/fitbit-data-exploration-part-ii/","section":"post","summary":"The second part of the Fitbit data exploration project, where we explore some visualisations of the data obtained in part I.","tags":null,"title":"Fitbit Data Exploration Part II","type":"post"}]