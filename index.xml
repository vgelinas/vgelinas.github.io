<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vincent Gélinas</title>
    <link>https://vgelinas.github.io/</link>
      <atom:link href="https://vgelinas.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Vincent Gélinas</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 15 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://vgelinas.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Vincent Gélinas</title>
      <link>https://vgelinas.github.io/</link>
    </image>
    
    <item>
      <title>Predicting Toronto Rent Prices</title>
      <link>https://vgelinas.github.io/post/predicting-toronto-rent-prices/</link>
      <pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://vgelinas.github.io/post/predicting-toronto-rent-prices/</guid>
      <description>&lt;p&gt;In this Python jupyter notebook we will go through an end-to-end machine learning project and build a predictive model for apartment rent prices in Toronto. This is a classic regression task, and we will go through the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data collection.&lt;/li&gt;
&lt;li&gt;Data preparation &amp;amp; feature engineering.&lt;/li&gt;
&lt;li&gt;Model selection and evaluation.&lt;/li&gt;
&lt;li&gt;Deployment.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/rent-prices-toronto/Rent-Prices-Toronto_58_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;See the 
&lt;a href=&#34;https://github.com/vgelinas/data-projects/tree/master/Rent-Prices-Toronto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt; repository for this project&amp;rsquo;s notebook, datasets and additional scripts.&lt;/p&gt;
&lt;h4 id=&#34;required-libraries&#34;&gt;Required libraries&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# General libraries
import pandas as pd
import numpy as np
from scipy.stats import uniform, randint

# Visualisations
import geopandas as gpd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

# Data preprocessing and transformers
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import OneHotEncoder

# Model selection
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import Lasso
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor

# Evaluate regression models
from sklearn.metrics import mean_squared_error
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;1-data-collection&#34;&gt;1. Data collection&lt;/h2&gt;
&lt;p&gt;The data was scraped from the rental site &lt;a href=&#34;https://www.rentals.ca/&#34;&gt;https://www.rentals.ca/&lt;/a&gt; and consists of ~6800 data points with 13 columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;price (per month)&lt;/li&gt;
&lt;li&gt;city&lt;/li&gt;
&lt;li&gt;street address&lt;/li&gt;
&lt;li&gt;postal code&lt;/li&gt;
&lt;li&gt;longitude&lt;/li&gt;
&lt;li&gt;latitude&lt;/li&gt;
&lt;li&gt;rental type&lt;/li&gt;
&lt;li&gt;number of bedrooms&lt;/li&gt;
&lt;li&gt;number of bathrooms&lt;/li&gt;
&lt;li&gt;sqft&lt;/li&gt;
&lt;li&gt;text description&lt;/li&gt;
&lt;li&gt;year built&lt;/li&gt;
&lt;li&gt;number of parking spots&lt;/li&gt;
&lt;/ul&gt;
&lt;/br&gt;
&lt;h2 id=&#34;2-data-preparation&#34;&gt;2. Data preparation&lt;/h2&gt;
&lt;h3 id=&#34;21-exploration--feature-engineering&#34;&gt;2.1. Exploration &amp;amp; feature engineering&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s take a first look at the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load and display dataset
df = pd.read_csv(&amp;quot;./data/toronto_apartment_rentals_2020.csv&amp;quot;)
df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;street_address&lt;/th&gt;
      &lt;th&gt;city&lt;/th&gt;
      &lt;th&gt;postal_code&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;rental_type&lt;/th&gt;
      &lt;th&gt;bedrooms&lt;/th&gt;
      &lt;th&gt;bathrooms&lt;/th&gt;
      &lt;th&gt;sqft&lt;/th&gt;
      &lt;th&gt;description_text&lt;/th&gt;
      &lt;th&gt;year_built&lt;/th&gt;
      &lt;th&gt;parking_spots&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;334 Gladstone Avenue&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M6J 3L6&lt;/td&gt;
      &lt;td&gt;2999.0&lt;/td&gt;
      &lt;td&gt;-79.431071&lt;/td&gt;
      &lt;td&gt;43.652523&lt;/td&gt;
      &lt;td&gt;Accommodation&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;38 Waterbury Drive&lt;/td&gt;
      &lt;td&gt;Etobicoke&lt;/td&gt;
      &lt;td&gt;M9R 3X6&lt;/td&gt;
      &lt;td&gt;1950.0&lt;/td&gt;
      &lt;td&gt;-79.571523&lt;/td&gt;
      &lt;td&gt;43.686387&lt;/td&gt;
      &lt;td&gt;Accommodation&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;11 Wellesley Street West&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M4Y 1E8&lt;/td&gt;
      &lt;td&gt;1950.0&lt;/td&gt;
      &lt;td&gt;-79.385247&lt;/td&gt;
      &lt;td&gt;43.664503&lt;/td&gt;
      &lt;td&gt;Accommodation&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Candy Factory Loft - Penthouse&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M6J 1H2&lt;/td&gt;
      &lt;td&gt;8495.0&lt;/td&gt;
      &lt;td&gt;-79.415616&lt;/td&gt;
      &lt;td&gt;43.644649&lt;/td&gt;
      &lt;td&gt;Accommodation&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;77 Finch Avenue East&lt;/td&gt;
      &lt;td&gt;North York&lt;/td&gt;
      &lt;td&gt;M2N 6H8&lt;/td&gt;
      &lt;td&gt;1200.0&lt;/td&gt;
      &lt;td&gt;-79.411910&lt;/td&gt;
      &lt;td&gt;43.780085&lt;/td&gt;
      &lt;td&gt;Accommodation&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6826&lt;/th&gt;
      &lt;td&gt;38 Elm Street&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5G 2K5&lt;/td&gt;
      &lt;td&gt;2400.0&lt;/td&gt;
      &lt;td&gt;-79.383640&lt;/td&gt;
      &lt;td&gt;43.657599&lt;/td&gt;
      &lt;td&gt;Accommodation&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6827&lt;/th&gt;
      &lt;td&gt;500 Sherbourne Street&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M4X 1L1&lt;/td&gt;
      &lt;td&gt;2975.0&lt;/td&gt;
      &lt;td&gt;-79.375682&lt;/td&gt;
      &lt;td&gt;43.667902&lt;/td&gt;
      &lt;td&gt;Accommodation&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;900.0&lt;/td&gt;
      &lt;td&gt;500 SHERBOURNE STREET, SUITE 803\n\nTORONTO - ...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6828&lt;/th&gt;
      &lt;td&gt;159 Gerrard Street East&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5A 2E4&lt;/td&gt;
      &lt;td&gt;3195.0&lt;/td&gt;
      &lt;td&gt;-79.374276&lt;/td&gt;
      &lt;td&gt;43.660670&lt;/td&gt;
      &lt;td&gt;Apartment&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6829&lt;/th&gt;
      &lt;td&gt;460 Adelaide Street East&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5A 1N6&lt;/td&gt;
      &lt;td&gt;2150.0&lt;/td&gt;
      &lt;td&gt;-79.366487&lt;/td&gt;
      &lt;td&gt;43.652770&lt;/td&gt;
      &lt;td&gt;Accommodation&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6830&lt;/th&gt;
      &lt;td&gt;135 Lambton Avenue&lt;/td&gt;
      &lt;td&gt;York&lt;/td&gt;
      &lt;td&gt;M6N 2S8&lt;/td&gt;
      &lt;td&gt;2500.0&lt;/td&gt;
      &lt;td&gt;-79.491092&lt;/td&gt;
      &lt;td&gt;43.682418&lt;/td&gt;
      &lt;td&gt;Accommodation&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;6831 rows × 13 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;At first sight some columns have a lot of missing values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.isna().sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;street_address         0
city                   0
postal_code           21
price                  0
longitude              0
latitude               0
rental_type            0
bedrooms             899
bathrooms           1013
sqft                5182
description_text    5192
year_built          6119
parking_spots       6119
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like the last 4 columns are pretty sparse. Since there&amp;rsquo;s so little in the last 3 columns and it&amp;rsquo;s not clear how useful these features would be, for simplicity we will simply drop them. The sqft feature is also rather sparse, but it should have good predictive power and so we&amp;rsquo;ll keep it. We can instead try to infer it from the other features.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll drop the street address column as well, since the location data is already encoded in the postal code and longitude/latitude.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at the rental type column.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.rental_type.value_counts()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accommodation    5919
Apartment         912
Name: rental_type, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Accommodation just seems like a generic term (as opposed to, say, apartment vs condo vs house). We&amp;rsquo;ll drop this column as well since there isn&amp;rsquo;t a clear divide between accommodation and apartment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.drop([&#39;description_text&#39;, &#39;year_built&#39;, &#39;parking_spots&#39;, &#39;street_address&#39;, &#39;rental_type&#39;], axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;city&lt;/th&gt;
      &lt;th&gt;postal_code&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;bedrooms&lt;/th&gt;
      &lt;th&gt;bathrooms&lt;/th&gt;
      &lt;th&gt;sqft&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M6J 3L6&lt;/td&gt;
      &lt;td&gt;2999.0&lt;/td&gt;
      &lt;td&gt;-79.431071&lt;/td&gt;
      &lt;td&gt;43.652523&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Etobicoke&lt;/td&gt;
      &lt;td&gt;M9R 3X6&lt;/td&gt;
      &lt;td&gt;1950.0&lt;/td&gt;
      &lt;td&gt;-79.571523&lt;/td&gt;
      &lt;td&gt;43.686387&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M4Y 1E8&lt;/td&gt;
      &lt;td&gt;1950.0&lt;/td&gt;
      &lt;td&gt;-79.385247&lt;/td&gt;
      &lt;td&gt;43.664503&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M6J 1H2&lt;/td&gt;
      &lt;td&gt;8495.0&lt;/td&gt;
      &lt;td&gt;-79.415616&lt;/td&gt;
      &lt;td&gt;43.644649&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;North York&lt;/td&gt;
      &lt;td&gt;M2N 6H8&lt;/td&gt;
      &lt;td&gt;1200.0&lt;/td&gt;
      &lt;td&gt;-79.411910&lt;/td&gt;
      &lt;td&gt;43.780085&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6826&lt;/th&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5G 2K5&lt;/td&gt;
      &lt;td&gt;2400.0&lt;/td&gt;
      &lt;td&gt;-79.383640&lt;/td&gt;
      &lt;td&gt;43.657599&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6827&lt;/th&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M4X 1L1&lt;/td&gt;
      &lt;td&gt;2975.0&lt;/td&gt;
      &lt;td&gt;-79.375682&lt;/td&gt;
      &lt;td&gt;43.667902&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;900.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6828&lt;/th&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5A 2E4&lt;/td&gt;
      &lt;td&gt;3195.0&lt;/td&gt;
      &lt;td&gt;-79.374276&lt;/td&gt;
      &lt;td&gt;43.660670&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6829&lt;/th&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5A 1N6&lt;/td&gt;
      &lt;td&gt;2150.0&lt;/td&gt;
      &lt;td&gt;-79.366487&lt;/td&gt;
      &lt;td&gt;43.652770&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6830&lt;/th&gt;
      &lt;td&gt;York&lt;/td&gt;
      &lt;td&gt;M6N 2S8&lt;/td&gt;
      &lt;td&gt;2500.0&lt;/td&gt;
      &lt;td&gt;-79.491092&lt;/td&gt;
      &lt;td&gt;43.682418&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;6831 rows × 8 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id=&#34;infer-missing-values&#34;&gt;Infer missing values&lt;/h4&gt;
&lt;p&gt;A feature like the square footage should carry a lot of importance in determining price. Since we only have ~1700 rows with sqft data, we would like to infer it from the other (non-price) features. First let&amp;rsquo;s take a look at the sqft data itself.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, axes = plt.subplots(1, 2, figsize=(15, 5))

sns.boxplot(df.sqft, ax=axes[0])
sns.kdeplot(df.sqft, ax=axes[1])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x7f223bb5d5b0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/rent-prices-toronto/Rent-Prices-Toronto_16_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[df.sqft &amp;gt; 100000]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;city&lt;/th&gt;
      &lt;th&gt;postal_code&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;bedrooms&lt;/th&gt;
      &lt;th&gt;bathrooms&lt;/th&gt;
      &lt;th&gt;sqft&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;446&lt;/th&gt;
      &lt;td&gt;Vaughan&lt;/td&gt;
      &lt;td&gt;L4K 1W8&lt;/td&gt;
      &lt;td&gt;2350.0&lt;/td&gt;
      &lt;td&gt;-79.521298&lt;/td&gt;
      &lt;td&gt;43.795942&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;800899.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6578&lt;/th&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5J 0B1&lt;/td&gt;
      &lt;td&gt;3000.0&lt;/td&gt;
      &lt;td&gt;-79.382245&lt;/td&gt;
      &lt;td&gt;43.641850&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;700799.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;These seem to be clear errors. The owner probably meant to write a range like 800-899 sqft, so we&amp;rsquo;ll pick the range middle point as value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.loc[446, &#39;sqft&#39;] = 850
df.loc[6578, &#39;sqft&#39;] = 750
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, axes = plt.subplots(1, 2, figsize=(15, 5))

sns.boxplot(df.sqft, ax=axes[0])
sns.kdeplot(df.sqft, ax=axes[1])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x7f223b4b1640&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/rent-prices-toronto/Rent-Prices-Toronto_20_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[df.sqft &amp;gt; 5000]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;city&lt;/th&gt;
      &lt;th&gt;postal_code&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;bedrooms&lt;/th&gt;
      &lt;th&gt;bathrooms&lt;/th&gt;
      &lt;th&gt;sqft&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;990&lt;/th&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M6B 2Z1&lt;/td&gt;
      &lt;td&gt;9900.0&lt;/td&gt;
      &lt;td&gt;-79.426037&lt;/td&gt;
      &lt;td&gt;43.705619&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;5900.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5451&lt;/th&gt;
      &lt;td&gt;North York&lt;/td&gt;
      &lt;td&gt;M2M 2E4&lt;/td&gt;
      &lt;td&gt;3800.0&lt;/td&gt;
      &lt;td&gt;-79.414209&lt;/td&gt;
      &lt;td&gt;43.787862&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;20000.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5829&lt;/th&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5R 2T8&lt;/td&gt;
      &lt;td&gt;2000.0&lt;/td&gt;
      &lt;td&gt;-79.406443&lt;/td&gt;
      &lt;td&gt;43.672597&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;6700.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;These datapoints seem more genuine (that&amp;rsquo;s one big house, but it&amp;rsquo;s also in North York).&lt;/p&gt;
&lt;p&gt;We can infer the missing values from the other features using 
&lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iterative imputing&lt;/a&gt;. In short, for each feature with missing data, one can train a regression model on the other features and fill the missing values with the predicted ones.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Infer missing values
impute_columns = df[[&#39;bedrooms&#39;, &#39;bathrooms&#39;, &#39;sqft&#39;, &#39;latitude&#39;, &#39;longitude&#39;]]
other_columns = df.drop([&#39;bedrooms&#39;, &#39;bathrooms&#39;, &#39;sqft&#39;, &#39;latitude&#39;, &#39;longitude&#39;], axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;impute_columns
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bedrooms&lt;/th&gt;
      &lt;th&gt;bathrooms&lt;/th&gt;
      &lt;th&gt;sqft&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;43.652523&lt;/td&gt;
      &lt;td&gt;-79.431071&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;43.686387&lt;/td&gt;
      &lt;td&gt;-79.571523&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;43.664503&lt;/td&gt;
      &lt;td&gt;-79.385247&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;43.644649&lt;/td&gt;
      &lt;td&gt;-79.415616&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;43.780085&lt;/td&gt;
      &lt;td&gt;-79.411910&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6826&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;43.657599&lt;/td&gt;
      &lt;td&gt;-79.383640&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6827&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;900.0&lt;/td&gt;
      &lt;td&gt;43.667902&lt;/td&gt;
      &lt;td&gt;-79.375682&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6828&lt;/th&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;43.660670&lt;/td&gt;
      &lt;td&gt;-79.374276&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6829&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;43.652770&lt;/td&gt;
      &lt;td&gt;-79.366487&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6830&lt;/th&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;43.682418&lt;/td&gt;
      &lt;td&gt;-79.491092&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;6831 rows × 5 columns&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Instantiate imputer
imp = IterativeImputer(max_iter=10, random_state=42, initial_strategy=&#39;median&#39;)

# Fit imputer on data
imp.fit(impute_columns)

# Transform impute_columns
filled_columns = pd.DataFrame(imp.transform(impute_columns), columns=impute_columns.columns)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;filled_columns
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bedrooms&lt;/th&gt;
      &lt;th&gt;bathrooms&lt;/th&gt;
      &lt;th&gt;sqft&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.643610&lt;/td&gt;
      &lt;td&gt;1.333009&lt;/td&gt;
      &lt;td&gt;809.493593&lt;/td&gt;
      &lt;td&gt;43.652523&lt;/td&gt;
      &lt;td&gt;-79.431071&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;946.382590&lt;/td&gt;
      &lt;td&gt;43.686387&lt;/td&gt;
      &lt;td&gt;-79.571523&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;548.168647&lt;/td&gt;
      &lt;td&gt;43.664503&lt;/td&gt;
      &lt;td&gt;-79.385247&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.617246&lt;/td&gt;
      &lt;td&gt;1.311559&lt;/td&gt;
      &lt;td&gt;791.577622&lt;/td&gt;
      &lt;td&gt;43.644649&lt;/td&gt;
      &lt;td&gt;-79.415616&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1.927558&lt;/td&gt;
      &lt;td&gt;1.525194&lt;/td&gt;
      &lt;td&gt;972.947696&lt;/td&gt;
      &lt;td&gt;43.780085&lt;/td&gt;
      &lt;td&gt;-79.411910&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6826&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;545.054086&lt;/td&gt;
      &lt;td&gt;43.657599&lt;/td&gt;
      &lt;td&gt;-79.383640&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6827&lt;/th&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;900.000000&lt;/td&gt;
      &lt;td&gt;43.667902&lt;/td&gt;
      &lt;td&gt;-79.375682&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6828&lt;/th&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1237.402968&lt;/td&gt;
      &lt;td&gt;43.660670&lt;/td&gt;
      &lt;td&gt;-79.374276&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6829&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;539.186295&lt;/td&gt;
      &lt;td&gt;43.652770&lt;/td&gt;
      &lt;td&gt;-79.366487&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6830&lt;/th&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;1716.988502&lt;/td&gt;
      &lt;td&gt;43.682418&lt;/td&gt;
      &lt;td&gt;-79.491092&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;6831 rows × 5 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We should have filled most of our missing data. Let&amp;rsquo;s compare missing values pre and post imputing.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Missing values pre-imputing
df.isna().sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;city              0
postal_code      21
price             0
longitude         0
latitude          0
bedrooms        899
bathrooms      1013
sqft           5182
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Merge filled_columns and other_columns back
df = pd.merge(filled_columns, other_columns, left_index=True, right_index=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Missing values post-imputing
df.isna().sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;bedrooms        0
bathrooms       0
sqft            0
latitude        0
longitude       0
city            0
postal_code    21
price           0
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(6831, 8)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;the-city-and-postal-code-columns&#34;&gt;The city and postal code columns&lt;/h4&gt;
&lt;p&gt;Finally we look at these two categorical columns.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.city.value_counts()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Toronto        4616
North York      970
Scarborough     411
Etobicoke       386
York            161
Vaughan         107
East York        99
Mississauga      49
Markham          30
Brampton          2
Name: city, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.postal_code.value_counts()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;None       57
M5B 2C2    52
M5A 1Z4    42
L4K 2M7    36
M5A 2Y8    33
           ..
M4S 1V5     1
M6K 2X9     1
M4B 1K7     1
M5A 4M8     1
M5A 1K2     1
Name: postal_code, Length: 2614, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The postal code column has missing data entered as &amp;lsquo;None&amp;rsquo; on top of the 21 NaN listed above. We can convert them to NaN and, at this point, we will drop all remaining rows with missing data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Drop remaining rows with missing values
df = df.replace(&#39;None&#39;, np.NaN)
df = df.dropna()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(6753, 8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the 6 digit postal code seems like it&amp;rsquo;s a bit too granular as we have 2614 distinct entries. We&amp;rsquo;ll extract the first 3 digits of each code to group properties within similar areas.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Extract first 3 digits of postal_code
df.postal_code = df.postal_code.apply(lambda x: x[:3])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.postal_code.value_counts()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;M5V    1001
M4Y     456
M5A     367
M5J     302
M2N     299
       ... 
m4v       1
M5Y       1
A1A       1
l4X       1
M3E       1
Name: postal_code, Length: 115, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bedrooms&lt;/th&gt;
      &lt;th&gt;bathrooms&lt;/th&gt;
      &lt;th&gt;sqft&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;city&lt;/th&gt;
      &lt;th&gt;postal_code&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.643610&lt;/td&gt;
      &lt;td&gt;1.333009&lt;/td&gt;
      &lt;td&gt;809.493593&lt;/td&gt;
      &lt;td&gt;43.652523&lt;/td&gt;
      &lt;td&gt;-79.431071&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M6J&lt;/td&gt;
      &lt;td&gt;2999.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;946.382590&lt;/td&gt;
      &lt;td&gt;43.686387&lt;/td&gt;
      &lt;td&gt;-79.571523&lt;/td&gt;
      &lt;td&gt;Etobicoke&lt;/td&gt;
      &lt;td&gt;M9R&lt;/td&gt;
      &lt;td&gt;1950.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;548.168647&lt;/td&gt;
      &lt;td&gt;43.664503&lt;/td&gt;
      &lt;td&gt;-79.385247&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M4Y&lt;/td&gt;
      &lt;td&gt;1950.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.617246&lt;/td&gt;
      &lt;td&gt;1.311559&lt;/td&gt;
      &lt;td&gt;791.577622&lt;/td&gt;
      &lt;td&gt;43.644649&lt;/td&gt;
      &lt;td&gt;-79.415616&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M6J&lt;/td&gt;
      &lt;td&gt;8495.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1.927558&lt;/td&gt;
      &lt;td&gt;1.525194&lt;/td&gt;
      &lt;td&gt;972.947696&lt;/td&gt;
      &lt;td&gt;43.780085&lt;/td&gt;
      &lt;td&gt;-79.411910&lt;/td&gt;
      &lt;td&gt;North York&lt;/td&gt;
      &lt;td&gt;M2N&lt;/td&gt;
      &lt;td&gt;1200.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6826&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;545.054086&lt;/td&gt;
      &lt;td&gt;43.657599&lt;/td&gt;
      &lt;td&gt;-79.383640&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5G&lt;/td&gt;
      &lt;td&gt;2400.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6827&lt;/th&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;900.000000&lt;/td&gt;
      &lt;td&gt;43.667902&lt;/td&gt;
      &lt;td&gt;-79.375682&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M4X&lt;/td&gt;
      &lt;td&gt;2975.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6828&lt;/th&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1237.402968&lt;/td&gt;
      &lt;td&gt;43.660670&lt;/td&gt;
      &lt;td&gt;-79.374276&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5A&lt;/td&gt;
      &lt;td&gt;3195.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6829&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;539.186295&lt;/td&gt;
      &lt;td&gt;43.652770&lt;/td&gt;
      &lt;td&gt;-79.366487&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5A&lt;/td&gt;
      &lt;td&gt;2150.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6830&lt;/th&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;1716.988502&lt;/td&gt;
      &lt;td&gt;43.682418&lt;/td&gt;
      &lt;td&gt;-79.491092&lt;/td&gt;
      &lt;td&gt;York&lt;/td&gt;
      &lt;td&gt;M6N&lt;/td&gt;
      &lt;td&gt;2500.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;6753 rows × 8 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;22-visualisations&#34;&gt;2.2. Visualisations&lt;/h3&gt;
&lt;h4 id=&#34;price-distribution&#34;&gt;Price distribution&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, axes = plt.subplots(1, 2, figsize=(15, 5))

sns.boxplot(df.price, ax=axes[0])
sns.kdeplot(df.price, ax=axes[1])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x7f223b347d90&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/rent-prices-toronto/Rent-Prices-Toronto_44_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are some clear price outliers. Let&amp;rsquo;s fix this by dropping the first and last 5% quantiles of price data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# drop the first and last 5% quantiles
df = df[df.price.between(df.price.quantile(0.05), df.price.quantile(0.95))]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, axes = plt.subplots(1, 2, figsize=(15, 5))

sns.boxplot(df.price, ax=axes[0])
sns.kdeplot(df.price, ax=axes[1])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x7f223b2f1d00&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/rent-prices-toronto/Rent-Prices-Toronto_47_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[df.price &amp;gt;= 3500]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bedrooms&lt;/th&gt;
      &lt;th&gt;bathrooms&lt;/th&gt;
      &lt;th&gt;sqft&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;city&lt;/th&gt;
      &lt;th&gt;postal_code&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;15&lt;/th&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;1332.060845&lt;/td&gt;
      &lt;td&gt;43.640527&lt;/td&gt;
      &lt;td&gt;-79.397104&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5V&lt;/td&gt;
      &lt;td&gt;3800.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;19&lt;/th&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;1331.951335&lt;/td&gt;
      &lt;td&gt;43.685292&lt;/td&gt;
      &lt;td&gt;-79.319300&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M4C&lt;/td&gt;
      &lt;td&gt;3750.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;43&lt;/th&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;1584.904699&lt;/td&gt;
      &lt;td&gt;43.781543&lt;/td&gt;
      &lt;td&gt;-79.405602&lt;/td&gt;
      &lt;td&gt;North York&lt;/td&gt;
      &lt;td&gt;M2N&lt;/td&gt;
      &lt;td&gt;3600.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;79&lt;/th&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;995.672755&lt;/td&gt;
      &lt;td&gt;43.671801&lt;/td&gt;
      &lt;td&gt;-79.387633&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M4W&lt;/td&gt;
      &lt;td&gt;3800.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;117&lt;/th&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;989.526746&lt;/td&gt;
      &lt;td&gt;43.655180&lt;/td&gt;
      &lt;td&gt;-79.389638&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5T&lt;/td&gt;
      &lt;td&gt;3500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6645&lt;/th&gt;
      &lt;td&gt;1.593235&lt;/td&gt;
      &lt;td&gt;1.291247&lt;/td&gt;
      &lt;td&gt;774.670649&lt;/td&gt;
      &lt;td&gt;43.638332&lt;/td&gt;
      &lt;td&gt;-79.397831&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5V&lt;/td&gt;
      &lt;td&gt;3500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6663&lt;/th&gt;
      &lt;td&gt;1.706047&lt;/td&gt;
      &lt;td&gt;1.370630&lt;/td&gt;
      &lt;td&gt;841.911221&lt;/td&gt;
      &lt;td&gt;43.685679&lt;/td&gt;
      &lt;td&gt;-79.404689&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M4V&lt;/td&gt;
      &lt;td&gt;3500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6781&lt;/th&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;1336.474829&lt;/td&gt;
      &lt;td&gt;43.658836&lt;/td&gt;
      &lt;td&gt;-79.384655&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5G&lt;/td&gt;
      &lt;td&gt;3500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6786&lt;/th&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;993.051122&lt;/td&gt;
      &lt;td&gt;43.665517&lt;/td&gt;
      &lt;td&gt;-79.387096&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5S&lt;/td&gt;
      &lt;td&gt;3500.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6812&lt;/th&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;985.164964&lt;/td&gt;
      &lt;td&gt;43.638521&lt;/td&gt;
      &lt;td&gt;-79.399464&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5V&lt;/td&gt;
      &lt;td&gt;3500.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;197 rows × 8 columns&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[df.price &amp;lt;= 1700]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bedrooms&lt;/th&gt;
      &lt;th&gt;bathrooms&lt;/th&gt;
      &lt;th&gt;sqft&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;city&lt;/th&gt;
      &lt;th&gt;postal_code&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;189&lt;/th&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;800.000000&lt;/td&gt;
      &lt;td&gt;43.663109&lt;/td&gt;
      &lt;td&gt;-79.375154&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5A&lt;/td&gt;
      &lt;td&gt;1659.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;196&lt;/th&gt;
      &lt;td&gt;2.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;800.000000&lt;/td&gt;
      &lt;td&gt;43.718479&lt;/td&gt;
      &lt;td&gt;-79.260475&lt;/td&gt;
      &lt;td&gt;Scarborough&lt;/td&gt;
      &lt;td&gt;M1K&lt;/td&gt;
      &lt;td&gt;1600.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;208&lt;/th&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;594.808222&lt;/td&gt;
      &lt;td&gt;43.764094&lt;/td&gt;
      &lt;td&gt;-79.415858&lt;/td&gt;
      &lt;td&gt;North York&lt;/td&gt;
      &lt;td&gt;M2N&lt;/td&gt;
      &lt;td&gt;1650.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;215&lt;/th&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;560.000000&lt;/td&gt;
      &lt;td&gt;43.614332&lt;/td&gt;
      &lt;td&gt;-79.487306&lt;/td&gt;
      &lt;td&gt;Etobicoke&lt;/td&gt;
      &lt;td&gt;M8V&lt;/td&gt;
      &lt;td&gt;1700.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;218&lt;/th&gt;
      &lt;td&gt;1.63832&lt;/td&gt;
      &lt;td&gt;1.317654&lt;/td&gt;
      &lt;td&gt;797.502978&lt;/td&gt;
      &lt;td&gt;43.663109&lt;/td&gt;
      &lt;td&gt;-79.375154&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M5A&lt;/td&gt;
      &lt;td&gt;1659.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6480&lt;/th&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;550.000000&lt;/td&gt;
      &lt;td&gt;43.669195&lt;/td&gt;
      &lt;td&gt;-79.452815&lt;/td&gt;
      &lt;td&gt;Toronto&lt;/td&gt;
      &lt;td&gt;M6N&lt;/td&gt;
      &lt;td&gt;1600.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6484&lt;/th&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;600.000000&lt;/td&gt;
      &lt;td&gt;43.609696&lt;/td&gt;
      &lt;td&gt;-79.592960&lt;/td&gt;
      &lt;td&gt;Mississauga&lt;/td&gt;
      &lt;td&gt;L4Y&lt;/td&gt;
      &lt;td&gt;1699.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6521&lt;/th&gt;
      &lt;td&gt;2.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;920.770784&lt;/td&gt;
      &lt;td&gt;43.794746&lt;/td&gt;
      &lt;td&gt;-79.273062&lt;/td&gt;
      &lt;td&gt;Scarborough&lt;/td&gt;
      &lt;td&gt;M1S&lt;/td&gt;
      &lt;td&gt;1650.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6561&lt;/th&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;589.380038&lt;/td&gt;
      &lt;td&gt;43.786449&lt;/td&gt;
      &lt;td&gt;-79.353657&lt;/td&gt;
      &lt;td&gt;North York&lt;/td&gt;
      &lt;td&gt;M2J&lt;/td&gt;
      &lt;td&gt;1600.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6630&lt;/th&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;550.865882&lt;/td&gt;
      &lt;td&gt;43.601470&lt;/td&gt;
      &lt;td&gt;-79.505850&lt;/td&gt;
      &lt;td&gt;Etobicoke&lt;/td&gt;
      &lt;td&gt;M8V&lt;/td&gt;
      &lt;td&gt;1650.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;221 rows × 8 columns&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(df.price)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2353.290710915894
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.std(df.price)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;474.7314238176986
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(6147, 8)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;location-distribution&#34;&gt;Location distribution&lt;/h4&gt;
&lt;p&gt;Since we have the Latitude/Longitude data, we can see where our datapoints are situated. We can use 
&lt;a href=&#34;https://geopandas.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;geopandas&lt;/a&gt; to overlay a scatterplot onto a plot of Toronto, or of the Greater Toronto Area.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;toronto_map = gpd.read_file(&amp;quot;./data/shapefiles/toronto/CENTRELINE_WGS84.shp&amp;quot;)
gta_map = gpd.read_file(&amp;quot;./data/shapefiles/gta/GTA2013_Index.shp&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s look at where our data is situated within the GTA.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(figsize=(15, 15))

gta_map.plot(ax=ax, alpha=0.3, color=&#39;grey&#39;)
sns.scatterplot(df.longitude, df.latitude, ax=ax, hue=df.price, palette=&#39;RdYlGn_r&#39;)
ax.set_title(&#39;Distribution of rent prices data across the Greater Toronto Area&#39;)
ax.set_xlabel(&#39;Longitude&#39;)
ax.set_ylabel(&#39;Latitude&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(109.3490879304675, 0.5, &#39;Latitude&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/rent-prices-toronto/Rent-Prices-Toronto_56_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s look at Toronto proper.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(figsize=(15, 15))

toronto_map.plot(ax=ax, alpha=0.2, color=&#39;grey&#39;)
sns.scatterplot(df.longitude, df.latitude, ax=ax, hue=df.price, palette=&#39;RdYlGn_r&#39;)
ax.set_title(&#39;Distribution of rent prices data across Toronto&#39;)
ax.set_xlabel(&#39;Longitude&#39;)
ax.set_ylabel(&#39;Latitude&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(95.5, 0.5, &#39;Latitude&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/rent-prices-toronto/Rent-Prices-Toronto_58_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are some clear concentrations of high rent places around downtown Toronto and the North York area, while rent seems to get cheaper as you go further west/east of Toronto.&lt;/p&gt;
&lt;p&gt;Next, let&amp;rsquo;s look at the distribution of postal codes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;postal_codes = df.postal_code.value_counts()

fig, ax = plt.subplots(figsize=(20, 20))
sns.barplot(x=postal_codes, y=postal_codes.index)
ax.set_xlabel(&#39;Postal code frequency&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &#39;Postal code frequency&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/rent-prices-toronto/Rent-Prices-Toronto_60_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Looking up these postal codes, we again see that the data seems heavily concentrated downtown and in North York. Let&amp;rsquo;s look at the  6 most frequent postal codes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.google.com/maps/place/Toronto,&amp;#43;ON&amp;#43;M5V/@43.6314062,-79.4116401,14z/data=!3m1!4b1!4m5!3m4!1s0x882b35252b71ef0d:0xee11d2dff47e9a1!8m2!3d43.6289467!4d-79.3944199&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M5V&lt;/a&gt; : Fashion/Entertainment District &amp;amp; Toronto Islands&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.google.com/maps/place/Toronto,&amp;#43;ON&amp;#43;M4Y/@43.6666517,-79.3903063,15z/data=!3m1!4b1!4m5!3m4!1s0x882b34b2c1714f89:0xd093238d56b31c9c!8m2!3d43.6658599!4d-79.3831599&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M4Y&lt;/a&gt; : Church &amp;amp; Wellesley Area&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.google.com/maps/place/Toronto,&amp;#43;ON&amp;#43;M5A/@43.6499487,-79.3878643,13z/data=!3m1!4b1!4m5!3m4!1s0x89d4cb16c81cbaa7:0x33cea1547f0c5278!8m2!3d43.6542599!4d-79.3606359&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M5A&lt;/a&gt; : Corktown &amp;amp; Cabbagetown South Area&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.google.com/maps/place/North&amp;#43;York,&amp;#43;ON&amp;#43;M2N/@43.7681432,-79.448698,13z/data=!3m1!4b1!4m5!3m4!1s0x882b2d6fd314b9eb:0x2f13d8da0be397e4!8m2!3d43.7701199!4d-79.4084928&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M2N&lt;/a&gt; : North York - Willowdale Area&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.google.com/maps/place/Toronto,&amp;#43;ON&amp;#43;M5J/@43.6297434,-79.4014638,13z/data=!4m5!3m4!1s0x89d4cada7048225d:0x8ff689c9a9de81d0!8m2!3d43.6408157!4d-79.3817523&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M5J&lt;/a&gt; : Waterfront &amp;amp; Toronto Islands&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.google.com/maps/place/Toronto,&amp;#43;ON&amp;#43;M5B/@43.6530176,-79.3988573,14.1z/data=!4m5!3m4!1s0x89d4cb356600009f:0x5815e4db84791f34!8m2!3d43.6571618!4d-79.3789371&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M5B&lt;/a&gt; : Ryerson University Area&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next, let&amp;rsquo;s look at the longtitude/latitude distributions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, axes = plt.subplots(1, 2, figsize=(15,5))

sns.boxplot(df.longitude, ax=axes[0])
sns.kdeplot(df.longitude, ax=axes[1])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x7f222209b6d0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/rent-prices-toronto/Rent-Prices-Toronto_62_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, axes = plt.subplots(1, 2, figsize=(15,5))

sns.boxplot(df.latitude, ax=axes[0])
sns.kdeplot(df.latitude, ax=axes[1])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x7f222207b3d0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/rent-prices-toronto/Rent-Prices-Toronto_63_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s clear our data is concentrated in a vertical corridor centered on downtown. We also see small bumps in the Yonge/Eglinton and North York areas on the latitude plots.&lt;/p&gt;
&lt;h4 id=&#34;features-scatter-matrix&#34;&gt;Features scatter matrix&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.pairplot(df.sample(500), hue=&#39;price&#39;, diag_kind=&#39;hist&#39;, palette=&#39;RdYlGn_r&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;seaborn.axisgrid.PairGrid at 0x7f22220f4640&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/rent-prices-toronto/Rent-Prices-Toronto_66_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;23-onehotencode&#34;&gt;2.3. OneHotEncode&lt;/h3&gt;
&lt;p&gt;Finally we encode categorical features (city, postal code) as 0/1 variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Split categorical from numerical data
df_categorical = df.select_dtypes(include=[object])
df_numerical = df.drop(columns=df_categorical.columns, axis=1)
    
# Fit one-hot encoder on categorical data
onehot = OneHotEncoder(handle_unknown=&#39;ignore&#39;)
onehot.fit(df_categorical)

# Transform categorical data
df_onehot = pd.DataFrame(onehot.transform(df_categorical).toarray(), 
                         columns=onehot.get_feature_names(),
                         index=df_categorical.index)

# Merge data
df = pd.merge(df_numerical, df_onehot, left_index=True, right_index=True)

# Display
df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bedrooms&lt;/th&gt;
      &lt;th&gt;bathrooms&lt;/th&gt;
      &lt;th&gt;sqft&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;x0_Brampton&lt;/th&gt;
      &lt;th&gt;x0_East York&lt;/th&gt;
      &lt;th&gt;x0_Etobicoke&lt;/th&gt;
      &lt;th&gt;x0_Markham&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;x1_M9M&lt;/th&gt;
      &lt;th&gt;x1_M9N&lt;/th&gt;
      &lt;th&gt;x1_M9P&lt;/th&gt;
      &lt;th&gt;x1_M9R&lt;/th&gt;
      &lt;th&gt;x1_M9V&lt;/th&gt;
      &lt;th&gt;x1_M9W&lt;/th&gt;
      &lt;th&gt;x1_l4X&lt;/th&gt;
      &lt;th&gt;x1_m4g&lt;/th&gt;
      &lt;th&gt;x1_m4v&lt;/th&gt;
      &lt;th&gt;x1_m6h&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.64361&lt;/td&gt;
      &lt;td&gt;1.333009&lt;/td&gt;
      &lt;td&gt;809.493593&lt;/td&gt;
      &lt;td&gt;43.652523&lt;/td&gt;
      &lt;td&gt;-79.431071&lt;/td&gt;
      &lt;td&gt;2999.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;946.382590&lt;/td&gt;
      &lt;td&gt;43.686387&lt;/td&gt;
      &lt;td&gt;-79.571523&lt;/td&gt;
      &lt;td&gt;1950.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;548.168647&lt;/td&gt;
      &lt;td&gt;43.664503&lt;/td&gt;
      &lt;td&gt;-79.385247&lt;/td&gt;
      &lt;td&gt;1950.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;2.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;894.076951&lt;/td&gt;
      &lt;td&gt;43.661927&lt;/td&gt;
      &lt;td&gt;-79.386511&lt;/td&gt;
      &lt;td&gt;2500.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;3.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1275.463512&lt;/td&gt;
      &lt;td&gt;43.717287&lt;/td&gt;
      &lt;td&gt;-79.441846&lt;/td&gt;
      &lt;td&gt;2450.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6826&lt;/th&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;545.054086&lt;/td&gt;
      &lt;td&gt;43.657599&lt;/td&gt;
      &lt;td&gt;-79.383640&lt;/td&gt;
      &lt;td&gt;2400.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6827&lt;/th&gt;
      &lt;td&gt;2.00000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;900.000000&lt;/td&gt;
      &lt;td&gt;43.667902&lt;/td&gt;
      &lt;td&gt;-79.375682&lt;/td&gt;
      &lt;td&gt;2975.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6828&lt;/th&gt;
      &lt;td&gt;3.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1237.402968&lt;/td&gt;
      &lt;td&gt;43.660670&lt;/td&gt;
      &lt;td&gt;-79.374276&lt;/td&gt;
      &lt;td&gt;3195.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6829&lt;/th&gt;
      &lt;td&gt;1.00000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;539.186295&lt;/td&gt;
      &lt;td&gt;43.652770&lt;/td&gt;
      &lt;td&gt;-79.366487&lt;/td&gt;
      &lt;td&gt;2150.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6830&lt;/th&gt;
      &lt;td&gt;4.00000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;1716.988502&lt;/td&gt;
      &lt;td&gt;43.682418&lt;/td&gt;
      &lt;td&gt;-79.491092&lt;/td&gt;
      &lt;td&gt;2500.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;6147 rows × 130 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;24-store-clean-dataset&#34;&gt;2.4. Store clean dataset&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.to_csv(&amp;quot;./data/toronto_apartment_rentals_2020_clean.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;3-model-selection&#34;&gt;3. Model selection&lt;/h2&gt;
&lt;p&gt;We now try to predict the price based on the remaining features.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load clean dataset
df = pd.read_csv(&amp;quot;./data/toronto_apartment_rentals_2020_clean.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Split into labels and features
y = df.price
X = df.drop(&#39;price&#39;, axis=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;31-algorithms-comparison&#34;&gt;3.1. Algorithms comparison&lt;/h3&gt;
&lt;p&gt;We&amp;rsquo;ll try out Lasso Regression, K-Nearest Neighbors Regression and XGBoost (a.k.a. Extreme Gradient Boosting Machines). For each model, we&amp;rsquo;ll use RandomizedSearchCV to tune their parameters and then pick the best one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Collecting models and their parameter grids to optimise over.

models = {
    &#39;lasso&#39;: {
        &#39;name&#39;: &#39;Lasso Regression&#39;,
        &#39;model&#39;: Lasso,
        &#39;param_grid&#39;: {
            &#39;alpha&#39;: [0.1, 1, 10],
            &#39;normalize&#39;: [True, False]
        }
    },
    
    &#39;knn&#39;: {
        &#39;name&#39;: &#39;K-Nearest Neighbors&#39;,
        &#39;model&#39;: KNeighborsRegressor,
        &#39;param_grid&#39;: {
            &#39;n_neighbors&#39;: list(range(1, 21)),
            &#39;p&#39;: [1, 2, 3]
        }
    },
    
    &#39;xgb&#39;: {
        &#39;name&#39;: &#39;XGBoost&#39;,
        &#39;model&#39;: XGBRegressor,
        &#39;param_grid&#39;: {
            &amp;quot;colsample_bytree&amp;quot;: uniform(0.7, 0.3),
            &amp;quot;gamma&amp;quot;: uniform(0, 0.5),
            &amp;quot;learning_rate&amp;quot;: uniform(0.03, 0.3), 
            &amp;quot;max_depth&amp;quot;: randint(2, 6), 
            &amp;quot;n_estimators&amp;quot;: randint(100, 400), 
            &amp;quot;subsample&amp;quot;: uniform(0.6, 0.4),
            &amp;quot;objective&amp;quot;: [&amp;quot;reg:squarederror&amp;quot;],
            &amp;quot;random_state&amp;quot;: [42]
        } 
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To train and compare models, we&amp;rsquo;ll use the helper function below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train_and_pick_best_model(models, features, labels, n_iter=10, verbose=False):
    &amp;quot;&amp;quot;&amp;quot;Train and tune a collection of models via RandomizedSearchCV and return 
    the best one, fitted on a training set.
    
    Args:
        models (dict): a dictionary of models containing model name, class constructor and params grid.
        features (dataframe): features to train models.
        labels (series): labels to predict.
        n_iter (int): number of iterations for RandomizedSearchCV, 
                      increase for gain of accuracy vs loss of training speed.
        verbose (bool): Set to true to print model info as they are trained/evaluated.
        
    Returns:
        fitted model trained with best RandomizedSearchCV params.
    &amp;quot;&amp;quot;&amp;quot;
    
    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.1, random_state=42)

    # Keep record of best model found
    best_model = {
        &#39;fitted_instance&#39;: None,
        &#39;R2_train_score&#39;: None,
        &#39;R2_test_score&#39;: None,
        &#39;RMSE_train_score&#39;: None,
        &#39;RMSE_test_score&#39;: None,
        &#39;name&#39;: None,
        &#39;params&#39;: None
    }
    
    for m in models:
        # Train and tune model via RandomizedSearchCV
        model = models[m][&#39;model&#39;]()
        param_grid = models[m][&#39;param_grid&#39;]
        reg = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=n_iter, random_state=42)
        reg.fit(X_train, y_train)

        # Collect model info/scores
        R2_train_score = reg.score(X_train, y_train)
        R2_test_score = reg.score(X_test, y_test)
        RMSE_train_score = mean_squared_error(y_train, reg.predict(X_train), squared=False)
        RMSE_test_score = mean_squared_error(y_test, reg.predict(X_test), squared=False)
        best_params = reg.best_params_
        
        # Print model information
        if verbose:
            print(&amp;quot;-&amp;quot;*115)
            print(&amp;quot;Model: {}&amp;quot;.format(models[m][&#39;name&#39;]))
            print(&amp;quot;R^2 train score:  {:.3f}&amp;quot;.format(R2_train_score))
            print(&amp;quot;R^2 test score:   {:.3f}&amp;quot;.format(R2_test_score))
            print(&amp;quot;RMSE train score: {:.2f}&amp;quot;.format(RMSE_train_score))
            print(&amp;quot;RMSE test score:  {:.2f}&amp;quot;.format(RMSE_test_score))
            print(&amp;quot;Best params: {}&amp;quot;.format(best_params))
        
        # Update best model found so far
        if not best_model[&#39;fitted_instance&#39;] or R2_test_score &amp;gt; best_model[&#39;R2_test_score&#39;]:
            best_model.update({
                &#39;fitted_instance&#39;: reg,
                &#39;R2_train_score&#39;: R2_train_score,
                &#39;R2_test_score&#39;: R2_test_score,
                &#39;RMSE_train_score&#39;: RMSE_train_score, 
                &#39;RMSE_test_score&#39;: RMSE_test_score,
                &#39;params&#39;: best_params,
                &#39;name&#39;: models[m][&#39;name&#39;]
            })

    # Print best model info
    if verbose:
        print(&amp;quot;=&amp;quot;*115)
        print(&amp;quot;Best model: {}&amp;quot;.format(best_model[&#39;name&#39;]))
        print(&amp;quot;R^2 train score:  {:.3f}&amp;quot;.format(best_model[&#39;R2_train_score&#39;]))
        print(&amp;quot;R^2 test score:   {:.3f}&amp;quot;.format(best_model[&#39;R2_test_score&#39;]))
        print(&amp;quot;RMSE train score: {:.2f}&amp;quot;.format(best_model[&#39;RMSE_train_score&#39;]))
        print(&amp;quot;RMSE test score:  {:.2f}&amp;quot;.format(best_model[&#39;RMSE_test_score&#39;]))
        print(&amp;quot;Params: {}&amp;quot;.format(best_model[&#39;params&#39;]))
    
    return best_model[&#39;fitted_instance&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# This should take about a minute
reg = train_and_pick_best_model(models=models, features=X, labels=y, n_iter = 20, verbose=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-------------------------------------------------------------------------------------------------------------------
Model: Lasso Regression
R^2 train score:  0.591
R^2 test score:   0.619
RMSE train score: 302.49
RMSE test score:  302.14
Best params: {&#39;normalize&#39;: False, &#39;alpha&#39;: 0.1}
-------------------------------------------------------------------------------------------------------------------
Model: K-Nearest Neighbors
R^2 train score:  0.700
R^2 test score:   0.619
RMSE train score: 258.98
RMSE test score:  302.06
Best params: {&#39;p&#39;: 1, &#39;n_neighbors&#39;: 5}
-------------------------------------------------------------------------------------------------------------------
Model: XGBoost
R^2 train score:  0.772
R^2 test score:   0.714
RMSE train score: 225.65
RMSE test score:  262.02
Best params: {&#39;colsample_bytree&#39;: 0.7121300768615294, &#39;gamma&#39;: 0.3553314448428937, &#39;learning_rate&#39;: 0.0632672462435494, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 278, &#39;objective&#39;: &#39;reg:squarederror&#39;, &#39;random_state&#39;: 42, &#39;subsample&#39;: 0.6125716742746937}
===================================================================================================================
Best model: XGBoost
R^2 train score:  0.772
R^2 test score:   0.714
RMSE train score: 225.65
RMSE test score:  262.02
Params: {&#39;colsample_bytree&#39;: 0.7121300768615294, &#39;gamma&#39;: 0.3553314448428937, &#39;learning_rate&#39;: 0.0632672462435494, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 278, &#39;objective&#39;: &#39;reg:squarederror&#39;, &#39;random_state&#39;: 42, &#39;subsample&#39;: 0.6125716742746937}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;32-make-a-prediction-based-on-user-input&#34;&gt;3.2. Make a prediction based on user input&lt;/h3&gt;
&lt;p&gt;At this point we have a trained regression model, and we want to use it to make predictions. We&amp;rsquo;ll want to accept user input in the original format for our data, and then transform it using the original Imputer and OneHotEncoder which we used to transform our data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_input = {
    &#39;bedrooms&#39;: 1.0,
    &#39;bathrooms&#39;: 1.0,
    &#39;sqft&#39;: 950,
    &#39;latitude&#39;: np.NaN,
    &#39;longitude&#39;: np.NaN,
    &#39;city&#39;: &amp;quot;Toronto&amp;quot;,
    &#39;postal_code&#39;: &amp;quot;M6R&amp;quot;, 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def transform_user_input(input_dict):
    &amp;quot;&amp;quot;&amp;quot;Prepare user input for prediction using Imputer and OneHotEncoder transformers.&amp;quot;&amp;quot;&amp;quot;
    
    columns = [&#39;bedrooms&#39;, &#39;bathrooms&#39;, &#39;sqft&#39;,&#39;latitude&#39;,&#39;longitude&#39;,&#39;city&#39;,&#39;postal_code&#39;]
    df = pd.DataFrame(input_dict, columns=columns, index=[0])
    
    # Split into categorical and numerical columns
    df_numerical = df[[&#39;bedrooms&#39;, &#39;bathrooms&#39;, &#39;sqft&#39;, &#39;latitude&#39;, &#39;longitude&#39;]]    
    df_categorical = df[[&#39;city&#39;, &#39;postal_code&#39;]]
    
    # Impute missing values in the numerical columns
    # These are: bedrooms, bathrooms, sqft, latitude, longitude
    df_impute = pd.DataFrame(imp.transform(df_numerical), 
                             columns=df_numerical.columns)
    
    # Onehot encode categorical values
    df_onehot = pd.DataFrame(onehot.transform(df_categorical).toarray(), 
                             columns=onehot.get_feature_names())

    # Return merged dataframe
    return pd.merge(df_impute, df_onehot, left_index=True, right_index=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user_input = transform_user_input(user_input)
user_input
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;bedrooms&lt;/th&gt;
      &lt;th&gt;bathrooms&lt;/th&gt;
      &lt;th&gt;sqft&lt;/th&gt;
      &lt;th&gt;latitude&lt;/th&gt;
      &lt;th&gt;longitude&lt;/th&gt;
      &lt;th&gt;x0_Brampton&lt;/th&gt;
      &lt;th&gt;x0_East York&lt;/th&gt;
      &lt;th&gt;x0_Etobicoke&lt;/th&gt;
      &lt;th&gt;x0_Markham&lt;/th&gt;
      &lt;th&gt;x0_Mississauga&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;x1_M9M&lt;/th&gt;
      &lt;th&gt;x1_M9N&lt;/th&gt;
      &lt;th&gt;x1_M9P&lt;/th&gt;
      &lt;th&gt;x1_M9R&lt;/th&gt;
      &lt;th&gt;x1_M9V&lt;/th&gt;
      &lt;th&gt;x1_M9W&lt;/th&gt;
      &lt;th&gt;x1_l4X&lt;/th&gt;
      &lt;th&gt;x1_m4g&lt;/th&gt;
      &lt;th&gt;x1_m4v&lt;/th&gt;
      &lt;th&gt;x1_m6h&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;950.0&lt;/td&gt;
      &lt;td&gt;43.679707&lt;/td&gt;
      &lt;td&gt;-79.399019&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;1 rows × 129 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;At this point we can make a prediction.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg.predict(user_input)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([2141.0542], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;handle-new-categorical-inputs&#34;&gt;Handle new categorical inputs&lt;/h4&gt;
&lt;p&gt;When initiating our OneHotEncoder, we set it to ignore unknown categorical values. This means the transformer sets them to 0, and our model can make a prediction from there (although not being aided by the additional data in that feature).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_input = {
    &#39;bedrooms&#39;: 1.0,
    &#39;bathrooms&#39;: 1.0,
    &#39;sqft&#39;: np.NaN,
    &#39;latitude&#39;: np.NaN,
    &#39;longitude&#39;: np.NaN,
    &#39;city&#39;: &amp;quot;New York&amp;quot;,
    &#39;postal_code&#39;: &amp;quot;BABASTART&amp;quot;, 
}

reg.predict(transform_user_input(new_input))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([2074.4036], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;4-model-deployment&#34;&gt;4. Model deployment&lt;/h2&gt;
&lt;h3 id=&#34;41-save-our-model-and-transformers&#34;&gt;4.1. Save our model and transformers&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;joblib.dump(reg, &amp;quot;./model/model.pkl&amp;quot;)
joblib.dump(imp, &amp;quot;./model/imp.pkl&amp;quot;)
joblib.dump(onehot, &amp;quot;./model/onehot.pkl&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;./model/onehot.pkl&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;42-deploy-to-heroku&#34;&gt;4.2. Deploy to Heroku&lt;/h3&gt;
&lt;p&gt;At this point we can deploy our model. See &lt;a href=&#34;https://predict-toronto-rent.herokuapp.com/&#34;&gt;https://predict-toronto-rent.herokuapp.com/&lt;/a&gt; for a live version on the Heroku platform!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Space Basketball Probability</title>
      <link>https://vgelinas.github.io/post/space-basketball-probability/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://vgelinas.github.io/post/space-basketball-probability/</guid>
      <description>&lt;p&gt;This comic was posted by 
&lt;a href=&#34;https://xkcd.com/2328/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;xkcd&lt;/a&gt; last friday:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/xkcd/space_basketball.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I thought I&amp;rsquo;d estimate the number of throws needed to make 30 shots in a row. How long will this take?&lt;/p&gt;
&lt;h3 id=&#34;a-coin-flipping-problem&#34;&gt;A coin flipping problem&lt;/h3&gt;
&lt;p&gt;This problem is pretty much the same as the following coin flipping problem: What is the expected number of coin flips needed before you get N consecutive tails?&lt;/p&gt;
&lt;p&gt;This is a pretty well-known problem, and google will tell you a lot. The only difference is the probability of success (50% for a fair coin, and 30% for a free throw), but let&amp;rsquo;s start simple.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;  
&lt;h4 id=&#34;expected-number-of-flips-before-getting-one-tail&#34;&gt;Expected number of flips before getting one tail&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s start with getting a single tail. Intuitively, we should expect 2 flips are needed on average to get a tail, but let&amp;rsquo;s work it out.&lt;/p&gt;
&lt;p&gt;The expected value E is the weighted average over possible outcomes, weighted by the odds of each outcome. In this scenario, the number of flips required could be k = 1, 2, 3, 4, &amp;hellip; and so on. For each number of flips k required, we then figure out the odds of that outcome:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;k = 1: These are the odds of getting tail at the start. The probability is 1/2.&lt;/li&gt;
&lt;li&gt;k = 2: These are the odds of getting head, then tail. This comes out to (1/2)(1/2) = 1/4.&lt;/li&gt;
&lt;li&gt;k = 3: These are the odds of getting head, head, then tail. This is $(1/2)^3 = 1/8$.&lt;/li&gt;
&lt;li&gt;$\ldots$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Following the pattern, if you require exactly $k$ flips then you had $k-1$ heads followed by a tail, and the odds of this are $(1/2)^k$. The expected value is then the sum
\begin{align*}
E &amp;amp;= \sum_{outcome} ({\rm outcome})*({\rm odds}) \newline
&amp;amp;= \ \ \sum_{k = 1}^{\infty} k  (1/2)^k \newline
&amp;amp;= 1 \cdot \frac{1}{2} + 2 \cdot \frac{1}{4} + 3 \cdot \frac{1}{8} + 4 \cdot \frac{1}{16} + &amp;hellip;
\end{align*}
This is a known variant of the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Geometric_series#Geometric_power_series&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;geometric series&lt;/a&gt;. The general sum can be expressed as
\begin{align*}
\sum_{k = 1}^{\infty} kx^k  = \frac{x}{(x-1)^2}
\end{align*}
valid for all $|x| &amp;lt; 1$. Setting $x = 1/2$ then gives $E = 2$.&lt;/p&gt;
&lt;p&gt;So, our initial guess was right: you&amp;rsquo;d expect to need 2 flips before getting a tail.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;  
&lt;h4 id=&#34;expected-number-of-flips-to-get-two-consecutive-tails&#34;&gt;Expected number of flips to get two consecutive tails&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s consider a slightly more complex case. This time, we want the expected number of flips before getting two tails in a row.&lt;/p&gt;
&lt;p&gt;Just as before we could list all possible scenarios, with the odds of each scenario, and take the weighted average. Instead, we will use a nice trick to simplify our calculations: if you fail after one or two flips, then your expected number E of flips increases by that amount!&lt;/p&gt;
&lt;p&gt;To see what this means, let&amp;rsquo;s run over the possible scenarios but only for the first two flips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the first flip is head, then your expected value at this point is $E+1$. This happens with probability 1/2.&lt;/li&gt;
&lt;li&gt;If the first flip is tail, and then head, then your expected value becomes $E+2$. This happens with probability 1/4.&lt;/li&gt;
&lt;li&gt;If your first two flips are tail, then you are done. This happens with probability 1/4.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since this covers all possible outcomes, the expected value can then be computed as the sum of expected values for each outcome, weighted by the odds of that outcome. This gives&lt;br&gt;
\begin{align*}
E &amp;amp;= \frac{1}{2}(E+1) + \frac{1}{4}(E+2) + \frac{1}{4}(2).
\end{align*}
This simplifies to
\begin{align*}
E &amp;amp;= (\frac{1}{2} + \frac{1}{4})E + (\frac{1}{2} + \frac{2}{4} + \frac{2}{4}) \newline
&amp;amp;= \frac{3}{4}E + \frac{6}{4}.
\end{align*}
Solving gives $E=6$. So we&amp;rsquo;d expect about 6 flips before seeing two consecutive tails.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;  
&lt;h4 id=&#34;expected-number-of-flips-to-get-n-consecutive-tails&#34;&gt;Expected number of flips to get N consecutive tails&lt;/h4&gt;
&lt;p&gt;We can use the same trick as above to go from $2$ to $N$. This time, we look at the possible scenarios for the first N flips and the expected value of each scenario.&lt;/p&gt;
&lt;p&gt;If your coin flips start as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a head, then your expected value becomes $E+1$. The odds of this are 1/2.&lt;/li&gt;
&lt;li&gt;a tail, followed by a head, your expected value becomes $E+2$. The odds are 1/4.&lt;/li&gt;
&lt;li&gt;tail, tail, then head, then your expected value becomes $E+3$. The odds are 1/8.&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;li&gt;(N-1) tails, then head, then your expected value becomes $E+N$. The odds are (1/2)^N.&lt;/li&gt;
&lt;li&gt;N tails, then you&amp;rsquo;re done. The odds of this are also (1/2)^N.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Summing up the expected value of each outcome, we obtain&lt;/p&gt;
&lt;p&gt;\begin{align*}
E &amp;amp;= \frac{1}{2}(E+1) + \frac{1}{4}(E+2) + \frac{1}{8}(E+3) + &amp;hellip; + \frac{1}{2^N}(E+N) + \frac{1}{2^N}  N.
\end{align*}&lt;/p&gt;
&lt;p&gt;Collecting terms, this simplifies to
\begin{align*}
E = \left(\sum_{k=1}^N \frac{1}{2^k}\right) E + \left( \sum_{k=1}^N \frac{k}{2^k} \right) + \frac{1}{2^N} N.&lt;br&gt;
\end{align*}&lt;/p&gt;
&lt;p&gt;As before, these sums are variants of the geometric series. Their general values are
\begin{align*}
&amp;amp;\sum_{k=1}^N x^k = \frac{x^{N+1} - 1}{x-1}\newline
&amp;amp;\sum_{k=1}^N kx^k = \frac{(Nx - N - 1)x^{N+1} + x}{(x-1)^2}.
\end{align*}&lt;/p&gt;
&lt;p&gt;Setting $x = \frac{1}{2}$ and solving for $E$ gives $E = 2^{N+1} - 2$. Hence we should expect to flip on average $2^{N+1} - 2$ coins before getting $N$ consecutive tails.&lt;/p&gt;
&lt;p&gt;As we see the number of coin flips needed grows rapidly with $N$. Here are the first six values:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/xkcd/coinflips.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;  
&lt;h3 id=&#34;back-to-space-basketball&#34;&gt;Back to Space Basketball&lt;/h3&gt;
&lt;p&gt;We can use a similar logic to solve our original problem. In the coin problem the odds of success/failure were 50%/50%, while our free throw success/failure rates were 30%/70%.&lt;/p&gt;
&lt;p&gt;In general, denote by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;p: the chance of success for free throws.&lt;/li&gt;
&lt;li&gt;q: the chance of failure for free throws.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our setting these values are $p = 0.30$ and $q = 1 - p = 0.70$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compute the expected number E of free throws needed to make N shots in a row. Like the previous case, we can break down the expected value in terms of what happens on the first $N$ throws:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the 1st throw is a failure, then the expectation becomes $E+1$. The odds of this are $q$.&lt;/li&gt;
&lt;li&gt;If we get success, then failure, the expectation is $E+2$. The odds of this are $pq$.&lt;/li&gt;
&lt;li&gt;If we get success, success, then failure, the expectation becomes $E+3$ with odds $p^2q$.&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;li&gt;If we get (N-1) successes then failure, the expectation is $E+N$ with odds $p^{N-1}q$.&lt;/li&gt;
&lt;li&gt;If we get N successes, we are done. The odds of this are $p^N$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The expected value $E$ is again the sum of expectations over all outcomes, weighted by the odds of each outcome. In other words,&lt;/p&gt;
&lt;p&gt;$$ E = q(E+1) + pq(E+2) + p^2q(E+3) + &amp;hellip; + p^{N-1}q(E+N) + p^N N. $$&lt;/p&gt;
&lt;p&gt;Solving this using the same technique as above (and remembering $q = 1 - p$), we obtain&lt;/p&gt;
&lt;p&gt;$$ E = \frac{1-p^N}{p^N(1-p)}. $$&lt;/p&gt;
&lt;p&gt;Now, our free throw accuracy is 30%, and so our expected number of shots before getting N=30 consecutive shots in is roughly&lt;/p&gt;
&lt;p&gt;$$ E \approx 6.93848×10^{15} = 6,938,480,000,000,000.$$&lt;/p&gt;
&lt;p&gt;A non-starter.&lt;/p&gt;
&lt;p&gt;Kawhi Leonard is 
&lt;a href=&#34;https://www.teamrankings.com/nba/player-stat/free-throw-percentage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;listed&lt;/a&gt; as having free throw accuracy of 88.9% this season. His expected number of shots to make 30 in a row, for comparison, is about&lt;/p&gt;
&lt;p&gt;$$ E \approx 298 $$&lt;/p&gt;
&lt;p&gt;which is way more manageable.&lt;/p&gt;
&lt;p&gt;We can get professional player stats for the 2020 Basketball season from 
&lt;a href=&#34;https://www.teamrankings.com/nba/player-stat/free-throw-percentage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;various&lt;/a&gt; 
&lt;a href=&#34;https://www.basketball-reference.com/leagues/NBA_stats_per_game.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sites&lt;/a&gt;. For example, the average NBA player had a free throw success rate of 77.1% in the last season. Collecting a few players, we can see how long each player needs to make 30 shots in a row.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;player&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;free throw success rate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;expected value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;Devin Booker&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.916&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;154&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;Kawhi Leonard&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.889&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;298&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;Kyle Lowry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.861&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;634&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;Average NBA Player&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.771&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10675&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;LeBron James&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.697&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;166558&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>A Simple Dashboard in Flask</title>
      <link>https://vgelinas.github.io/post/flask-dashboard/</link>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://vgelinas.github.io/post/flask-dashboard/</guid>
      <description>&lt;p&gt;The 
&lt;a href=&#34;https://flask.palletsprojects.com/en/1.1.x/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Flask&lt;/a&gt; python library can help create quick and minimalist dashboards. In this project, I created a mini-dashboard with a mix of Python, SQL and Flask to help me visualise smartwear data. This is the result:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/dashboard/dashboard.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;project-structure&#34;&gt;Project structure&lt;/h3&gt;
&lt;p&gt;The project layers are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Fitbit API wrapper: A python class that handles interactions with the Fitbit REST API, dealing with the token authentication/refresh process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Database: A MySQL database holding the Fitbit API authentication credentials, as well as tables of fitness data (continuously updated).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Server: A python class that handles communicating with the Database, and fetching/updating Fitbit API credentials.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plotting module: Creates plots from database tables.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Flask app: Serve in a beautiful webpage and voilà!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See the project code in the 
&lt;a href=&#34;https://github.com/vgelinas/data-projects/tree/master/Dashboard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Income From Census Data</title>
      <link>https://vgelinas.github.io/post/predicting-income-from-census-data/</link>
      <pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://vgelinas.github.io/post/predicting-income-from-census-data/</guid>
      <description>&lt;p&gt;In this notebook we run through a basic prediction task using the 
&lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/Census&amp;#43;Income&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Census Income UCI Dataset&lt;/a&gt;. The task is to predict whether income exceeds $50k/year based on census data.&lt;/p&gt;
&lt;h3 id=&#34;dataset-information&#34;&gt;Dataset information&lt;/h3&gt;
&lt;p&gt;This dataset consists of 48842 instances and 15 features. The features take on a mix of categorical/numerical values:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;age&lt;/strong&gt;: 16+ (continuous).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;workclass&lt;/strong&gt;: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fnlwgt&lt;/strong&gt;: Final weight, see below (continuous).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;education&lt;/strong&gt;: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;education_num&lt;/strong&gt;: Total number of years of education (continuous).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;marital_status&lt;/strong&gt;: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;occupation&lt;/strong&gt;: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;relationship&lt;/strong&gt;: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;race&lt;/strong&gt;: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sex&lt;/strong&gt;: Female, Male.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;capital_gain&lt;/strong&gt;: continuous.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;capital_loss&lt;/strong&gt;: continuous.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hours_per_week&lt;/strong&gt;: Number of hours worked per week (continuous).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;native_country&lt;/strong&gt;: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&amp;amp;Tobago, Peru, Hong, Holand-Netherlands.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Income&lt;/strong&gt;: &amp;gt;50k, &amp;lt;=50k.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The dataset is already split into train-test sets of sizes (2/3, 1/3). Both sets contain missing values, denoted by &amp;lsquo;?&#39;.&lt;/p&gt;
&lt;h4 id=&#34;description-of-fnlwgt-final-weight&#34;&gt;Description of fnlwgt (final weight):&lt;/h4&gt;
&lt;p&gt;The UCI repository lists the additional information for the fnlwgt column:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The weights on the Current Population Survey (CPS) files are controlled to independent estimates of the civilian noninstitutional population of the US. These are prepared monthly for us by Population Division here at the Census Bureau. We use 3 sets of controls. These are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A single cell estimate of the population 16+ for each state.&lt;/li&gt;
&lt;li&gt;Controls for Hispanic Origin by age and sex.&lt;/li&gt;
&lt;li&gt;Controls by Race, age and sex.&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;dependencies&#34;&gt;Dependencies&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python 3+&lt;/li&gt;
&lt;li&gt;Pandas, numpy, scikit-learn&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Standard libraries
import pandas as pd
import numpy as np

# Train and evaluate models
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;1-data-cleaning-and-preprocessing&#34;&gt;1. Data cleaning and preprocessing&lt;/h2&gt;
&lt;p&gt;To be consistent in cleaning our data, we will merge the train and test sets into a full dataset and separate it later for model validation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load data
columns = [&#39;age&#39;, &#39;workclass&#39;, &#39;fnlwgt&#39;, &#39;education&#39;, &#39;education_num&#39;, &#39;marital_status&#39;, &#39;occupation&#39;, 
           &#39;relationship&#39;, &#39;race&#39;, &#39;sex&#39;, &#39;capital_gain&#39;, &#39;capital_loss&#39;, &#39;hours_per_week&#39;, &#39;native_country&#39;,
           &#39;income&#39;]

data_train = pd.read_csv(&amp;quot;./data/adult.data&amp;quot;, sep=&#39;, &#39;, header=None, names=columns, engine=&#39;python&#39;)
data_test = pd.read_csv(&amp;quot;./data/adult.test&amp;quot;, sep=&#39;, &#39;, header=None, skiprows=1, names=columns, engine=&#39;python&#39;)

display(data_train.head(3))
display(data_test.head(3))
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;workclass&lt;/th&gt;
      &lt;th&gt;fnlwgt&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;education_num&lt;/th&gt;
      &lt;th&gt;marital_status&lt;/th&gt;
      &lt;th&gt;occupation&lt;/th&gt;
      &lt;th&gt;relationship&lt;/th&gt;
      &lt;th&gt;race&lt;/th&gt;
      &lt;th&gt;sex&lt;/th&gt;
      &lt;th&gt;capital_gain&lt;/th&gt;
      &lt;th&gt;capital_loss&lt;/th&gt;
      &lt;th&gt;hours_per_week&lt;/th&gt;
      &lt;th&gt;native_country&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;State-gov&lt;/td&gt;
      &lt;td&gt;77516&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Never-married&lt;/td&gt;
      &lt;td&gt;Adm-clerical&lt;/td&gt;
      &lt;td&gt;Not-in-family&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;2174&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;Self-emp-not-inc&lt;/td&gt;
      &lt;td&gt;83311&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Married-civ-spouse&lt;/td&gt;
      &lt;td&gt;Exec-managerial&lt;/td&gt;
      &lt;td&gt;Husband&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;215646&lt;/td&gt;
      &lt;td&gt;HS-grad&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Divorced&lt;/td&gt;
      &lt;td&gt;Handlers-cleaners&lt;/td&gt;
      &lt;td&gt;Not-in-family&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;workclass&lt;/th&gt;
      &lt;th&gt;fnlwgt&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;education_num&lt;/th&gt;
      &lt;th&gt;marital_status&lt;/th&gt;
      &lt;th&gt;occupation&lt;/th&gt;
      &lt;th&gt;relationship&lt;/th&gt;
      &lt;th&gt;race&lt;/th&gt;
      &lt;th&gt;sex&lt;/th&gt;
      &lt;th&gt;capital_gain&lt;/th&gt;
      &lt;th&gt;capital_loss&lt;/th&gt;
      &lt;th&gt;hours_per_week&lt;/th&gt;
      &lt;th&gt;native_country&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;226802&lt;/td&gt;
      &lt;td&gt;11th&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Never-married&lt;/td&gt;
      &lt;td&gt;Machine-op-inspct&lt;/td&gt;
      &lt;td&gt;Own-child&lt;/td&gt;
      &lt;td&gt;Black&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;89814&lt;/td&gt;
      &lt;td&gt;HS-grad&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Married-civ-spouse&lt;/td&gt;
      &lt;td&gt;Farming-fishing&lt;/td&gt;
      &lt;td&gt;Husband&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;Local-gov&lt;/td&gt;
      &lt;td&gt;336951&lt;/td&gt;
      &lt;td&gt;Assoc-acdm&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;Married-civ-spouse&lt;/td&gt;
      &lt;td&gt;Protective-serv&lt;/td&gt;
      &lt;td&gt;Husband&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;gt;50K.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Merge datasets
data = pd.concat([data_train, data_test])
data
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;workclass&lt;/th&gt;
      &lt;th&gt;fnlwgt&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;education_num&lt;/th&gt;
      &lt;th&gt;marital_status&lt;/th&gt;
      &lt;th&gt;occupation&lt;/th&gt;
      &lt;th&gt;relationship&lt;/th&gt;
      &lt;th&gt;race&lt;/th&gt;
      &lt;th&gt;sex&lt;/th&gt;
      &lt;th&gt;capital_gain&lt;/th&gt;
      &lt;th&gt;capital_loss&lt;/th&gt;
      &lt;th&gt;hours_per_week&lt;/th&gt;
      &lt;th&gt;native_country&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;State-gov&lt;/td&gt;
      &lt;td&gt;77516&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Never-married&lt;/td&gt;
      &lt;td&gt;Adm-clerical&lt;/td&gt;
      &lt;td&gt;Not-in-family&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;2174&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;Self-emp-not-inc&lt;/td&gt;
      &lt;td&gt;83311&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Married-civ-spouse&lt;/td&gt;
      &lt;td&gt;Exec-managerial&lt;/td&gt;
      &lt;td&gt;Husband&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;215646&lt;/td&gt;
      &lt;td&gt;HS-grad&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Divorced&lt;/td&gt;
      &lt;td&gt;Handlers-cleaners&lt;/td&gt;
      &lt;td&gt;Not-in-family&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;53&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;234721&lt;/td&gt;
      &lt;td&gt;11th&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Married-civ-spouse&lt;/td&gt;
      &lt;td&gt;Handlers-cleaners&lt;/td&gt;
      &lt;td&gt;Husband&lt;/td&gt;
      &lt;td&gt;Black&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;338409&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Married-civ-spouse&lt;/td&gt;
      &lt;td&gt;Prof-specialty&lt;/td&gt;
      &lt;td&gt;Wife&lt;/td&gt;
      &lt;td&gt;Black&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;Cuba&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16276&lt;/th&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;215419&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Divorced&lt;/td&gt;
      &lt;td&gt;Prof-specialty&lt;/td&gt;
      &lt;td&gt;Not-in-family&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16277&lt;/th&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
      &lt;td&gt;321403&lt;/td&gt;
      &lt;td&gt;HS-grad&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Widowed&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
      &lt;td&gt;Other-relative&lt;/td&gt;
      &lt;td&gt;Black&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16278&lt;/th&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;374983&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Married-civ-spouse&lt;/td&gt;
      &lt;td&gt;Prof-specialty&lt;/td&gt;
      &lt;td&gt;Husband&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16279&lt;/th&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;83891&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Divorced&lt;/td&gt;
      &lt;td&gt;Adm-clerical&lt;/td&gt;
      &lt;td&gt;Own-child&lt;/td&gt;
      &lt;td&gt;Asian-Pac-Islander&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;5455&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;lt;=50K.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16280&lt;/th&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;Self-emp-inc&lt;/td&gt;
      &lt;td&gt;182148&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Married-civ-spouse&lt;/td&gt;
      &lt;td&gt;Exec-managerial&lt;/td&gt;
      &lt;td&gt;Husband&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;&amp;gt;50K.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;48842 rows × 15 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;11-relevant-features&#34;&gt;1.1. Relevant features&lt;/h3&gt;
&lt;p&gt;The value of the fnlwgt (final weight) column should have no predictive power, at least without further processing. We will drop this column.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Drop fnlwgt column
data.drop(&#39;fnlwgt&#39;, axis=1, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;12-missing-values&#34;&gt;1.2. Missing values&lt;/h3&gt;
&lt;p&gt;Missing values are represented as the string &amp;lsquo;?&#39;. We first replace them with NaNs.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Replace &#39;?&#39; with NaN
data.replace(&#39;?&#39;, np.nan, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Count missing values
data.isna().sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;age                  0
workclass         2799
education            0
education_num        0
marital_status       0
occupation        2809
relationship         0
race                 0
sex                  0
capital_gain         0
capital_loss         0
hours_per_week       0
native_country     857
income               0
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The missing values are all within categorical variables. Since we can&amp;rsquo;t usefully infer them, we drop each row with a missing value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data.dropna(axis=0, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, reset index for convenience and store full dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# reset index
data.reset_index(drop=True, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Store full dataset
data.to_csv(&amp;quot;./data/adult.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;13-encode-target-feature-as-integers&#34;&gt;1.3. Encode target feature as integers&lt;/h3&gt;
&lt;p&gt;The target feature (income column) consists of strings of the form below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data.income
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0         &amp;lt;=50K
1         &amp;lt;=50K
2         &amp;lt;=50K
3         &amp;lt;=50K
4         &amp;lt;=50K
          ...  
45217    &amp;lt;=50K.
45218    &amp;lt;=50K.
45219    &amp;lt;=50K.
45220    &amp;lt;=50K.
45221     &amp;gt;50K.
Name: income, Length: 45222, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We encode income below 50k as 0, and above 50k as 1.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data.income = data.income.str.strip(&amp;quot;.&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data.income = data.income.map({&amp;quot;&amp;lt;=50K&amp;quot;: 0, &amp;quot;&amp;gt;50K&amp;quot;: 1})
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data.income.value_counts()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    34014
1    11208
Name: income, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;workclass&lt;/th&gt;
      &lt;th&gt;education&lt;/th&gt;
      &lt;th&gt;education_num&lt;/th&gt;
      &lt;th&gt;marital_status&lt;/th&gt;
      &lt;th&gt;occupation&lt;/th&gt;
      &lt;th&gt;relationship&lt;/th&gt;
      &lt;th&gt;race&lt;/th&gt;
      &lt;th&gt;sex&lt;/th&gt;
      &lt;th&gt;capital_gain&lt;/th&gt;
      &lt;th&gt;capital_loss&lt;/th&gt;
      &lt;th&gt;hours_per_week&lt;/th&gt;
      &lt;th&gt;native_country&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;State-gov&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Never-married&lt;/td&gt;
      &lt;td&gt;Adm-clerical&lt;/td&gt;
      &lt;td&gt;Not-in-family&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;2174&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;Self-emp-not-inc&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Married-civ-spouse&lt;/td&gt;
      &lt;td&gt;Exec-managerial&lt;/td&gt;
      &lt;td&gt;Husband&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;HS-grad&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Divorced&lt;/td&gt;
      &lt;td&gt;Handlers-cleaners&lt;/td&gt;
      &lt;td&gt;Not-in-family&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;53&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;11th&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Married-civ-spouse&lt;/td&gt;
      &lt;td&gt;Handlers-cleaners&lt;/td&gt;
      &lt;td&gt;Husband&lt;/td&gt;
      &lt;td&gt;Black&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Married-civ-spouse&lt;/td&gt;
      &lt;td&gt;Prof-specialty&lt;/td&gt;
      &lt;td&gt;Wife&lt;/td&gt;
      &lt;td&gt;Black&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;Cuba&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;45217&lt;/th&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Never-married&lt;/td&gt;
      &lt;td&gt;Prof-specialty&lt;/td&gt;
      &lt;td&gt;Own-child&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;45218&lt;/th&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Divorced&lt;/td&gt;
      &lt;td&gt;Prof-specialty&lt;/td&gt;
      &lt;td&gt;Not-in-family&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Female&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;45219&lt;/th&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Married-civ-spouse&lt;/td&gt;
      &lt;td&gt;Prof-specialty&lt;/td&gt;
      &lt;td&gt;Husband&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;45220&lt;/th&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;Private&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Divorced&lt;/td&gt;
      &lt;td&gt;Adm-clerical&lt;/td&gt;
      &lt;td&gt;Own-child&lt;/td&gt;
      &lt;td&gt;Asian-Pac-Islander&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;5455&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;45221&lt;/th&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;Self-emp-inc&lt;/td&gt;
      &lt;td&gt;Bachelors&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Married-civ-spouse&lt;/td&gt;
      &lt;td&gt;Exec-managerial&lt;/td&gt;
      &lt;td&gt;Husband&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;Male&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;United-States&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;45222 rows × 14 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;14-one-hot-encode-the-categorical-values&#34;&gt;1.4. One-Hot encode the categorical values&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = pd.get_dummies(data)
data.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(45222, 104)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data.head(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;education_num&lt;/th&gt;
      &lt;th&gt;capital_gain&lt;/th&gt;
      &lt;th&gt;capital_loss&lt;/th&gt;
      &lt;th&gt;hours_per_week&lt;/th&gt;
      &lt;th&gt;income&lt;/th&gt;
      &lt;th&gt;workclass_Federal-gov&lt;/th&gt;
      &lt;th&gt;workclass_Local-gov&lt;/th&gt;
      &lt;th&gt;workclass_Private&lt;/th&gt;
      &lt;th&gt;workclass_Self-emp-inc&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;native_country_Portugal&lt;/th&gt;
      &lt;th&gt;native_country_Puerto-Rico&lt;/th&gt;
      &lt;th&gt;native_country_Scotland&lt;/th&gt;
      &lt;th&gt;native_country_South&lt;/th&gt;
      &lt;th&gt;native_country_Taiwan&lt;/th&gt;
      &lt;th&gt;native_country_Thailand&lt;/th&gt;
      &lt;th&gt;native_country_Trinadad&amp;amp;Tobago&lt;/th&gt;
      &lt;th&gt;native_country_United-States&lt;/th&gt;
      &lt;th&gt;native_country_Vietnam&lt;/th&gt;
      &lt;th&gt;native_country_Yugoslavia&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;2174&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;3 rows × 104 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;15-feature-scaling&#34;&gt;1.5. Feature scaling&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X, y = data.drop(&#39;income&#39;, axis=1), data.income
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;scaler = StandardScaler()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;scaler.fit(X.loc[:, &#39;age&#39;: &#39;hours_per_week&#39;])
X.loc[:, &#39;age&#39;: &#39;hours_per_week&#39;] = scaler.transform(X.loc[:, &#39;age&#39;: &#39;hours_per_week&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X.head(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;education_num&lt;/th&gt;
      &lt;th&gt;capital_gain&lt;/th&gt;
      &lt;th&gt;capital_loss&lt;/th&gt;
      &lt;th&gt;hours_per_week&lt;/th&gt;
      &lt;th&gt;workclass_Federal-gov&lt;/th&gt;
      &lt;th&gt;workclass_Local-gov&lt;/th&gt;
      &lt;th&gt;workclass_Private&lt;/th&gt;
      &lt;th&gt;workclass_Self-emp-inc&lt;/th&gt;
      &lt;th&gt;workclass_Self-emp-not-inc&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;native_country_Portugal&lt;/th&gt;
      &lt;th&gt;native_country_Puerto-Rico&lt;/th&gt;
      &lt;th&gt;native_country_Scotland&lt;/th&gt;
      &lt;th&gt;native_country_South&lt;/th&gt;
      &lt;th&gt;native_country_Taiwan&lt;/th&gt;
      &lt;th&gt;native_country_Thailand&lt;/th&gt;
      &lt;th&gt;native_country_Trinadad&amp;amp;Tobago&lt;/th&gt;
      &lt;th&gt;native_country_United-States&lt;/th&gt;
      &lt;th&gt;native_country_Vietnam&lt;/th&gt;
      &lt;th&gt;native_country_Yugoslavia&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.034201&lt;/td&gt;
      &lt;td&gt;1.128753&lt;/td&gt;
      &lt;td&gt;0.142888&lt;/td&gt;
      &lt;td&gt;-0.21878&lt;/td&gt;
      &lt;td&gt;-0.078120&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.866417&lt;/td&gt;
      &lt;td&gt;1.128753&lt;/td&gt;
      &lt;td&gt;-0.146733&lt;/td&gt;
      &lt;td&gt;-0.21878&lt;/td&gt;
      &lt;td&gt;-2.326738&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-0.041455&lt;/td&gt;
      &lt;td&gt;-0.438122&lt;/td&gt;
      &lt;td&gt;-0.146733&lt;/td&gt;
      &lt;td&gt;-0.21878&lt;/td&gt;
      &lt;td&gt;-0.078120&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1.093385&lt;/td&gt;
      &lt;td&gt;-1.221559&lt;/td&gt;
      &lt;td&gt;-0.146733&lt;/td&gt;
      &lt;td&gt;-0.21878&lt;/td&gt;
      &lt;td&gt;-0.078120&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;-0.798015&lt;/td&gt;
      &lt;td&gt;1.128753&lt;/td&gt;
      &lt;td&gt;-0.146733&lt;/td&gt;
      &lt;td&gt;-0.21878&lt;/td&gt;
      &lt;td&gt;-0.078120&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 103 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;2-data-modelling&#34;&gt;2. Data modelling&lt;/h2&gt;
&lt;h3 id=&#34;21-train-test-split&#34;&gt;2.1. Train test split&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;22-gradient-boosted-trees&#34;&gt;2.2. Gradient Boosted Trees&lt;/h3&gt;
&lt;p&gt;We will use gradient boosted trees, with parameters tuned via grid search cross-validation. First, let&amp;rsquo;s look at the cross-validation scores with default values to get a baseline.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Gradient boosted trees with default values
gbt = GradientBoostingClassifier(random_state=0)
kfold = KFold(n_splits=5, shuffle=True, random_state=0)
scores = cross_val_score(gbt, X, y, cv=kfold)
print(&amp;quot;Cross-validation scores: mean {:.3f}, std {:.3f}&amp;quot;.format(scores.mean(), scores.std()))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Cross-validation scores: mean 0.863, std 0.001
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;ll try tuning the following parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;max_depth (defaults to 3)&lt;/li&gt;
&lt;li&gt;n_estimators (defaults to 100)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also keep random_state=0 as above for reproducibility.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Parameter grid to optimise over
param_grid = {
    &#39;random_state&#39;: [0],
    &#39;max_depth&#39;: [2, 3, 4, 5],
    &#39;n_estimators&#39;: [50, 100, 150, 200, 250, 300],
}

# Train our model
grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Get scores and model information
print(&amp;quot;Best parameters: {}&amp;quot;.format(grid_search.best_params_))
print(&amp;quot;Best cross-validation score: {:.3f}&amp;quot;.format(grid_search.best_score_))
print(&amp;quot;Best estimator:\n{}&amp;quot;.format(grid_search.best_estimator_))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best parameters: {&#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 200, &#39;random_state&#39;: 0}
Best cross-validation score: 0.869
Best estimator:
GradientBoostingClassifier(ccp_alpha=0.0, criterion=&#39;friedman_mse&#39;, init=None,
                           learning_rate=0.1, loss=&#39;deviance&#39;, max_depth=5,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=200,
                           n_iter_no_change=None, presort=&#39;deprecated&#39;,
                           random_state=0, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With some tuning of parameters, we see that our accuracy should raise to ~0.869 using max_depth = 5 and n_estimators=200. Let&amp;rsquo;s try our newly fitted model on the test set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get train and test set scores
print(&amp;quot;Train set score: {:.3f}&amp;quot;.format(grid_search.score(X_train, y_train)))
print(&amp;quot;Test set score: {:.3f}&amp;quot;.format(grid_search.score(X_test, y_test)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train set score: 0.884
Test set score: 0.870
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our gradient boosted tree model then achieves a test set accuracy of 87%.&lt;/p&gt;
&lt;h3 id=&#34;23-further-tuning&#34;&gt;2.3. Further tuning&lt;/h3&gt;
&lt;p&gt;Should we wish to tune parameters further, it would be useful to look at the grid search history to see the accuracy as a function of parameters. This can be extracted by the cv_results_ method, as seen below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = pd.DataFrame(grid_search.cv_results_)
results.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mean_fit_time&lt;/th&gt;
      &lt;th&gt;std_fit_time&lt;/th&gt;
      &lt;th&gt;mean_score_time&lt;/th&gt;
      &lt;th&gt;std_score_time&lt;/th&gt;
      &lt;th&gt;param_max_depth&lt;/th&gt;
      &lt;th&gt;param_n_estimators&lt;/th&gt;
      &lt;th&gt;param_random_state&lt;/th&gt;
      &lt;th&gt;params&lt;/th&gt;
      &lt;th&gt;split0_test_score&lt;/th&gt;
      &lt;th&gt;split1_test_score&lt;/th&gt;
      &lt;th&gt;split2_test_score&lt;/th&gt;
      &lt;th&gt;split3_test_score&lt;/th&gt;
      &lt;th&gt;split4_test_score&lt;/th&gt;
      &lt;th&gt;mean_test_score&lt;/th&gt;
      &lt;th&gt;std_test_score&lt;/th&gt;
      &lt;th&gt;rank_test_score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.160780&lt;/td&gt;
      &lt;td&gt;0.037878&lt;/td&gt;
      &lt;td&gt;0.007860&lt;/td&gt;
      &lt;td&gt;0.000821&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;{&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 50, &#39;random_s...&lt;/td&gt;
      &lt;td&gt;0.847583&lt;/td&gt;
      &lt;td&gt;0.855079&lt;/td&gt;
      &lt;td&gt;0.854931&lt;/td&gt;
      &lt;td&gt;0.850361&lt;/td&gt;
      &lt;td&gt;0.852573&lt;/td&gt;
      &lt;td&gt;0.852105&lt;/td&gt;
      &lt;td&gt;0.002848&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.187243&lt;/td&gt;
      &lt;td&gt;0.120700&lt;/td&gt;
      &lt;td&gt;0.009864&lt;/td&gt;
      &lt;td&gt;0.000370&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;{&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 100, &#39;random_...&lt;/td&gt;
      &lt;td&gt;0.851562&lt;/td&gt;
      &lt;td&gt;0.861566&lt;/td&gt;
      &lt;td&gt;0.857880&lt;/td&gt;
      &lt;td&gt;0.853457&lt;/td&gt;
      &lt;td&gt;0.857585&lt;/td&gt;
      &lt;td&gt;0.856410&lt;/td&gt;
      &lt;td&gt;0.003531&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3.099185&lt;/td&gt;
      &lt;td&gt;0.006702&lt;/td&gt;
      &lt;td&gt;0.011799&lt;/td&gt;
      &lt;td&gt;0.000122&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;{&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 150, &#39;random_...&lt;/td&gt;
      &lt;td&gt;0.855248&lt;/td&gt;
      &lt;td&gt;0.863777&lt;/td&gt;
      &lt;td&gt;0.859502&lt;/td&gt;
      &lt;td&gt;0.856848&lt;/td&gt;
      &lt;td&gt;0.858322&lt;/td&gt;
      &lt;td&gt;0.858739&lt;/td&gt;
      &lt;td&gt;0.002895&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4.104170&lt;/td&gt;
      &lt;td&gt;0.012035&lt;/td&gt;
      &lt;td&gt;0.013877&lt;/td&gt;
      &lt;td&gt;0.000131&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;{&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 200, &#39;random_...&lt;/td&gt;
      &lt;td&gt;0.858343&lt;/td&gt;
      &lt;td&gt;0.867168&lt;/td&gt;
      &lt;td&gt;0.860386&lt;/td&gt;
      &lt;td&gt;0.859502&lt;/td&gt;
      &lt;td&gt;0.860829&lt;/td&gt;
      &lt;td&gt;0.861246&lt;/td&gt;
      &lt;td&gt;0.003081&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5.089706&lt;/td&gt;
      &lt;td&gt;0.007495&lt;/td&gt;
      &lt;td&gt;0.015999&lt;/td&gt;
      &lt;td&gt;0.000137&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;250&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;{&#39;max_depth&#39;: 2, &#39;n_estimators&#39;: 250, &#39;random_...&lt;/td&gt;
      &lt;td&gt;0.859080&lt;/td&gt;
      &lt;td&gt;0.868790&lt;/td&gt;
      &lt;td&gt;0.862893&lt;/td&gt;
      &lt;td&gt;0.860091&lt;/td&gt;
      &lt;td&gt;0.861566&lt;/td&gt;
      &lt;td&gt;0.862484&lt;/td&gt;
      &lt;td&gt;0.003408&lt;/td&gt;
      &lt;td&gt;17&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;However to keep this post short, we will leave it at that.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fitbit Data Exploration Part I</title>
      <link>https://vgelinas.github.io/post/fitbit-data-exploration-part-i/</link>
      <pubDate>Mon, 18 May 2020 00:00:00 +0000</pubDate>
      <guid>https://vgelinas.github.io/post/fitbit-data-exploration-part-i/</guid>
      <description>&lt;p&gt;In this project we will explore some Fitbit activity data pulled via 
&lt;a href=&#34;https://github.com/orcasgit/python-fitbit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;orcasgit&amp;rsquo;s python-fitbit api&lt;/a&gt;. We will go through the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data collection&lt;/li&gt;
&lt;li&gt;Data cleaning&lt;/li&gt;
&lt;li&gt;Data visualisation&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;dependencies&#34;&gt;Dependencies&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python 3+&lt;/li&gt;
&lt;li&gt;The 
&lt;a href=&#34;https://pypi.org/project/fitbit/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;python-fitbit package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The 
&lt;a href=&#34;https://pypi.org/project/ratelimit/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ratelimit package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The datetime, json, matplotlib and pandas standard libraries&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s load our packages.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import fitbit
import json
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from ratelimit import limits, sleep_and_retry

%matplotlib inline
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;1-data-collection&#34;&gt;1. Data Collection&lt;/h2&gt;
&lt;p&gt;We do this in two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We first access the API via python-fitbit, dealing with the necessary authentication steps.&lt;/li&gt;
&lt;li&gt;We then sample some responses, and build datasets by querying over a range of dates.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;11-authentication-setup&#34;&gt;1.1. Authentication setup&lt;/h3&gt;
&lt;p&gt;To collect personal data, we first need to 
&lt;a href=&#34;https://dev.fitbit.com/apps/new&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;set-up a Fitbit app&lt;/a&gt;, and to collect the client_id and client_secret for this app. For this project I&amp;rsquo;ve chosen to keep these in a credentials.json file stored in a dedicated subfolder named &amp;lsquo;oauth&amp;rsquo;, but just make sure you have these on hand.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!cat oauth/credentials.json
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&amp;quot;client_id&amp;quot;: &amp;quot;YOUR_CLIENT_ID&amp;quot;, &amp;quot;client_secret&amp;quot;: &amp;quot;YOUR_CLIENT_SECRET&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also need tokens for authentication. We need:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An access token.&lt;/li&gt;
&lt;li&gt;A refresh token.&lt;/li&gt;
&lt;li&gt;An expiration time for the access token (the refresh token never expires).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These can be obtained by going to the 
&lt;a href=&#34;https://dev.fitbit.com/apps&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Manage my apps&lt;/a&gt; section on the Fitbit website, selecting your app and navigating to &amp;ldquo;OAuth 2.0 tutorial page&amp;rdquo;. Alternatively, you can run the script &amp;ldquo;gather_keys_oauth2.py&amp;rdquo; from the python-fitbit 
&lt;a href=&#34;https://github.com/orcasgit/python-fitbit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github page&lt;/a&gt;, in which case you should set your Fitbit app&amp;rsquo;s callback URL to https://127.0.0.1:8080/.&lt;/p&gt;
&lt;p&gt;The access token serves to authenticate and typically expires after ~8 hours. The refresh token is then used to obtain a new pair (access_token, refresh_token) from the API. Similar to above, I chose to store these in a json file named &amp;lsquo;tokens&amp;rsquo;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!cat oauth/tokens.json
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&amp;quot;access_token&amp;quot;: &amp;quot;YOUR_ACCESS_TOKEN&amp;quot;, &amp;quot;expires_in&amp;quot;: 28800, &amp;quot;refresh_token&amp;quot;: &amp;quot;YOUR_REFRESH_TOKEN&amp;quot;, &amp;quot;scope&amp;quot;: [&amp;quot;location&amp;quot;, &amp;quot;heartrate&amp;quot;, &amp;quot;social&amp;quot;, &amp;quot;weight&amp;quot;, &amp;quot;settings&amp;quot;, &amp;quot;profile&amp;quot;, &amp;quot;nutrition&amp;quot;, &amp;quot;activity&amp;quot;, &amp;quot;sleep&amp;quot;], &amp;quot;token_type&amp;quot;: &amp;quot;Bearer&amp;quot;, &amp;quot;user_id&amp;quot;: &amp;quot;USER_ID&amp;quot;, &amp;quot;expires_at&amp;quot;: 1590460521.3563423}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only important keys above are &amp;ldquo;access_token&amp;rdquo;, &amp;ldquo;refresh_token&amp;rdquo; and &amp;ldquo;expires_at&amp;rdquo; (the rest corresponds to optional arguments).&lt;/p&gt;
&lt;p&gt;Next up, the code below instantiates a fitbit client which will handle API calls for us. We pass along the credentials and tokens as arguments, and we also pass a &amp;ldquo;token refresh&amp;rdquo; function which will store the new (access_token, refresh_token) pair sent by the API whenever the first one expires.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load credentials
with open(&amp;quot;./oauth/credentials.json&amp;quot;, &amp;quot;r&amp;quot;) as f:
    credentials = json.load(f)

# Load tokens
with open(&amp;quot;./oauth/tokens.json&amp;quot;, &amp;quot;r&amp;quot;) as f:  
    tokens = json.load(f)  

client_id = credentials[&#39;client_id&#39;] 
client_secret = credentials[&#39;client_secret&#39;]
access_token = tokens[&#39;access_token&#39;]
refresh_token = tokens[&#39;refresh_token&#39;]
expires_at = tokens[&#39;expires_at&#39;] 

# Token refresh method 
def refresh_callback(token):   
    &amp;quot;&amp;quot;&amp;quot; Called when the OAuth token has been refreshed &amp;quot;&amp;quot;&amp;quot; 
    with open(&amp;quot;./oauth/tokens.json&amp;quot;, &amp;quot;w&amp;quot;) as f: 
        json.dump(token, f)  

# Initialise client  
client = fitbit.Fitbit(client_id=client_id, 
                       client_secret=client_secret,
                       access_token=access_token,
                       refresh_token=refresh_token,
                       refresh_cb=refresh_callback)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first time this is called you should be served an authorisation page for authentication, but afterwards the refresh token song &amp;amp; dance should handle this in the background, and we won&amp;rsquo;t need to set it up again unless you lose your tokens.&lt;/p&gt;
&lt;h3 id=&#34;12-a-first-look-at-the-response-data&#34;&gt;1.2. A first look at the response data&lt;/h3&gt;
&lt;p&gt;The python-fitbit api supports the methods listed 
&lt;a href=&#34;https://python-fitbit.readthedocs.io/en/latest/#fitbit-api&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. For example, we could call:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;client.sleep&lt;/strong&gt;, to get basic sleep data (bed time and wake time, time awake at night, &amp;hellip;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;client.activities&lt;/strong&gt;, to get timestamps for activities (walking, running, cycling, &amp;hellip;) and summary data (number of steps, minutes active, &amp;hellip;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;client.intraday_time_series&lt;/strong&gt;, to get granular data on various activities (such as heart rate or steps rate for every minute of the day).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We&amp;rsquo;ll be interested in the activities and intraday steps data. Now, let&amp;rsquo;s take a look at the response for one date, say May 1st.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get activity data for May 1st
# The API takes a date formatted as &#39;YYYY-MM-DD&#39;
date = &#39;2020-05-01&#39;
activities_response = client.activities(date=date)

# Display response
activities_response
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;activities&#39;: [{&#39;activityId&#39;: 90013,
   &#39;activityParentId&#39;: 90013,
   &#39;activityParentName&#39;: &#39;Walk&#39;,
   &#39;calories&#39;: 302,
   &#39;description&#39;: &#39;Walking less than 2 mph, strolling very slowly&#39;,
   &#39;duration&#39;: 2714000,
   &#39;hasStartTime&#39;: True,
   &#39;isFavorite&#39;: False,
   &#39;lastModified&#39;: &#39;2020-05-01T13:10:18.000Z&#39;,
   &#39;logId&#39;: 30758911349,
   &#39;name&#39;: &#39;Walk&#39;,
   &#39;startDate&#39;: &#39;2020-05-01&#39;,
   &#39;startTime&#39;: &#39;08:20&#39;,
   &#39;steps&#39;: 4000}],
 &#39;goals&#39;: {&#39;activeMinutes&#39;: 30,
  &#39;caloriesOut&#39;: 2745,
  &#39;distance&#39;: 5,
  &#39;steps&#39;: 12500},
 &#39;summary&#39;: {&#39;activeScore&#39;: -1,
  &#39;activityCalories&#39;: 1379,
  &#39;caloriesBMR&#39;: 1659,
  &#39;caloriesOut&#39;: 2826,
  &#39;distances&#39;: [{&#39;activity&#39;: &#39;total&#39;, &#39;distance&#39;: 4.69},
   {&#39;activity&#39;: &#39;tracker&#39;, &#39;distance&#39;: 4.69},
   {&#39;activity&#39;: &#39;loggedActivities&#39;, &#39;distance&#39;: 0},
   {&#39;activity&#39;: &#39;veryActive&#39;, &#39;distance&#39;: 2.6},
   {&#39;activity&#39;: &#39;moderatelyActive&#39;, &#39;distance&#39;: 0.4},
   {&#39;activity&#39;: &#39;lightlyActive&#39;, &#39;distance&#39;: 1.68},
   {&#39;activity&#39;: &#39;sedentaryActive&#39;, &#39;distance&#39;: 0}],
  &#39;fairlyActiveMinutes&#39;: 25,
  &#39;heartRateZones&#39;: [{&#39;caloriesOut&#39;: 1916.74877,
    &#39;max&#39;: 94,
    &#39;min&#39;: 30,
    &#39;minutes&#39;: 1255,
    &#39;name&#39;: &#39;Out of Range&#39;},
   {&#39;caloriesOut&#39;: 775.70893,
    &#39;max&#39;: 132,
    &#39;min&#39;: 94,
    &#39;minutes&#39;: 137,
    &#39;name&#39;: &#39;Fat Burn&#39;},
   {&#39;caloriesOut&#39;: 81.56868,
    &#39;max&#39;: 160,
    &#39;min&#39;: 132,
    &#39;minutes&#39;: 8,
    &#39;name&#39;: &#39;Cardio&#39;},
   {&#39;caloriesOut&#39;: 0, &#39;max&#39;: 220, &#39;min&#39;: 160, &#39;minutes&#39;: 0, &#39;name&#39;: &#39;Peak&#39;}],
  &#39;lightlyActiveMinutes&#39;: 210,
  &#39;marginalCalories&#39;: 828,
  &#39;restingHeartRate&#39;: 59,
  &#39;sedentaryMinutes&#39;: 578,
  &#39;steps&#39;: 9887,
  &#39;veryActiveMinutes&#39;: 49}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s look at the type of the response object.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;type(activities_response)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dict
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The response consists of nested dictionaries. We&amp;rsquo;ll extract two datasets from the &amp;lsquo;activities&amp;rsquo; and &amp;lsquo;summary&amp;rsquo; keys.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get activities dataset
activities = activities_response[&#39;activities&#39;]
activities = pd.DataFrame(activities)
activities
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;activityId&lt;/th&gt;
      &lt;th&gt;activityParentId&lt;/th&gt;
      &lt;th&gt;activityParentName&lt;/th&gt;
      &lt;th&gt;calories&lt;/th&gt;
      &lt;th&gt;description&lt;/th&gt;
      &lt;th&gt;duration&lt;/th&gt;
      &lt;th&gt;hasStartTime&lt;/th&gt;
      &lt;th&gt;isFavorite&lt;/th&gt;
      &lt;th&gt;lastModified&lt;/th&gt;
      &lt;th&gt;logId&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;startDate&lt;/th&gt;
      &lt;th&gt;startTime&lt;/th&gt;
      &lt;th&gt;steps&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;90013&lt;/td&gt;
      &lt;td&gt;90013&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;302&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;2714000&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;2020-05-01T13:10:18.000Z&lt;/td&gt;
      &lt;td&gt;30758911349&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;2020-05-01&lt;/td&gt;
      &lt;td&gt;08:20&lt;/td&gt;
      &lt;td&gt;4000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get summary dataset
summary = activities_response[&#39;summary&#39;]

# Remove sub-dictionaries
del summary[&#39;distances&#39;]
del summary[&#39;heartRateZones&#39;]

summary = pd.DataFrame(summary, index=[0])  # all values are scalars, must pass an index
summary
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;activeScore&lt;/th&gt;
      &lt;th&gt;activityCalories&lt;/th&gt;
      &lt;th&gt;caloriesBMR&lt;/th&gt;
      &lt;th&gt;caloriesOut&lt;/th&gt;
      &lt;th&gt;fairlyActiveMinutes&lt;/th&gt;
      &lt;th&gt;lightlyActiveMinutes&lt;/th&gt;
      &lt;th&gt;marginalCalories&lt;/th&gt;
      &lt;th&gt;restingHeartRate&lt;/th&gt;
      &lt;th&gt;sedentaryMinutes&lt;/th&gt;
      &lt;th&gt;steps&lt;/th&gt;
      &lt;th&gt;veryActiveMinutes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;1379&lt;/td&gt;
      &lt;td&gt;1659&lt;/td&gt;
      &lt;td&gt;2826&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;210&lt;/td&gt;
      &lt;td&gt;828&lt;/td&gt;
      &lt;td&gt;59&lt;/td&gt;
      &lt;td&gt;578&lt;/td&gt;
      &lt;td&gt;9887&lt;/td&gt;
      &lt;td&gt;49&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Next, let&amp;rsquo;s look at the intraday step data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get intraday steps data
steps_response = client.intraday_time_series(&#39;activities/steps&#39;, base_date=date, detail_level=&amp;quot;1min&amp;quot;)

# Extract dataset from response object
steps = steps_response[&#39;activities-steps-intraday&#39;][&#39;dataset&#39;]

# Display dataset
steps = pd.DataFrame(steps)
steps
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
      &lt;th&gt;value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;00:00:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;00:01:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;00:02:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;00:03:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;00:04:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1435&lt;/th&gt;
      &lt;td&gt;23:55:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1436&lt;/th&gt;
      &lt;td&gt;23:56:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1437&lt;/th&gt;
      &lt;td&gt;23:57:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1438&lt;/th&gt;
      &lt;td&gt;23:58:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1439&lt;/th&gt;
      &lt;td&gt;23:59:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;1440 rows × 2 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We get the minute-by-minute count of steps on that day. Let&amp;rsquo;s take a quick look at a plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;steps.plot()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/fitbit/notebook_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;13-collect-activity-and-intraday-steps-data-since-october-1st&#34;&gt;1.3. Collect activity and intraday steps data since October 1st.&lt;/h3&gt;
&lt;p&gt;We can now build our datasets, which will consists of general activity data and intraday steps data from October 1st to yesterday. We will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Produce a list of dates in &amp;lsquo;YYYY-MM-DD&amp;rsquo; string format for our queries.&lt;/li&gt;
&lt;li&gt;Query the API for each date, extracting our &amp;lsquo;activities&amp;rsquo;, &amp;lsquo;summary&amp;rsquo; and &amp;lsquo;steps&amp;rsquo; datasets from the response.&lt;/li&gt;
&lt;li&gt;Limit our query rate to 150/hour (since this is the Fitbit API rate limit).&lt;/li&gt;
&lt;li&gt;Combine and store the results.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, let&amp;rsquo;s get a list of dates. We can use the pandas &lt;strong&gt;date_range&lt;/strong&gt; method to produce a list of datetime objects, and format them using the &lt;strong&gt;strftime&lt;/strong&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get date range from October 1st to yesterday
start = pd.to_datetime(&amp;quot;2019-10-01&amp;quot;)
date_range = pd.date_range(start=start, end=datetime.today() - timedelta(days=1))
date_range = [datetime.strftime(date, &amp;quot;%Y-%m-%d&amp;quot;) for date in date_range]
date_range[-5:]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;2020-05-14&#39;, &#39;2020-05-15&#39;, &#39;2020-05-16&#39;, &#39;2020-05-17&#39;, &#39;2020-05-18&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we query the API for each date in date_range.&lt;/p&gt;
&lt;p&gt;As seen when we first took a look at the response data, we actually make two API calls per date (i.e. client.activities and client.intraday_time_series). Since the Fitbit API has a rate limit of 150 calls/hour, we should query at most 75 dates an hour. We can accomplish this via the 
&lt;a href=&#34;https://pypi.org/project/ratelimit/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ratelimit&lt;/a&gt; package, which lets you limit the number of times a function is called over a time period.&lt;/p&gt;
&lt;p&gt;Finally, we call the API for each day, timestamp the resulting datasets, and store the total in csv files locally.
We do this for each of the &amp;lsquo;activities&amp;rsquo;, &amp;lsquo;summary&amp;rsquo; and &amp;lsquo;steps&amp;rsquo; datasets. The script below accomplishes this.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# We define a data collection function, and we use the ratelimit package
# to limit our function to 150 API calls / hour.
ONE_HOUR = 3600

@sleep_and_retry
@limits(calls=70, period=ONE_HOUR)
def call_fitbit_api(date):
    &amp;quot;&amp;quot;&amp;quot; Call the Fitbit API for given date in format &#39;YYYY-MM-DD&#39;,
        Return tuple (activities, summary, steps) of dataframes &amp;quot;&amp;quot;&amp;quot;
    
    # Call API twice to get activities and steps responses
    activities_data = client.activities(date=date)
    steps_data = client.intraday_time_series(&#39;activities/steps&#39;, base_date=date, detail_level=&#39;1min&#39;)
        
    # Get activities dataset
    activities = activities_data[&#39;activities&#39;]
    activities = pd.DataFrame(activities)
    
    # Get summary dataset
    summary = activities_data[&#39;summary&#39;]
    del summary[&#39;distances&#39;]
    del summary[&#39;heartRateZones&#39;]
    summary = pd.DataFrame(summary, index=[0])
        
    # Get steps intraday dataset  
    steps = steps_data[&#39;activities-steps-intraday&#39;][&#39;dataset&#39;]
    steps = pd.DataFrame(steps)
    
    # Add a date column
    activities[&#39;date&#39;] = [date for i in activities.index]
    summary[&#39;date&#39;] = [date]
    steps[&#39;date&#39;] = [date for i in steps.index]
    
    return activities, summary, steps


def get_fitbit_data(date_range):
    &amp;quot;&amp;quot;&amp;quot; Collect &#39;activities&#39;, &#39;summary&#39; and &#39;steps&#39; datasets over given dates
        Store as CSV files with format RESOURCE_DATE_to_DATE.csv &amp;quot;&amp;quot;&amp;quot;
    
    daily_df = {
        &#39;activities&#39;: [],
        &#39;summary&#39;: [],
        &#39;steps&#39;: []
    }

    for date in date_range:
        # Call API and get three datasets
        activities, summary, steps = call_fitbit_api(date)
    
        # Append to previous datasets
        daily_df[&#39;activities&#39;].append(activities)
        daily_df[&#39;summary&#39;].append(summary)
        daily_df[&#39;steps&#39;].append(steps)
        
    # Store total dataset as file with format &amp;quot;resource_DATE_to_DATE.csv&amp;quot;
    start, end = date_range[0], date_range[-1]

    for resource in daily_df:
        df = pd.concat(daily_df[resource], ignore_index=True)
        df.to_csv(&amp;quot;./data/raw/{}_{}_to_{}.csv&amp;quot;.format(resource, start, end), index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Collect Fitbit &#39;activities&#39;, &#39;summary&#39; and &#39;steps&#39; data since October 1st, 2019
get_fitbit_data(date_range=date_range)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;2-cleaning-the-data&#34;&gt;2. Cleaning the data&lt;/h2&gt;
&lt;p&gt;It&amp;rsquo;s time to take a look at each dataset.&lt;/p&gt;
&lt;h3 id=&#34;21-the-activity-dataset&#34;&gt;2.1. The activity dataset&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;activities = pd.read_csv(&amp;quot;./data/raw/activities_2019-10-01_to_2020-05-18.csv&amp;quot;)
activities.head(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;activityId&lt;/th&gt;
      &lt;th&gt;activityParentId&lt;/th&gt;
      &lt;th&gt;activityParentName&lt;/th&gt;
      &lt;th&gt;calories&lt;/th&gt;
      &lt;th&gt;description&lt;/th&gt;
      &lt;th&gt;duration&lt;/th&gt;
      &lt;th&gt;hasStartTime&lt;/th&gt;
      &lt;th&gt;isFavorite&lt;/th&gt;
      &lt;th&gt;lastModified&lt;/th&gt;
      &lt;th&gt;logId&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;startDate&lt;/th&gt;
      &lt;th&gt;startTime&lt;/th&gt;
      &lt;th&gt;steps&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;distance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;90013.0&lt;/td&gt;
      &lt;td&gt;90013.0&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;245.0&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;1843000.0&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;2019-10-01T15:36:45.000Z&lt;/td&gt;
      &lt;td&gt;2.568779e+10&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;10:46&lt;/td&gt;
      &lt;td&gt;2059.0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;90013.0&lt;/td&gt;
      &lt;td&gt;90013.0&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;194.0&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;1792000.0&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;2019-10-01T17:44:08.000Z&lt;/td&gt;
      &lt;td&gt;2.569041e+10&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;11:52&lt;/td&gt;
      &lt;td&gt;1977.0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;90013.0&lt;/td&gt;
      &lt;td&gt;90013.0&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;165.0&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;1485000.0&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;2019-10-02T02:00:43.000Z&lt;/td&gt;
      &lt;td&gt;2.570412e+10&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;18:20&lt;/td&gt;
      &lt;td&gt;1443.0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;activities.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(354, 16)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have 16 columns, many of which contain logging information, True/False data or duplicate information which is not useful to us. Let&amp;rsquo;s drop these.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;drop_columns = [&#39;activityId&#39;, &#39;activityParentId&#39;, &#39;activityParentName&#39;, &#39;hasStartTime&#39;, 
                &#39;isFavorite&#39;, &#39;lastModified&#39;, &#39;logId&#39;, &#39;startDate&#39;]

activities.drop(drop_columns, axis=1, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let&amp;rsquo;s look at the distance column. Consulting the documentation, we see that this means logged distance. Since I&amp;rsquo;ve rarely used the feature, it looks like the column consists mostly of missing values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;activities.distance.value_counts()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.310468    1
0.773283    1
Name: distance, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we only have 2 non-missing values in 354 rows, let&amp;rsquo;s drop the column.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;activities.drop(&#39;distance&#39;, axis=1, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some of the column names are in camelCase. Let&amp;rsquo;s rename them to Python&amp;rsquo;s favored snake_case.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;activities.rename(columns={&#39;startTime&#39;: &#39;start_time&#39;}, inplace=True)
activities.head(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;calories&lt;/th&gt;
      &lt;th&gt;description&lt;/th&gt;
      &lt;th&gt;duration&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;start_time&lt;/th&gt;
      &lt;th&gt;steps&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;245.0&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;1843000.0&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;10:46&lt;/td&gt;
      &lt;td&gt;2059.0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;194.0&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;1792000.0&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;11:52&lt;/td&gt;
      &lt;td&gt;1977.0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;165.0&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;1485000.0&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;18:20&lt;/td&gt;
      &lt;td&gt;1443.0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The duration column isn&amp;rsquo;t easy to parse and is missing units. The Fitbit api 
&lt;a href=&#34;https://dev.fitbit.com/build/reference/web-api/activity/#activity-logging&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; lists the duration as being in millisecond, so let&amp;rsquo;s put it in minutes and rename accordingly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;activities.duration = activities.duration.apply(lambda x: round(x/60000))
activities.rename(columns={&#39;duration&#39;: &#39;duration_min&#39;}, inplace=True)

activities.head(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;calories&lt;/th&gt;
      &lt;th&gt;description&lt;/th&gt;
      &lt;th&gt;duration_min&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;start_time&lt;/th&gt;
      &lt;th&gt;steps&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;245.0&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;10:46&lt;/td&gt;
      &lt;td&gt;2059.0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;194.0&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;11:52&lt;/td&gt;
      &lt;td&gt;1977.0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;165.0&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;18:20&lt;/td&gt;
      &lt;td&gt;1443.0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;To help with analysis, let&amp;rsquo;s format the start_time column as &amp;ldquo;YYYY-MM-DD H:M:S&amp;rdquo; to more easily convert to a datetime object. Since we have the activity duration, we can also add an end_time column.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Format start_time column and convert to datetime object
activities.start_time = activities.date + &amp;quot; &amp;quot; + activities.start_time + &amp;quot;:00&amp;quot;
activities.start_time = pd.to_datetime(activities.start_time)

# Create end_time column by adding the duration_min column to start_time
activities_duration = activities.duration_min.apply(lambda x: timedelta(minutes=x))
activities[&#39;end_time&#39;] = activities.start_time + activities_duration

# Display result
activities.head(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;calories&lt;/th&gt;
      &lt;th&gt;description&lt;/th&gt;
      &lt;th&gt;duration_min&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;start_time&lt;/th&gt;
      &lt;th&gt;steps&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;end_time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;245.0&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;2019-10-01 10:46:00&lt;/td&gt;
      &lt;td&gt;2059.0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;2019-10-01 11:17:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;194.0&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;2019-10-01 11:52:00&lt;/td&gt;
      &lt;td&gt;1977.0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;2019-10-01 12:22:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;165.0&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;2019-10-01 18:20:00&lt;/td&gt;
      &lt;td&gt;1443.0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;2019-10-01 18:45:00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Finally, let&amp;rsquo;s reorder the columns for readability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Reorder columns
column_order = [&#39;date&#39;, &#39;name&#39;, &#39;description&#39;, &#39;start_time&#39;, &#39;end_time&#39;, &#39;duration_min&#39;, &#39;steps&#39;, &#39;calories&#39;]
activities = activities[column_order]

# Store dataset
start, end = date_range[0], date_range[-1]
activities.to_csv(&amp;quot;./data/tidy/activities_{}_to_{}.csv&amp;quot;.format(start, end), index=False)

# Look at end result
activities
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;description&lt;/th&gt;
      &lt;th&gt;start_time&lt;/th&gt;
      &lt;th&gt;end_time&lt;/th&gt;
      &lt;th&gt;duration_min&lt;/th&gt;
      &lt;th&gt;steps&lt;/th&gt;
      &lt;th&gt;calories&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;2019-10-01 10:46:00&lt;/td&gt;
      &lt;td&gt;2019-10-01 11:17:00&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;2059.0&lt;/td&gt;
      &lt;td&gt;245.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;2019-10-01 11:52:00&lt;/td&gt;
      &lt;td&gt;2019-10-01 12:22:00&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;1977.0&lt;/td&gt;
      &lt;td&gt;194.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;2019-10-01 18:20:00&lt;/td&gt;
      &lt;td&gt;2019-10-01 18:45:00&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;1443.0&lt;/td&gt;
      &lt;td&gt;165.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;2019-10-01 19:38:00&lt;/td&gt;
      &lt;td&gt;2019-10-01 20:05:00&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;1624.0&lt;/td&gt;
      &lt;td&gt;176.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2019-10-02&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;2019-10-02 13:38:00&lt;/td&gt;
      &lt;td&gt;2019-10-02 15:20:00&lt;/td&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td&gt;7035.0&lt;/td&gt;
      &lt;td&gt;552.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;349&lt;/th&gt;
      &lt;td&gt;2020-05-14&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;2020-05-14 08:22:00&lt;/td&gt;
      &lt;td&gt;2020-05-14 09:07:00&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;4394.0&lt;/td&gt;
      &lt;td&gt;361.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;350&lt;/th&gt;
      &lt;td&gt;2020-05-15&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;2020-05-15 12:23:00&lt;/td&gt;
      &lt;td&gt;2020-05-15 13:59:00&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;9430.0&lt;/td&gt;
      &lt;td&gt;658.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;351&lt;/th&gt;
      &lt;td&gt;2020-05-15&lt;/td&gt;
      &lt;td&gt;Run&lt;/td&gt;
      &lt;td&gt;Running - 5 mph (12 min/mile)&lt;/td&gt;
      &lt;td&gt;2020-05-15 20:13:00&lt;/td&gt;
      &lt;td&gt;2020-05-15 20:34:00&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;2865.0&lt;/td&gt;
      &lt;td&gt;245.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;352&lt;/th&gt;
      &lt;td&gt;2020-05-16&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;2020-05-16 10:29:00&lt;/td&gt;
      &lt;td&gt;2020-05-16 12:33:00&lt;/td&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;10487.0&lt;/td&gt;
      &lt;td&gt;855.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;353&lt;/th&gt;
      &lt;td&gt;2020-05-17&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;2020-05-17 11:11:00&lt;/td&gt;
      &lt;td&gt;2020-05-17 13:15:00&lt;/td&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;10757.0&lt;/td&gt;
      &lt;td&gt;835.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;354 rows × 8 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;22-the-summary-dataset&#34;&gt;2.2. The summary dataset&lt;/h3&gt;
&lt;p&gt;Now let&amp;rsquo;s take a look at the second dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;summary = pd.read_csv(&amp;quot;./data/raw/summary_2019-10-01_to_2020-05-18.csv&amp;quot;)
summary
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;activeScore&lt;/th&gt;
      &lt;th&gt;activityCalories&lt;/th&gt;
      &lt;th&gt;caloriesBMR&lt;/th&gt;
      &lt;th&gt;caloriesOut&lt;/th&gt;
      &lt;th&gt;fairlyActiveMinutes&lt;/th&gt;
      &lt;th&gt;lightlyActiveMinutes&lt;/th&gt;
      &lt;th&gt;marginalCalories&lt;/th&gt;
      &lt;th&gt;restingHeartRate&lt;/th&gt;
      &lt;th&gt;sedentaryMinutes&lt;/th&gt;
      &lt;th&gt;steps&lt;/th&gt;
      &lt;th&gt;veryActiveMinutes&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;4097&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;5018&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;420&lt;/td&gt;
      &lt;td&gt;2670&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;209&lt;/td&gt;
      &lt;td&gt;25576&lt;/td&gt;
      &lt;td&gt;152&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;1967&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;3211&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
      &lt;td&gt;1145&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;379&lt;/td&gt;
      &lt;td&gt;16471&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;2019-10-02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;1540&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;2923&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;251&lt;/td&gt;
      &lt;td&gt;920&lt;/td&gt;
      &lt;td&gt;57&lt;/td&gt;
      &lt;td&gt;674&lt;/td&gt;
      &lt;td&gt;13510&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;2019-10-03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;1470&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;2883&lt;/td&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;163&lt;/td&gt;
      &lt;td&gt;940&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
      &lt;td&gt;659&lt;/td&gt;
      &lt;td&gt;11443&lt;/td&gt;
      &lt;td&gt;65&lt;/td&gt;
      &lt;td&gt;2019-10-04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;1776&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;3136&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;178&lt;/td&gt;
      &lt;td&gt;1110&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
      &lt;td&gt;572&lt;/td&gt;
      &lt;td&gt;18711&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;2019-10-05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;226&lt;/th&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;1179&lt;/td&gt;
      &lt;td&gt;1659&lt;/td&gt;
      &lt;td&gt;2614&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;218&lt;/td&gt;
      &lt;td&gt;680&lt;/td&gt;
      &lt;td&gt;57&lt;/td&gt;
      &lt;td&gt;636&lt;/td&gt;
      &lt;td&gt;8938&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;2020-05-14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;227&lt;/th&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;1589&lt;/td&gt;
      &lt;td&gt;1659&lt;/td&gt;
      &lt;td&gt;2988&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;183&lt;/td&gt;
      &lt;td&gt;1011&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
      &lt;td&gt;565&lt;/td&gt;
      &lt;td&gt;15358&lt;/td&gt;
      &lt;td&gt;99&lt;/td&gt;
      &lt;td&gt;2020-05-15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;228&lt;/th&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;1512&lt;/td&gt;
      &lt;td&gt;1659&lt;/td&gt;
      &lt;td&gt;2903&lt;/td&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;959&lt;/td&gt;
      &lt;td&gt;55&lt;/td&gt;
      &lt;td&gt;614&lt;/td&gt;
      &lt;td&gt;15115&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;2020-05-16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;229&lt;/th&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;1922&lt;/td&gt;
      &lt;td&gt;1659&lt;/td&gt;
      &lt;td&gt;3230&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;224&lt;/td&gt;
      &lt;td&gt;1215&lt;/td&gt;
      &lt;td&gt;55&lt;/td&gt;
      &lt;td&gt;399&lt;/td&gt;
      &lt;td&gt;18880&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;2020-05-17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;230&lt;/th&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;468&lt;/td&gt;
      &lt;td&gt;1659&lt;/td&gt;
      &lt;td&gt;2046&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;116&lt;/td&gt;
      &lt;td&gt;235&lt;/td&gt;
      &lt;td&gt;54&lt;/td&gt;
      &lt;td&gt;713&lt;/td&gt;
      &lt;td&gt;2341&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2020-05-18&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;231 rows × 12 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now, looking around the documentation the activeScore column seems to be an old feature. All values are -1 in our dataset so there&amp;rsquo;s not much loss of information in dropping the column.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;(summary.activeScore == -1).all()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;summary.drop(&#39;activeScore&#39;, axis=1, inplace=True)
summary.head(2)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;activityCalories&lt;/th&gt;
      &lt;th&gt;caloriesBMR&lt;/th&gt;
      &lt;th&gt;caloriesOut&lt;/th&gt;
      &lt;th&gt;fairlyActiveMinutes&lt;/th&gt;
      &lt;th&gt;lightlyActiveMinutes&lt;/th&gt;
      &lt;th&gt;marginalCalories&lt;/th&gt;
      &lt;th&gt;restingHeartRate&lt;/th&gt;
      &lt;th&gt;sedentaryMinutes&lt;/th&gt;
      &lt;th&gt;steps&lt;/th&gt;
      &lt;th&gt;veryActiveMinutes&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;4097&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;5018&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;420&lt;/td&gt;
      &lt;td&gt;2670&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;209&lt;/td&gt;
      &lt;td&gt;25576&lt;/td&gt;
      &lt;td&gt;152&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1967&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;3211&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
      &lt;td&gt;1145&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
      &lt;td&gt;379&lt;/td&gt;
      &lt;td&gt;16471&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;2019-10-02&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Next, we again format all columns to snake_case and reorder for readability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Rename columns to snake_case
columns_map = {
    &#39;activityCalories&#39;: &#39;activity_calories&#39;,
    &#39;caloriesBMR&#39;: &#39;calories_BMR&#39;,
    &#39;caloriesOut&#39;: &#39;calories_out&#39;,
    &#39;fairlyActiveMinutes&#39;: &#39;fairly_active_minutes&#39;,
    &#39;lightlyActiveMinutes&#39;: &#39;lightly_active_minutes&#39;,
    &#39;marginalCalories&#39;: &#39;marginal_calories&#39;,
    &#39;restingHeartRate&#39;: &#39;resting_heart_rate&#39;,
    &#39;sedentaryMinutes&#39;: &#39;sedentary_minutes&#39;,
    &#39;veryActiveMinutes&#39;: &#39;very_active_minutes&#39;
}

summary.rename(columns=columns_map, inplace=True)

# Reorder columns
column_order = [&#39;date&#39;, &#39;steps&#39;, &#39;very_active_minutes&#39;, &#39;fairly_active_minutes&#39;, &#39;lightly_active_minutes&#39;, 
                &#39;sedentary_minutes&#39;, &#39;activity_calories&#39;, &#39;marginal_calories&#39;, &#39;calories_out&#39;, &#39;calories_BMR&#39;,
                &#39;resting_heart_rate&#39;]

summary = summary[column_order]

# Store dataset
start, end = summary.date[0], summary.date[len(summary.index)-1]
summary.to_csv(&amp;quot;./data/tidy/summary_{}_to_{}.csv&amp;quot;.format(start, end), index=False)

# Look at result
summary.head(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;steps&lt;/th&gt;
      &lt;th&gt;very_active_minutes&lt;/th&gt;
      &lt;th&gt;fairly_active_minutes&lt;/th&gt;
      &lt;th&gt;lightly_active_minutes&lt;/th&gt;
      &lt;th&gt;sedentary_minutes&lt;/th&gt;
      &lt;th&gt;activity_calories&lt;/th&gt;
      &lt;th&gt;marginal_calories&lt;/th&gt;
      &lt;th&gt;calories_out&lt;/th&gt;
      &lt;th&gt;calories_BMR&lt;/th&gt;
      &lt;th&gt;resting_heart_rate&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;25576&lt;/td&gt;
      &lt;td&gt;152&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;420&lt;/td&gt;
      &lt;td&gt;209&lt;/td&gt;
      &lt;td&gt;4097&lt;/td&gt;
      &lt;td&gt;2670&lt;/td&gt;
      &lt;td&gt;5018&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2019-10-02&lt;/td&gt;
      &lt;td&gt;16471&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
      &lt;td&gt;379&lt;/td&gt;
      &lt;td&gt;1967&lt;/td&gt;
      &lt;td&gt;1145&lt;/td&gt;
      &lt;td&gt;3211&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2019-10-03&lt;/td&gt;
      &lt;td&gt;13510&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;251&lt;/td&gt;
      &lt;td&gt;674&lt;/td&gt;
      &lt;td&gt;1540&lt;/td&gt;
      &lt;td&gt;920&lt;/td&gt;
      &lt;td&gt;2923&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;57&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;23-the-steps-dataset&#34;&gt;2.3. The steps dataset&lt;/h3&gt;
&lt;p&gt;Finally, we look at the intraday steps dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;steps = pd.read_csv(&amp;quot;./data/raw/steps_2019-10-01_to_2020-05-18.csv&amp;quot;)
steps
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
      &lt;th&gt;value&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;00:00:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;00:01:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;00:02:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;00:03:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;00:04:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332635&lt;/th&gt;
      &lt;td&gt;23:55:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2020-05-18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332636&lt;/th&gt;
      &lt;td&gt;23:56:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2020-05-18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332637&lt;/th&gt;
      &lt;td&gt;23:57:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2020-05-18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332638&lt;/th&gt;
      &lt;td&gt;23:58:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2020-05-18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332639&lt;/th&gt;
      &lt;td&gt;23:59:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2020-05-18&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;332640 rows × 3 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We can combine the time and date into a single column, in datetime format. We also rename value to the more descriptive &amp;lsquo;stepcount&amp;rsquo;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Combine date and time
steps.time = steps.date + &amp;quot; &amp;quot; + steps.time

# Rename value to stepcount
steps.rename(columns={&#39;value&#39;: &#39;stepcount&#39;}, inplace=True)

# Get endpoint dates to store the file
start, end = steps.date[0], steps.date[len(steps.index) - 1]

# Drop date column and store
steps.drop(&#39;date&#39;, axis=1, inplace=True)
steps.to_csv(&amp;quot;./data/tidy/steps_{}_to_{}.csv&amp;quot;.format(start, end), index=False)

# Look at end result
steps
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
      &lt;th&gt;stepcount&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:00:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:01:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:02:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:03:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:04:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332635&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:55:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332636&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:56:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332637&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:57:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332638&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:58:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332639&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:59:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;332640 rows × 2 columns&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;3-visualisations&#34;&gt;3. Visualisations&lt;/h2&gt;
&lt;p&gt;We now have clean datasets to explore further. We take a look at visualisations in 
&lt;a href=&#34;https://vgelinas.github.io/post/fitbit-data-exploration-part-ii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part II&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fitbit Data Exploration Part II</title>
      <link>https://vgelinas.github.io/post/fitbit-data-exploration-part-ii/</link>
      <pubDate>Mon, 18 May 2020 00:00:00 +0000</pubDate>
      <guid>https://vgelinas.github.io/post/fitbit-data-exploration-part-ii/</guid>
      <description>&lt;p&gt;In 
&lt;a href=&#34;https://vgelinas.github.io/post/fitbit-data-exploration-part-i/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Part I&lt;/a&gt; we showed how to connect to the Fitbit API via Python, and we built some datasets consisting of activities and intraday steps data from October to today. In this post we walk through some data visualisations, and will take a look in particular at the intraday steps data.&lt;/p&gt;
&lt;h3 id=&#34;31-activity-statistics-per-week-day&#34;&gt;3.1. Activity statistics per week day&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s compile some statistics based on day of the week. First, let&amp;rsquo;s take a look at summary data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use parse_dates to interpret our date column as datetime objects
summary = pd.read_csv(&amp;quot;./data/tidy/summary_2019-10-01_to_2020-05-18.csv&amp;quot;, parse_dates=[&#39;date&#39;])
summary
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;steps&lt;/th&gt;
      &lt;th&gt;very_active_minutes&lt;/th&gt;
      &lt;th&gt;fairly_active_minutes&lt;/th&gt;
      &lt;th&gt;lightly_active_minutes&lt;/th&gt;
      &lt;th&gt;sedentary_minutes&lt;/th&gt;
      &lt;th&gt;activity_calories&lt;/th&gt;
      &lt;th&gt;marginal_calories&lt;/th&gt;
      &lt;th&gt;calories_out&lt;/th&gt;
      &lt;th&gt;calories_BMR&lt;/th&gt;
      &lt;th&gt;resting_heart_rate&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;25576&lt;/td&gt;
      &lt;td&gt;152&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;420&lt;/td&gt;
      &lt;td&gt;209&lt;/td&gt;
      &lt;td&gt;4097&lt;/td&gt;
      &lt;td&gt;2670&lt;/td&gt;
      &lt;td&gt;5018&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2019-10-02&lt;/td&gt;
      &lt;td&gt;16471&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
      &lt;td&gt;379&lt;/td&gt;
      &lt;td&gt;1967&lt;/td&gt;
      &lt;td&gt;1145&lt;/td&gt;
      &lt;td&gt;3211&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2019-10-03&lt;/td&gt;
      &lt;td&gt;13510&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;251&lt;/td&gt;
      &lt;td&gt;674&lt;/td&gt;
      &lt;td&gt;1540&lt;/td&gt;
      &lt;td&gt;920&lt;/td&gt;
      &lt;td&gt;2923&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2019-10-04&lt;/td&gt;
      &lt;td&gt;11443&lt;/td&gt;
      &lt;td&gt;65&lt;/td&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;163&lt;/td&gt;
      &lt;td&gt;659&lt;/td&gt;
      &lt;td&gt;1470&lt;/td&gt;
      &lt;td&gt;940&lt;/td&gt;
      &lt;td&gt;2883&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2019-10-05&lt;/td&gt;
      &lt;td&gt;18711&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;178&lt;/td&gt;
      &lt;td&gt;572&lt;/td&gt;
      &lt;td&gt;1776&lt;/td&gt;
      &lt;td&gt;1110&lt;/td&gt;
      &lt;td&gt;3136&lt;/td&gt;
      &lt;td&gt;1663&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;226&lt;/th&gt;
      &lt;td&gt;2020-05-14&lt;/td&gt;
      &lt;td&gt;8938&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;218&lt;/td&gt;
      &lt;td&gt;636&lt;/td&gt;
      &lt;td&gt;1179&lt;/td&gt;
      &lt;td&gt;680&lt;/td&gt;
      &lt;td&gt;2614&lt;/td&gt;
      &lt;td&gt;1659&lt;/td&gt;
      &lt;td&gt;57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;227&lt;/th&gt;
      &lt;td&gt;2020-05-15&lt;/td&gt;
      &lt;td&gt;15358&lt;/td&gt;
      &lt;td&gt;99&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;183&lt;/td&gt;
      &lt;td&gt;565&lt;/td&gt;
      &lt;td&gt;1589&lt;/td&gt;
      &lt;td&gt;1011&lt;/td&gt;
      &lt;td&gt;2988&lt;/td&gt;
      &lt;td&gt;1659&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;228&lt;/th&gt;
      &lt;td&gt;2020-05-16&lt;/td&gt;
      &lt;td&gt;15115&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;614&lt;/td&gt;
      &lt;td&gt;1512&lt;/td&gt;
      &lt;td&gt;959&lt;/td&gt;
      &lt;td&gt;2903&lt;/td&gt;
      &lt;td&gt;1659&lt;/td&gt;
      &lt;td&gt;55&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;229&lt;/th&gt;
      &lt;td&gt;2020-05-17&lt;/td&gt;
      &lt;td&gt;18880&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;224&lt;/td&gt;
      &lt;td&gt;399&lt;/td&gt;
      &lt;td&gt;1922&lt;/td&gt;
      &lt;td&gt;1215&lt;/td&gt;
      &lt;td&gt;3230&lt;/td&gt;
      &lt;td&gt;1659&lt;/td&gt;
      &lt;td&gt;55&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;230&lt;/th&gt;
      &lt;td&gt;2020-05-18&lt;/td&gt;
      &lt;td&gt;2341&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;116&lt;/td&gt;
      &lt;td&gt;713&lt;/td&gt;
      &lt;td&gt;468&lt;/td&gt;
      &lt;td&gt;235&lt;/td&gt;
      &lt;td&gt;2046&lt;/td&gt;
      &lt;td&gt;1659&lt;/td&gt;
      &lt;td&gt;54&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;231 rows × 11 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We can use strftime to convert the date to a week day, and get group statistics per day of the week.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add a weekday column
summary[&#39;weekday&#39;] = summary.date.apply(lambda x: datetime.strftime(x, &amp;quot;%A&amp;quot;))

# Get statistics per day of the week
weekly_statistics = summary.groupby(&#39;weekday&#39;).describe()

# Row indices are days of the week, put them in order
row_order = [&#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Friday&#39;, &#39;Saturday&#39;, &#39;Sunday&#39;]
weekly_statistics = weekly_statistics.loc[row_order, :]

# Show results
weekly_statistics
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead tr th {
    text-align: left;
}

.dataframe thead tr:last-of-type th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th colspan=&#34;8&#34; halign=&#34;left&#34;&gt;steps&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;very_active_minutes&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th colspan=&#34;2&#34; halign=&#34;left&#34;&gt;calories_BMR&lt;/th&gt;
      &lt;th colspan=&#34;8&#34; halign=&#34;left&#34;&gt;resting_heart_rate&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;th&gt;max&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;weekday&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Monday&lt;/th&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;10817.393939&lt;/td&gt;
      &lt;td&gt;4075.727205&lt;/td&gt;
      &lt;td&gt;2341.0&lt;/td&gt;
      &lt;td&gt;7793.0&lt;/td&gt;
      &lt;td&gt;11663.0&lt;/td&gt;
      &lt;td&gt;13248.0&lt;/td&gt;
      &lt;td&gt;17668.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;50.848485&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;55.030303&lt;/td&gt;
      &lt;td&gt;1.610218&lt;/td&gt;
      &lt;td&gt;53.0&lt;/td&gt;
      &lt;td&gt;54.0&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;56.0&lt;/td&gt;
      &lt;td&gt;59.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Tuesday&lt;/th&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;11202.878788&lt;/td&gt;
      &lt;td&gt;4575.416680&lt;/td&gt;
      &lt;td&gt;3250.0&lt;/td&gt;
      &lt;td&gt;8211.0&lt;/td&gt;
      &lt;td&gt;10953.0&lt;/td&gt;
      &lt;td&gt;12994.0&lt;/td&gt;
      &lt;td&gt;25576.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;57.666667&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;55.030303&lt;/td&gt;
      &lt;td&gt;1.878910&lt;/td&gt;
      &lt;td&gt;52.0&lt;/td&gt;
      &lt;td&gt;53.0&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;56.0&lt;/td&gt;
      &lt;td&gt;59.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Wednesday&lt;/th&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;11007.939394&lt;/td&gt;
      &lt;td&gt;3307.219952&lt;/td&gt;
      &lt;td&gt;4486.0&lt;/td&gt;
      &lt;td&gt;9083.0&lt;/td&gt;
      &lt;td&gt;10795.0&lt;/td&gt;
      &lt;td&gt;13538.0&lt;/td&gt;
      &lt;td&gt;18402.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;53.909091&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;55.303030&lt;/td&gt;
      &lt;td&gt;1.704495&lt;/td&gt;
      &lt;td&gt;52.0&lt;/td&gt;
      &lt;td&gt;54.0&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;56.0&lt;/td&gt;
      &lt;td&gt;58.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Thursday&lt;/th&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;10867.212121&lt;/td&gt;
      &lt;td&gt;2882.811852&lt;/td&gt;
      &lt;td&gt;4525.0&lt;/td&gt;
      &lt;td&gt;8938.0&lt;/td&gt;
      &lt;td&gt;10856.0&lt;/td&gt;
      &lt;td&gt;13064.0&lt;/td&gt;
      &lt;td&gt;16621.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;54.212121&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;55.424242&lt;/td&gt;
      &lt;td&gt;1.581738&lt;/td&gt;
      &lt;td&gt;52.0&lt;/td&gt;
      &lt;td&gt;54.0&lt;/td&gt;
      &lt;td&gt;56.0&lt;/td&gt;
      &lt;td&gt;56.0&lt;/td&gt;
      &lt;td&gt;59.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Friday&lt;/th&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;12526.757576&lt;/td&gt;
      &lt;td&gt;4179.309722&lt;/td&gt;
      &lt;td&gt;2028.0&lt;/td&gt;
      &lt;td&gt;9887.0&lt;/td&gt;
      &lt;td&gt;12445.0&lt;/td&gt;
      &lt;td&gt;15358.0&lt;/td&gt;
      &lt;td&gt;18958.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;66.454545&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;55.424242&lt;/td&gt;
      &lt;td&gt;1.581738&lt;/td&gt;
      &lt;td&gt;52.0&lt;/td&gt;
      &lt;td&gt;54.0&lt;/td&gt;
      &lt;td&gt;56.0&lt;/td&gt;
      &lt;td&gt;56.0&lt;/td&gt;
      &lt;td&gt;59.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Saturday&lt;/th&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;14507.181818&lt;/td&gt;
      &lt;td&gt;4188.612206&lt;/td&gt;
      &lt;td&gt;6315.0&lt;/td&gt;
      &lt;td&gt;12446.0&lt;/td&gt;
      &lt;td&gt;15049.0&lt;/td&gt;
      &lt;td&gt;16924.0&lt;/td&gt;
      &lt;td&gt;23152.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;74.151515&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;55.575758&lt;/td&gt;
      &lt;td&gt;1.581738&lt;/td&gt;
      &lt;td&gt;53.0&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;56.0&lt;/td&gt;
      &lt;td&gt;60.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Sunday&lt;/th&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;12488.121212&lt;/td&gt;
      &lt;td&gt;4849.136191&lt;/td&gt;
      &lt;td&gt;4560.0&lt;/td&gt;
      &lt;td&gt;9659.0&lt;/td&gt;
      &lt;td&gt;11756.0&lt;/td&gt;
      &lt;td&gt;15944.0&lt;/td&gt;
      &lt;td&gt;25193.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;64.636364&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;1663.0&lt;/td&gt;
      &lt;td&gt;33.0&lt;/td&gt;
      &lt;td&gt;55.393939&lt;/td&gt;
      &lt;td&gt;1.456438&lt;/td&gt;
      &lt;td&gt;53.0&lt;/td&gt;
      &lt;td&gt;54.0&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
      &lt;td&gt;56.0&lt;/td&gt;
      &lt;td&gt;59.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;7 rows × 80 columns&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot the mean and first quartile for number of steps per weekday
mean_steps = weekly_statistics.steps[[&#39;mean&#39;, &#39;25%&#39;]]
mean_steps.plot(kind=&#39;bar&#39;)

plt.title(&#39;Weekly stepcount since October 1st, 2019&#39;)
plt.ylabel(&#39;steps&#39;)
plt.ylim([0, 18000])
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/fitbit/notebook_61_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;32-visualising-walks-over-the-day&#34;&gt;3.2. Visualising walks over the day&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now look at the steps intraday data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load in dataset
steps = pd.read_csv(&amp;quot;./data/tidy/steps_2019-10-01_to_2020-05-18.csv&amp;quot;, parse_dates=[&#39;time&#39;])
steps
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
      &lt;th&gt;stepcount&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:00:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:01:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:02:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:03:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:04:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332635&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:55:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332636&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:56:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332637&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:57:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332638&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:58:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332639&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:59:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;332640 rows × 2 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s visualise steps intraday data over a given day. We look at May 1st again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;date = &#39;2020-05-01&#39;

# Restrict to logs for given date
day_df = steps[steps.time.apply(lambda x: datetime.strftime(x, &amp;quot;%Y-%m-%d&amp;quot;)) == date].copy()

# Restrict to within waking hours
start_of_day = pd.to_datetime(&#39;2020-05-01 07:00:00&#39;)
end_of_day = pd.to_datetime(&#39;2020-05-01 23:00:00&#39;)
day_df = day_df[(day_df.time &amp;gt;= start_of_day)&amp;amp;(day_df.time &amp;lt;= end_of_day)]

# Convert time back to hr:min:sec format and set as index
day_df.time = day_df.time.apply(lambda x: datetime.strftime(x, &amp;quot;%H:%M:%S&amp;quot;))
day_df.set_index(&#39;time&#39;, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s plot steps during the day on May 1st.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot steps on May 1st
fig, ax = plt.subplots()

day_df.rolling(15).mean().plot(ax=ax)  # 15 min rolling avg to smooth out noise
ax.set_title(&#39;Steps on May 1st, 2020&#39;)
ax.set_xlabel(&#39;Time of Day&#39;)
ax.set_ylabel(&#39;Steps per min&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/fitbit/notebook_67_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here we can tell which period corresponds to exercise, and which results from general activity, but let&amp;rsquo;s be more systematic about this. We can isolate the steps that result from walks alone and not from general activity. The activity dataset has a start_time and end_time for each activity (walk, run, &amp;hellip;) and we may use these to filter our dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load activities dataset, parsing start_time and end_time columns as datetime objects
time_col = [&#39;start_time&#39;, &#39;end_time&#39;]
activities = pd.read_csv(&amp;quot;./data/tidy/activities_2019-10-01_to_2020-05-18.csv&amp;quot;, parse_dates=time_col)
activities.head(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;description&lt;/th&gt;
      &lt;th&gt;start_time&lt;/th&gt;
      &lt;th&gt;end_time&lt;/th&gt;
      &lt;th&gt;duration_min&lt;/th&gt;
      &lt;th&gt;steps&lt;/th&gt;
      &lt;th&gt;calories&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;2019-10-01 10:46:00&lt;/td&gt;
      &lt;td&gt;2019-10-01 11:17:00&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
      &lt;td&gt;2059.0&lt;/td&gt;
      &lt;td&gt;245.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;2019-10-01 11:52:00&lt;/td&gt;
      &lt;td&gt;2019-10-01 12:22:00&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;1977.0&lt;/td&gt;
      &lt;td&gt;194.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2019-10-01&lt;/td&gt;
      &lt;td&gt;Walk&lt;/td&gt;
      &lt;td&gt;Walking less than 2 mph, strolling very slowly&lt;/td&gt;
      &lt;td&gt;2019-10-01 18:20:00&lt;/td&gt;
      &lt;td&gt;2019-10-01 18:45:00&lt;/td&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;1443.0&lt;/td&gt;
      &lt;td&gt;165.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s add a column named &amp;lsquo;on_walk&amp;rsquo; to the steps dataset, with a True/False value. For this we cook up a helper function as below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Helper function to filter the intraday steps data by activity type
def is_during_activity(t, activity):
    &amp;quot;&amp;quot;&amp;quot; Takes a datetime object t and activity name
        Returns True if during activity, else False &amp;quot;&amp;quot;&amp;quot;
    # Get the activities dataset for that day
    date = datetime.strftime(t, &amp;quot;%Y-%m-%d&amp;quot;)
    df = activities[activities.date == date]
    
    # Subset to rows which represent activity
    df = df[df.name == activity]
    
    # Check if t is within the bounds of the activity
    for i in df.index:
        if df.loc[i, &#39;start_time&#39;] &amp;lt;= t &amp;lt;= df.loc[i, &#39;end_time&#39;]:
            return True
    
    return False


# Add &#39;on_walk&#39; column to steps dataframe
steps[&#39;on_walk&#39;] = steps.time.apply(is_during_activity, args=(&#39;Walk&#39;,))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at the stepcount during walks.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;steps[steps.on_walk == True]
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
      &lt;th&gt;stepcount&lt;/th&gt;
      &lt;th&gt;on_walk&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;646&lt;/th&gt;
      &lt;td&gt;2019-10-01 10:46:00&lt;/td&gt;
      &lt;td&gt;54&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;647&lt;/th&gt;
      &lt;td&gt;2019-10-01 10:47:00&lt;/td&gt;
      &lt;td&gt;76&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;648&lt;/th&gt;
      &lt;td&gt;2019-10-01 10:48:00&lt;/td&gt;
      &lt;td&gt;97&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;649&lt;/th&gt;
      &lt;td&gt;2019-10-01 10:49:00&lt;/td&gt;
      &lt;td&gt;75&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;650&lt;/th&gt;
      &lt;td&gt;2019-10-01 10:50:00&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;330551&lt;/th&gt;
      &lt;td&gt;2020-05-17 13:11:00&lt;/td&gt;
      &lt;td&gt;113&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;330552&lt;/th&gt;
      &lt;td&gt;2020-05-17 13:12:00&lt;/td&gt;
      &lt;td&gt;92&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;330553&lt;/th&gt;
      &lt;td&gt;2020-05-17 13:13:00&lt;/td&gt;
      &lt;td&gt;77&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;330554&lt;/th&gt;
      &lt;td&gt;2020-05-17 13:14:00&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;330555&lt;/th&gt;
      &lt;td&gt;2020-05-17 13:15:00&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;15007 rows × 3 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Using this, we can create a new dataframe consisting of walks stepcount data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set all steps outside of walks to zero
walks = steps.copy()
walks.stepcount = walks.stepcount.where(walks.on_walk == True, 0)
    
# Drop &#39;on_walk&#39; column
walks.drop(&#39;on_walk&#39;, axis=1, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s look at May 1st again.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;date = &#39;2020-05-01&#39;

# Restrict to logs for given date
day_walks = walks[walks.time.apply(lambda x: datetime.strftime(x, &amp;quot;%Y-%m-%d&amp;quot;)) == date].copy()

# Restrict to within waking hours
start_of_day = pd.to_datetime(&#39;2020-05-01 07:00:00&#39;)
end_of_day = pd.to_datetime(&#39;2020-05-01 23:00:00&#39;)
day_walks = day_walks[(day_walks.time &amp;gt;= start_of_day)&amp;amp;(day_walks.time &amp;lt;= end_of_day)]

# Convert time back to hr:min:sec format and set as index
day_walks.time = day_walks.time.apply(lambda x: datetime.strftime(x, &amp;quot;%H:%M:%S&amp;quot;))
day_walks.set_index(&#39;time&#39;, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot walks on May 1st
fig, ax = plt.subplots()

day_walks.rolling(15).mean().plot(ax=ax)  # 15 min rolling avg to smooth out noise
ax.set_title(&#39;Steps on May 1st 2020 during a walk&#39;)
ax.set_xlabel(&#39;Time of Day&#39;)
ax.set_ylabel(&#39;Steps per min&#39;)

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/fitbit/notebook_78_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;visualise-walk-times-for-each-day-of-the-week&#34;&gt;Visualise walk times for each day of the week.&lt;/h4&gt;
&lt;p&gt;We can build a picture of the &amp;lsquo;average&amp;rsquo; day over the last 5 months, broken down by day of the week.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Add a weekday column to walks dataset for grouping
walks[&#39;weekday&#39;] = walks.time.apply(lambda x: datetime.strftime(x, &amp;quot;%A&amp;quot;))
walks
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
      &lt;th&gt;stepcount&lt;/th&gt;
      &lt;th&gt;weekday&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:00:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Tuesday&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:01:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Tuesday&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:02:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Tuesday&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:03:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Tuesday&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2019-10-01 00:04:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Tuesday&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332635&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:55:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Monday&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332636&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:56:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Monday&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332637&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:57:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Monday&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332638&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:58:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Monday&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;332639&lt;/th&gt;
      &lt;td&gt;2020-05-18 23:59:00&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Monday&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;332640 rows × 3 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To build our daily picture, let&amp;rsquo;s first group the dataset by day of the week, then average the stepcount for each given minute. This should give us a sense of the distribution of walks on each day.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# change date column to hour:min strings for grouping
walks.time = walks.time.apply(lambda x: datetime.strftime(x, &amp;quot;%H:%M&amp;quot;))

# for each day of the week, average step count over all dates
walks_weekday = walks.groupby(&#39;weekday&#39;) 

weekdays = {}
for day_name, df in walks_weekday:
    # group by minute, then average over dates
    df = df.groupby(&#39;time&#39;).mean()
    weekdays[day_name] = df
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also get rid of the timestamps during the night, since I&amp;rsquo;m not up for midnight walks too often.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Restrict to waking hours, say 7:00am to 23:59pm
for day in weekdays:
    weekdays[day] = weekdays[day].iloc[420:]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let&amp;rsquo;s look at the distribution of walks on Mondays.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;weekdays[&#39;Monday&#39;].rolling(15).mean().plot()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/fitbit/notebook_86_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally, we do this for each day of the week separately.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot each day of the week
days = [&#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Friday&#39;, &#39;Saturday&#39;, &#39;Sunday&#39;]

fig1, axes1 = plt.subplots(1, 5, figsize=(25, 5))
fig2, axes2 = plt.subplots(1, 2, figsize=(25, 5))

# Plot Monday-Friday first
for i in range(5):
    # Take 15min rolling average
    df = weekdays[days[i]].rolling(15).mean()
    
    # Relabel
    df.rename(columns={&#39;stepcount&#39;: &#39;steps/min&#39;}, inplace=True)
    
    # Plot day
    df.plot(ax=axes1[i])
    axes1[i].set_title(days[i])
    axes1[i].set_xlabel(&amp;quot;Time of Day&amp;quot;)
    
# Then plot Saturday-Sunday
for i in range(2):
    # Take 15 min rolling average
    df = weekdays[days[5+i]].rolling(15).mean()
    
    # Relabel
    df.rename(columns={&#39;stepcount&#39;: &#39;steps/min&#39;}, inplace=True)

    # Plot day
    df.plot(ax=axes2[i])
    axes2[i].set_title(days[5+i])
    axes2[i].set_xlabel(&amp;quot;Time of Day&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/fitbit/notebook_88_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vgelinas.github.io/img/posts/fitbit/notebook_88_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
